| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=32, captioning_mask_prob=0.7, checkpoint_activations=None, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=0, cutmix_minmax=None, data_path='/COCO', deepscale=False, deepscale_config=None, deepspeed=False, deepspeed_config='./output_freeze/deepspeed_config.json', deepspeed_mpi=False, device='cuda', dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop_path=0.1, drop_worst_after=12000, drop_worst_ratio=0.2, enable_deepspeed=True, epochs=10, eval=False, eval_batch_size=None, finetune='/home/mm12318/DL_Class/BEiT/beit3_base_patch16_224.pth', gpu=0, initial_scale_power=16, input_size=480, label_smoothing=0.1, layer_decay=1.0, length_penalty=0.6, local_rank=-1, log_dir='./log_freeze', lr=4e-05, min_lr=1e-06, mixup=0, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='beit3_base_patch16_480', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_beams=3, num_max_bpe_tokens=32, num_workers=10, opt='adamw', opt_betas=[0.9, 0.999], opt_eps=1e-08, output_dir='./output_freeze', pin_mem=True, randaug=True, rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', save_ckpt=True, save_ckpt_freq=1, seed=42, sentencepiece_model='/home/mm12318/DL_Class/BEiT/beit3.spm', smoothing=0.1, start_epoch=0, task='coco_captioning', task_cache_path='./output_freeze', task_head_lr_weight=0, train_interpolation='bicubic', update_freq=1, vocab_size=64010, warmup_epochs=1, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.05, world_size=1, zero_stage=0)
Load 566747 image-text pairs from /COCO/coco_captioning.train.jsonl. 
Load 5000 image-text pairs from /COCO/coco_captioning.val.jsonl. 
model_config = beit3_base_patch16_480_captioning
Printing Model: BEiT3ForCaptioning(
  (beit3): BEiT3(
    (text_embed): TextEmbedding(64010, 768)
    (vision_embed): VisionEmbedding(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (encoder): Encoder(
      (dropout_module): Dropout(p=0.0, inplace=False)
      (embed_positions): MutliwayEmbedding(
        (A): PositionalEmbedding(903, 768)
        (B): PositionalEmbedding(1024, 768)
      )
      (layers): ModuleList(
        (0): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.0)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.009090909090909092)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.018181818181818184)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.027272727272727275)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.03636363636363637)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.04545454545454546)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.05454545454545455)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.06363636363636364)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (8): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.07272727272727274)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (9): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.08181818181818183)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (10): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.09090909090909093)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (11): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.1)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (layer_norm): MultiwayNetwork(
        (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mlm_head): Linear(in_features=768, out_features=64010, bias=True)
)
Load ckpt from /home/mm12318/DL_Class/BEiT/beit3_base_patch16_224.pth
Load state_dict by model_key = model
Position interpolate from 14x14 to 30x30
Weights from pretrained model not used in BEiT3ForCaptioning: ['mim_head.weight', 'mim_head.bias']
Model = BEiT3ForCaptioning(
  (beit3): BEiT3(
    (text_embed): TextEmbedding(64010, 768)
    (vision_embed): VisionEmbedding(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (encoder): Encoder(
      (dropout_module): Dropout(p=0.0, inplace=False)
      (embed_positions): MutliwayEmbedding(
        (A): PositionalEmbedding(903, 768)
        (B): PositionalEmbedding(1024, 768)
      )
      (layers): ModuleList(
        (0): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.0)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.009090909090909092)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.018181818181818184)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.027272727272727275)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.03636363636363637)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.04545454545454546)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (6): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.05454545454545455)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (7): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.06363636363636364)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (8): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.07272727272727274)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (9): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.08181818181818183)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (10): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.09090909090909093)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (11): EncoderLayer(
          (self_attn): MultiheadAttention(
            (k_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (v_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (q_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (out_proj): MultiwayNetwork(
              (A): Linear(in_features=768, out_features=768, bias=True)
              (B): Linear(in_features=768, out_features=768, bias=True)
            )
            (inner_attn_ln): MultiwayNetwork(
              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (dropout_module): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (dropout_module): Dropout(p=0.0, inplace=False)
          (drop_path): DropPath(p=0.1)
          (ffn): MultiwayNetwork(
            (A): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
            (B): FeedForwardNetwork(
              (activation_dropout_module): Dropout(p=0.0, inplace=False)
              (dropout_module): Dropout(p=0.0, inplace=False)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
            )
          )
          (final_layer_norm): MultiwayNetwork(
            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (layer_norm): MultiwayNetwork(
        (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mlm_head): Linear(in_features=768, out_features=64010, bias=True)
)
number of params: 270751754
number of trainable params: 49223690
LR = 0.00004000
Batch size = 32
Update frequent = 1
Number of training examples = 113350
Number of training training per epoch = 3542
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "mlm_head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "mlm_head.bias"
    ],
    "lr_scale": 1.0
  }
}
[2023-05-16 05:25:33,975] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.4.0, git-hash=unknown, git-branch=unknown
[2023-05-16 05:25:33,997] [INFO] [utils.py:11:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1
[2023-05-16 05:25:34,054] [INFO] [engine.py:172:__init__] DeepSpeed Flops Profiler Enabled: False
Using /home/mm12318/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/mm12318/.cache/torch_extensions/py38_cu118/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.5756485462188721 seconds
[2023-05-16 05:25:35,748] [INFO] [engine.py:691:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer
[2023-05-16 05:25:35,748] [INFO] [engine.py:696:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
[2023-05-16 05:25:35,748] [INFO] [logging.py:60:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-05-16 05:25:35,753] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2023-05-16 05:25:35,754] [INFO] [engine.py:509:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2023-05-16 05:25:35,754] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-05-16 05:25:35,754] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[4e-05, 4e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 05:25:35,754] [INFO] [config.py:900:print] DeepSpeedEngine configuration:
[2023-05-16 05:25:35,754] [INFO] [config.py:904:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-05-16 05:25:35,754] [INFO] [config.py:904:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   allreduce_always_fp32 ........ False
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   amp_enabled .................. False
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   amp_params ................... {'opt_level': 'O2'}
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   checkpoint_tag_validation_enabled  True
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   checkpoint_tag_validation_fail  False
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   disable_allgather ............ False
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   dump_state ................... False
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   eigenvalue_enabled ........... False
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   eigenvalue_gas_boundary_resolution  1
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   eigenvalue_layer_num ......... 0
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   eigenvalue_max_iter .......... 100
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   eigenvalue_stability ......... 1e-06
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   eigenvalue_tol ............... 0.01
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   eigenvalue_verbose ........... False
[2023-05-16 05:25:35,755] [INFO] [config.py:904:print]   elasticity_enabled ........... False
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   fp16_enabled ................. True
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   fp16_mixed_quantize .......... False
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   global_rank .................. 0
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   gradient_accumulation_steps .. 1
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   gradient_clipping ............ 0.0
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   gradient_predivide_factor .... 1.0
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   initial_dynamic_scale ........ 65536
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   loss_scale ................... 0
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   memory_breakdown ............. False
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   optimizer_legacy_fusion ...... False
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   optimizer_name ............... adam
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   optimizer_params ............. {'lr': 4e-05, 'weight_decay': 0.05, 'bias_correction': True, 'betas': [0.9, 0.999], 'eps': 1e-08}
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   pld_enabled .................. False
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   pld_params ................... False
[2023-05-16 05:25:35,756] [INFO] [config.py:904:print]   prescale_gradients ........... False
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_change_rate ......... 0.001
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_groups .............. 1
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_offset .............. 1000
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_period .............. 1000
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_rounding ............ 0
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_start_bits .......... 16
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_target_bits ......... 8
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_training_enabled .... False
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_type ................ 0
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   quantize_verbose ............. False
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   scheduler_name ............... None
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   scheduler_params ............. None
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   sparse_attention ............. None
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   sparse_gradients_enabled ..... False
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   steps_per_print .............. 1000
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   tensorboard_enabled .......... False
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   tensorboard_job_name ......... DeepSpeedJobName
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   tensorboard_output_path ...... 
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   train_batch_size ............. 32
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   train_micro_batch_size_per_gpu  32
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   use_quantizer_kernel ......... False
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   wall_clock_breakdown ......... False
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   world_size ................... 1
[2023-05-16 05:25:35,757] [INFO] [config.py:904:print]   zero_allow_untested_optimizer  False
[2023-05-16 05:25:35,758] [INFO] [config.py:904:print]   zero_config .................. {
    "stage": 0, 
    "contiguous_gradients": false, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": false, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": true, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+12, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_fp16_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "legacy_stage1": false
}
[2023-05-16 05:25:35,758] [INFO] [config.py:904:print]   zero_enabled ................. False
[2023-05-16 05:25:35,758] [INFO] [config.py:904:print]   zero_optimization_stage ...... 0
[2023-05-16 05:25:35,758] [INFO] [config.py:906:print]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 32, 
    "steps_per_print": 1000, 
    "optimizer": {
        "type": "Adam", 
        "adam_w_mode": true, 
        "params": {
            "lr": 4e-05, 
            "weight_decay": 0.05, 
            "bias_correction": true, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "initial_scale_power": 16, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "amp": {
        "enabled": false, 
        "opt_level": "O2"
    }
}
Using /home/mm12318/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...
Emitting ninja build file /home/mm12318/.cache/torch_extensions/py38_cu118/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.5075869560241699 seconds
model.gradient_accumulation_steps() = 1
Set warmup steps = 3542
Start training for 10 epochs
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
[2023-05-16 05:26:14,623] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 0
[2023-05-16 05:26:14,623] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-05-16 05:26:14,623] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
Epoch: [0]  [   0/3542]  eta: 1 day, 13:35:00  lr: 0.000000  min_lr: 0.000000  loss: 5.1992 (5.1992)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  time: 38.1989  data: 36.7153  max mem: 4077
[2023-05-16 05:26:15,709] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 1
[2023-05-16 05:26:15,709] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-05-16 05:26:15,709] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-05-16 05:26:16,382] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 2
[2023-05-16 05:26:16,382] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 05:26:16,382] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-05-16 05:26:17,556] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 4
[2023-05-16 05:26:17,556] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-05-16 05:26:17,557] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
Epoch: [0]  [  10/3542]  eta: 5:58:54  lr: 0.000000  min_lr: 0.000000  loss: 5.5469 (5.5305)  loss_scale: 4096.0000 (8564.3636)  weight_decay: 0.0500 (0.0500)  time: 6.0971  data: 5.4167  max mem: 4509
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [0]  [  20/3542]  eta: 4:12:55  lr: 0.000000  min_lr: 0.000000  loss: 5.5469 (5.5513)  loss_scale: 4096.0000 (6436.5714)  weight_decay: 0.0500 (0.0500)  time: 2.6143  data: 2.0250  max mem: 4509
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [0]  [  30/3542]  eta: 3:26:56  lr: 0.000000  min_lr: 0.000000  loss: 5.6094 (5.5843)  loss_scale: 4096.0000 (5681.5484)  weight_decay: 0.0500 (0.0500)  time: 2.1266  data: 1.5458  max mem: 4509
Epoch: [0]  [  40/3542]  eta: 2:56:45  lr: 0.000000  min_lr: 0.000000  loss: 5.5664 (5.5624)  loss_scale: 4096.0000 (5294.8293)  weight_decay: 0.0500 (0.0500)  time: 1.6837  data: 1.1022  max mem: 4509
Epoch: [0]  [  50/3542]  eta: 2:37:36  lr: 0.000001  min_lr: 0.000001  loss: 5.5742 (5.5787)  loss_scale: 4096.0000 (5059.7647)  weight_decay: 0.0500 (0.0500)  time: 1.4253  data: 0.8450  max mem: 4509
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [0]  [  60/3542]  eta: 2:24:00  lr: 0.000001  min_lr: 0.000001  loss: 5.6211 (5.5841)  loss_scale: 4096.0000 (4901.7705)  weight_decay: 0.0500 (0.0500)  time: 1.3606  data: 0.7770  max mem: 4509
Epoch: [0]  [  70/3542]  eta: 2:12:44  lr: 0.000001  min_lr: 0.000001  loss: 5.5234 (5.5792)  loss_scale: 4096.0000 (4788.2817)  weight_decay: 0.0500 (0.0500)  time: 1.2385  data: 0.6543  max mem: 4509
Epoch: [0]  [  80/3542]  eta: 2:03:20  lr: 0.000001  min_lr: 0.000001  loss: 5.5508 (5.5773)  loss_scale: 4096.0000 (4702.8148)  weight_decay: 0.0500 (0.0500)  time: 1.0891  data: 0.5081  max mem: 4509
Epoch: [0]  [  90/3542]  eta: 1:55:28  lr: 0.000001  min_lr: 0.000001  loss: 5.5547 (5.5811)  loss_scale: 4096.0000 (4636.1319)  weight_decay: 0.0500 (0.0500)  time: 0.9880  data: 0.4067  max mem: 4509
Epoch: [0]  [ 100/3542]  eta: 1:49:32  lr: 0.000001  min_lr: 0.000001  loss: 5.6094 (5.5888)  loss_scale: 4096.0000 (4582.6535)  weight_decay: 0.0500 (0.0500)  time: 0.9853  data: 0.4033  max mem: 4509
Epoch: [0]  [ 110/3542]  eta: 1:44:26  lr: 0.000001  min_lr: 0.000001  loss: 5.6055 (5.5804)  loss_scale: 4096.0000 (4538.8108)  weight_decay: 0.0500 (0.0500)  time: 1.0026  data: 0.4185  max mem: 4509
Epoch: [0]  [ 120/3542]  eta: 1:40:25  lr: 0.000001  min_lr: 0.000001  loss: 5.5000 (5.5743)  loss_scale: 4096.0000 (4502.2149)  weight_decay: 0.0500 (0.0500)  time: 1.0091  data: 0.4251  max mem: 4509
Epoch: [0]  [ 130/3542]  eta: 1:36:39  lr: 0.000001  min_lr: 0.000001  loss: 5.5430 (5.5722)  loss_scale: 4096.0000 (4471.2061)  weight_decay: 0.0500 (0.0500)  time: 0.9990  data: 0.4168  max mem: 4509
Epoch: [0]  [ 140/3542]  eta: 1:33:16  lr: 0.000002  min_lr: 0.000002  loss: 5.6094 (5.5765)  loss_scale: 4096.0000 (4444.5957)  weight_decay: 0.0500 (0.0500)  time: 0.9458  data: 0.3629  max mem: 4509
Epoch: [0]  [ 150/3542]  eta: 1:29:55  lr: 0.000002  min_lr: 0.000002  loss: 5.6172 (5.5815)  loss_scale: 4096.0000 (4421.5099)  weight_decay: 0.0500 (0.0500)  time: 0.8752  data: 0.2886  max mem: 4509
Epoch: [0]  [ 160/3542]  eta: 1:27:11  lr: 0.000002  min_lr: 0.000002  loss: 5.5156 (5.5712)  loss_scale: 4096.0000 (4401.2919)  weight_decay: 0.0500 (0.0500)  time: 0.8540  data: 0.2657  max mem: 4509
Epoch: [0]  [ 170/3542]  eta: 1:24:41  lr: 0.000002  min_lr: 0.000002  loss: 5.4883 (5.5706)  loss_scale: 4096.0000 (4383.4386)  weight_decay: 0.0500 (0.0500)  time: 0.8767  data: 0.2920  max mem: 4509
Epoch: [0]  [ 180/3542]  eta: 1:22:06  lr: 0.000002  min_lr: 0.000002  loss: 5.5469 (5.5693)  loss_scale: 4096.0000 (4367.5580)  weight_decay: 0.0500 (0.0500)  time: 0.8104  data: 0.2277  max mem: 4509
Epoch: [0]  [ 190/3542]  eta: 1:19:51  lr: 0.000002  min_lr: 0.000002  loss: 5.5391 (5.5674)  loss_scale: 4096.0000 (4353.3403)  weight_decay: 0.0500 (0.0500)  time: 0.7646  data: 0.1817  max mem: 4509
Epoch: [0]  [ 200/3542]  eta: 1:17:52  lr: 0.000002  min_lr: 0.000002  loss: 5.5039 (5.5628)  loss_scale: 4096.0000 (4340.5373)  weight_decay: 0.0500 (0.0500)  time: 0.7886  data: 0.2053  max mem: 4509
Epoch: [0]  [ 210/3542]  eta: 1:16:08  lr: 0.000002  min_lr: 0.000002  loss: 5.5312 (5.5657)  loss_scale: 4096.0000 (4328.9479)  weight_decay: 0.0500 (0.0500)  time: 0.8166  data: 0.2333  max mem: 4509
Epoch: [0]  [ 220/3542]  eta: 1:14:20  lr: 0.000002  min_lr: 0.000002  loss: 5.5508 (5.5632)  loss_scale: 4096.0000 (4318.4072)  weight_decay: 0.0500 (0.0500)  time: 0.7848  data: 0.2018  max mem: 4509
Epoch: [0]  [ 230/3542]  eta: 1:12:37  lr: 0.000003  min_lr: 0.000003  loss: 5.4141 (5.5595)  loss_scale: 4096.0000 (4308.7792)  weight_decay: 0.0500 (0.0500)  time: 0.7285  data: 0.1448  max mem: 4509
Epoch: [0]  [ 240/3542]  eta: 1:11:02  lr: 0.000003  min_lr: 0.000003  loss: 5.5352 (5.5593)  loss_scale: 4096.0000 (4299.9502)  weight_decay: 0.0500 (0.0500)  time: 0.7207  data: 0.1364  max mem: 4509
Epoch: [0]  [ 250/3542]  eta: 1:09:38  lr: 0.000003  min_lr: 0.000003  loss: 5.5352 (5.5562)  loss_scale: 4096.0000 (4291.8247)  weight_decay: 0.0500 (0.0500)  time: 0.7349  data: 0.1508  max mem: 4509
Epoch: [0]  [ 260/3542]  eta: 1:08:21  lr: 0.000003  min_lr: 0.000003  loss: 5.4609 (5.5518)  loss_scale: 4096.0000 (4284.3218)  weight_decay: 0.0500 (0.0500)  time: 0.7514  data: 0.1674  max mem: 4509
Epoch: [0]  [ 270/3542]  eta: 1:07:13  lr: 0.000003  min_lr: 0.000003  loss: 5.3477 (5.5437)  loss_scale: 4096.0000 (4277.3727)  weight_decay: 0.0500 (0.0500)  time: 0.7732  data: 0.1890  max mem: 4509
Epoch: [0]  [ 280/3542]  eta: 1:06:02  lr: 0.000003  min_lr: 0.000003  loss: 5.3438 (5.5390)  loss_scale: 4096.0000 (4270.9181)  weight_decay: 0.0500 (0.0500)  time: 0.7588  data: 0.1744  max mem: 4509
Epoch: [0]  [ 290/3542]  eta: 1:05:03  lr: 0.000003  min_lr: 0.000003  loss: 5.3906 (5.5333)  loss_scale: 4096.0000 (4264.9072)  weight_decay: 0.0500 (0.0500)  time: 0.7631  data: 0.1790  max mem: 4509
Epoch: [0]  [ 300/3542]  eta: 1:04:07  lr: 0.000003  min_lr: 0.000003  loss: 5.2969 (5.5258)  loss_scale: 4096.0000 (4259.2957)  weight_decay: 0.0500 (0.0500)  time: 0.7929  data: 0.2089  max mem: 4509
Epoch: [0]  [ 310/3542]  eta: 1:03:11  lr: 0.000004  min_lr: 0.000004  loss: 5.2969 (5.5189)  loss_scale: 4096.0000 (4254.0450)  weight_decay: 0.0500 (0.0500)  time: 0.7770  data: 0.1929  max mem: 4509
Epoch: [0]  [ 320/3542]  eta: 1:02:22  lr: 0.000004  min_lr: 0.000004  loss: 5.3281 (5.5123)  loss_scale: 4096.0000 (4249.1215)  weight_decay: 0.0500 (0.0500)  time: 0.7831  data: 0.1986  max mem: 4509
Epoch: [0]  [ 330/3542]  eta: 1:01:28  lr: 0.000004  min_lr: 0.000004  loss: 5.3203 (5.5089)  loss_scale: 4096.0000 (4244.4955)  weight_decay: 0.0500 (0.0500)  time: 0.7624  data: 0.1777  max mem: 4509
Epoch: [0]  [ 340/3542]  eta: 1:00:39  lr: 0.000004  min_lr: 0.000004  loss: 5.3203 (5.5025)  loss_scale: 4096.0000 (4240.1408)  weight_decay: 0.0500 (0.0500)  time: 0.7361  data: 0.1522  max mem: 4509
Epoch: [0]  [ 350/3542]  eta: 0:59:55  lr: 0.000004  min_lr: 0.000004  loss: 5.2344 (5.4969)  loss_scale: 4096.0000 (4236.0342)  weight_decay: 0.0500 (0.0500)  time: 0.7609  data: 0.1770  max mem: 4509
Epoch: [0]  [ 360/3542]  eta: 0:59:06  lr: 0.000004  min_lr: 0.000004  loss: 5.2344 (5.4926)  loss_scale: 4096.0000 (4232.1551)  weight_decay: 0.0500 (0.0500)  time: 0.7393  data: 0.1554  max mem: 4509
Epoch: [0]  [ 370/3542]  eta: 0:58:23  lr: 0.000004  min_lr: 0.000004  loss: 5.1602 (5.4870)  loss_scale: 4096.0000 (4228.4852)  weight_decay: 0.0500 (0.0500)  time: 0.7215  data: 0.1371  max mem: 4509
Epoch: [0]  [ 380/3542]  eta: 0:57:41  lr: 0.000004  min_lr: 0.000004  loss: 5.4023 (5.4837)  loss_scale: 4096.0000 (4225.0079)  weight_decay: 0.0500 (0.0500)  time: 0.7336  data: 0.1488  max mem: 4509
Epoch: [0]  [ 390/3542]  eta: 0:57:08  lr: 0.000004  min_lr: 0.000004  loss: 5.2656 (5.4750)  loss_scale: 4096.0000 (4221.7084)  weight_decay: 0.0500 (0.0500)  time: 0.7781  data: 0.1916  max mem: 4509
Epoch: [0]  [ 400/3542]  eta: 0:56:29  lr: 0.000005  min_lr: 0.000005  loss: 5.2266 (5.4703)  loss_scale: 4096.0000 (4218.5736)  weight_decay: 0.0500 (0.0500)  time: 0.7770  data: 0.1906  max mem: 4509
Epoch: [0]  [ 410/3542]  eta: 0:55:50  lr: 0.000005  min_lr: 0.000005  loss: 5.2695 (5.4650)  loss_scale: 4096.0000 (4215.5912)  weight_decay: 0.0500 (0.0500)  time: 0.7163  data: 0.1322  max mem: 4509
Epoch: [0]  [ 420/3542]  eta: 0:55:15  lr: 0.000005  min_lr: 0.000005  loss: 5.2344 (5.4592)  loss_scale: 4096.0000 (4212.7506)  weight_decay: 0.0500 (0.0500)  time: 0.7258  data: 0.1414  max mem: 4509
Epoch: [0]  [ 430/3542]  eta: 0:54:47  lr: 0.000005  min_lr: 0.000005  loss: 5.1523 (5.4520)  loss_scale: 4096.0000 (4210.0418)  weight_decay: 0.0500 (0.0500)  time: 0.7821  data: 0.1972  max mem: 4509
Epoch: [0]  [ 440/3542]  eta: 0:54:12  lr: 0.000005  min_lr: 0.000005  loss: 5.1211 (5.4435)  loss_scale: 4096.0000 (4207.4558)  weight_decay: 0.0500 (0.0500)  time: 0.7637  data: 0.1790  max mem: 4509
Epoch: [0]  [ 450/3542]  eta: 0:53:41  lr: 0.000005  min_lr: 0.000005  loss: 4.9844 (5.4316)  loss_scale: 4096.0000 (4204.9845)  weight_decay: 0.0500 (0.0500)  time: 0.7297  data: 0.1452  max mem: 4509
Epoch: [0]  [ 460/3542]  eta: 0:53:09  lr: 0.000005  min_lr: 0.000005  loss: 4.9219 (5.4228)  loss_scale: 4096.0000 (4202.6204)  weight_decay: 0.0500 (0.0500)  time: 0.7343  data: 0.1083  max mem: 4509
Epoch: [0]  [ 470/3542]  eta: 0:52:43  lr: 0.000005  min_lr: 0.000005  loss: 5.0117 (5.4146)  loss_scale: 4096.0000 (4200.3567)  weight_decay: 0.0500 (0.0500)  time: 0.7556  data: 0.1297  max mem: 4509
Epoch: [0]  [ 480/3542]  eta: 0:52:14  lr: 0.000005  min_lr: 0.000005  loss: 5.0938 (5.4082)  loss_scale: 4096.0000 (4198.1871)  weight_decay: 0.0500 (0.0500)  time: 0.7683  data: 0.1846  max mem: 4509
Epoch: [0]  [ 490/3542]  eta: 0:51:46  lr: 0.000006  min_lr: 0.000006  loss: 5.0859 (5.4010)  loss_scale: 4096.0000 (4196.1059)  weight_decay: 0.0500 (0.0500)  time: 0.7395  data: 0.1547  max mem: 4509
Epoch: [0]  [ 500/3542]  eta: 0:51:20  lr: 0.000006  min_lr: 0.000006  loss: 5.0625 (5.3939)  loss_scale: 4096.0000 (4194.1078)  weight_decay: 0.0500 (0.0500)  time: 0.7454  data: 0.1603  max mem: 4509
Epoch: [0]  [ 510/3542]  eta: 0:50:56  lr: 0.000006  min_lr: 0.000006  loss: 5.0742 (5.3875)  loss_scale: 4096.0000 (4192.1879)  weight_decay: 0.0500 (0.0500)  time: 0.7635  data: 0.1798  max mem: 4509
Epoch: [0]  [ 520/3542]  eta: 0:50:31  lr: 0.000006  min_lr: 0.000006  loss: 5.0742 (5.3791)  loss_scale: 4096.0000 (4190.3417)  weight_decay: 0.0500 (0.0500)  time: 0.7616  data: 0.1780  max mem: 4509
Epoch: [0]  [ 530/3542]  eta: 0:50:10  lr: 0.000006  min_lr: 0.000006  loss: 4.9922 (5.3719)  loss_scale: 4096.0000 (4188.5650)  weight_decay: 0.0500 (0.0500)  time: 0.7846  data: 0.2004  max mem: 4509
Epoch: [0]  [ 540/3542]  eta: 0:49:44  lr: 0.000006  min_lr: 0.000006  loss: 5.0273 (5.3646)  loss_scale: 4096.0000 (4186.8540)  weight_decay: 0.0500 (0.0500)  time: 0.7678  data: 0.1837  max mem: 4509
Epoch: [0]  [ 550/3542]  eta: 0:49:20  lr: 0.000006  min_lr: 0.000006  loss: 4.9766 (5.3582)  loss_scale: 4096.0000 (4185.2051)  weight_decay: 0.0500 (0.0500)  time: 0.7231  data: 0.1385  max mem: 4509
Epoch: [0]  [ 560/3542]  eta: 0:48:57  lr: 0.000006  min_lr: 0.000006  loss: 4.9492 (5.3512)  loss_scale: 4096.0000 (4183.6150)  weight_decay: 0.0500 (0.0500)  time: 0.7374  data: 0.1528  max mem: 4509
Epoch: [0]  [ 570/3542]  eta: 0:48:34  lr: 0.000006  min_lr: 0.000006  loss: 4.9180 (5.3436)  loss_scale: 4096.0000 (4182.0806)  weight_decay: 0.0500 (0.0500)  time: 0.7391  data: 0.1555  max mem: 4509
Epoch: [0]  [ 580/3542]  eta: 0:48:17  lr: 0.000007  min_lr: 0.000007  loss: 4.9375 (5.3358)  loss_scale: 4096.0000 (4180.5990)  weight_decay: 0.0500 (0.0500)  time: 0.7871  data: 0.2037  max mem: 4509
Epoch: [0]  [ 590/3542]  eta: 0:47:58  lr: 0.000007  min_lr: 0.000007  loss: 4.9453 (5.3302)  loss_scale: 4096.0000 (4179.1675)  weight_decay: 0.0500 (0.0500)  time: 0.8110  data: 0.2277  max mem: 4509
Epoch: [0]  [ 600/3542]  eta: 0:47:41  lr: 0.000007  min_lr: 0.000007  loss: 4.9141 (5.3224)  loss_scale: 4096.0000 (4177.7837)  weight_decay: 0.0500 (0.0500)  time: 0.8068  data: 0.2228  max mem: 4509
Epoch: [0]  [ 610/3542]  eta: 0:47:26  lr: 0.000007  min_lr: 0.000007  loss: 4.8711 (5.3148)  loss_scale: 4096.0000 (4176.4452)  weight_decay: 0.0500 (0.0500)  time: 0.8500  data: 0.2664  max mem: 4509
Epoch: [0]  [ 620/3542]  eta: 0:47:07  lr: 0.000007  min_lr: 0.000007  loss: 4.8789 (5.3090)  loss_scale: 4096.0000 (4175.1498)  weight_decay: 0.0500 (0.0500)  time: 0.8150  data: 0.2322  max mem: 4509
Epoch: [0]  [ 630/3542]  eta: 0:46:50  lr: 0.000007  min_lr: 0.000007  loss: 4.8789 (5.3018)  loss_scale: 4096.0000 (4173.8954)  weight_decay: 0.0500 (0.0500)  time: 0.7873  data: 0.2044  max mem: 4509
Epoch: [0]  [ 640/3542]  eta: 0:46:37  lr: 0.000007  min_lr: 0.000007  loss: 4.8438 (5.2944)  loss_scale: 4096.0000 (4172.6802)  weight_decay: 0.0500 (0.0500)  time: 0.8516  data: 0.2670  max mem: 4509
Epoch: [0]  [ 650/3542]  eta: 0:46:18  lr: 0.000007  min_lr: 0.000007  loss: 4.6953 (5.2849)  loss_scale: 4096.0000 (4171.5023)  weight_decay: 0.0500 (0.0500)  time: 0.8287  data: 0.2435  max mem: 4509
Epoch: [0]  [ 660/3542]  eta: 0:46:02  lr: 0.000007  min_lr: 0.000007  loss: 4.6953 (5.2763)  loss_scale: 4096.0000 (4170.3601)  weight_decay: 0.0500 (0.0500)  time: 0.7862  data: 0.2022  max mem: 4509
Epoch: [0]  [ 670/3542]  eta: 0:45:43  lr: 0.000008  min_lr: 0.000008  loss: 4.8281 (5.2699)  loss_scale: 4096.0000 (4169.2519)  weight_decay: 0.0500 (0.0500)  time: 0.7727  data: 0.1888  max mem: 4509
Epoch: [0]  [ 680/3542]  eta: 0:45:28  lr: 0.000008  min_lr: 0.000008  loss: 4.8047 (5.2625)  loss_scale: 4096.0000 (4168.1762)  weight_decay: 0.0500 (0.0500)  time: 0.7850  data: 0.2009  max mem: 4509
Epoch: [0]  [ 690/3542]  eta: 0:45:10  lr: 0.000008  min_lr: 0.000008  loss: 4.6367 (5.2529)  loss_scale: 4096.0000 (4167.1317)  weight_decay: 0.0500 (0.0500)  time: 0.7849  data: 0.2004  max mem: 4509
Epoch: [0]  [ 700/3542]  eta: 0:44:49  lr: 0.000008  min_lr: 0.000008  loss: 4.6289 (5.2453)  loss_scale: 4096.0000 (4166.1170)  weight_decay: 0.0500 (0.0500)  time: 0.7107  data: 0.1259  max mem: 4509
Epoch: [0]  [ 710/3542]  eta: 0:44:32  lr: 0.000008  min_lr: 0.000008  loss: 4.7773 (5.2387)  loss_scale: 4096.0000 (4165.1308)  weight_decay: 0.0500 (0.0500)  time: 0.7090  data: 0.1250  max mem: 4509
Epoch: [0]  [ 720/3542]  eta: 0:44:16  lr: 0.000008  min_lr: 0.000008  loss: 4.6250 (5.2301)  loss_scale: 4096.0000 (4164.1720)  weight_decay: 0.0500 (0.0500)  time: 0.7571  data: 0.1738  max mem: 4509
Epoch: [0]  [ 730/3542]  eta: 0:43:59  lr: 0.000008  min_lr: 0.000008  loss: 4.6172 (5.2231)  loss_scale: 4096.0000 (4163.2394)  weight_decay: 0.0500 (0.0500)  time: 0.7712  data: 0.1871  max mem: 4509
Epoch: [0]  [ 740/3542]  eta: 0:43:43  lr: 0.000008  min_lr: 0.000008  loss: 4.7188 (5.2163)  loss_scale: 4096.0000 (4162.3320)  weight_decay: 0.0500 (0.0500)  time: 0.7520  data: 0.1678  max mem: 4509
Epoch: [0]  [ 750/3542]  eta: 0:43:26  lr: 0.000008  min_lr: 0.000008  loss: 4.7344 (5.2102)  loss_scale: 4096.0000 (4161.4487)  weight_decay: 0.0500 (0.0500)  time: 0.7442  data: 0.1600  max mem: 4509
Epoch: [0]  [ 760/3542]  eta: 0:43:09  lr: 0.000009  min_lr: 0.000009  loss: 4.7344 (5.2037)  loss_scale: 4096.0000 (4160.5887)  weight_decay: 0.0500 (0.0500)  time: 0.7353  data: 0.1505  max mem: 4509
Epoch: [0]  [ 770/3542]  eta: 0:42:54  lr: 0.000009  min_lr: 0.000009  loss: 4.5977 (5.1962)  loss_scale: 4096.0000 (4159.7510)  weight_decay: 0.0500 (0.0500)  time: 0.7517  data: 0.1673  max mem: 4509
Epoch: [0]  [ 780/3542]  eta: 0:42:41  lr: 0.000009  min_lr: 0.000009  loss: 4.5898 (5.1891)  loss_scale: 4096.0000 (4158.9347)  weight_decay: 0.0500 (0.0500)  time: 0.7978  data: 0.2127  max mem: 4509
Epoch: [0]  [ 790/3542]  eta: 0:42:26  lr: 0.000009  min_lr: 0.000009  loss: 4.6367 (5.1838)  loss_scale: 4096.0000 (4158.1391)  weight_decay: 0.0500 (0.0500)  time: 0.7879  data: 0.2029  max mem: 4509
Epoch: [0]  [ 800/3542]  eta: 0:42:12  lr: 0.000009  min_lr: 0.000009  loss: 4.6719 (5.1766)  loss_scale: 4096.0000 (4157.3633)  weight_decay: 0.0500 (0.0500)  time: 0.7701  data: 0.1864  max mem: 4509
Epoch: [0]  [ 810/3542]  eta: 0:42:20  lr: 0.000009  min_lr: 0.000009  loss: 4.6211 (5.1701)  loss_scale: 4096.0000 (4156.6067)  weight_decay: 0.0500 (0.0500)  time: 1.1132  data: 0.5287  max mem: 4509
Epoch: [0]  [ 820/3542]  eta: 0:42:06  lr: 0.000009  min_lr: 0.000009  loss: 4.5469 (5.1629)  loss_scale: 4096.0000 (4155.8685)  weight_decay: 0.0500 (0.0500)  time: 1.1125  data: 0.5279  max mem: 4509
Epoch: [0]  [ 830/3542]  eta: 0:41:54  lr: 0.000009  min_lr: 0.000009  loss: 4.5000 (5.1551)  loss_scale: 4096.0000 (4155.1480)  weight_decay: 0.0500 (0.0500)  time: 0.8163  data: 0.2328  max mem: 4509
Epoch: [0]  [ 840/3542]  eta: 0:41:41  lr: 0.000009  min_lr: 0.000009  loss: 4.4648 (5.1481)  loss_scale: 4096.0000 (4154.4447)  weight_decay: 0.0500 (0.0500)  time: 0.8340  data: 0.2503  max mem: 4509
Epoch: [0]  [ 850/3542]  eta: 0:41:28  lr: 0.000010  min_lr: 0.000010  loss: 4.4766 (5.1405)  loss_scale: 4096.0000 (4153.7579)  weight_decay: 0.0500 (0.0500)  time: 0.8133  data: 0.2294  max mem: 4509
Epoch: [0]  [ 860/3542]  eta: 0:41:13  lr: 0.000010  min_lr: 0.000010  loss: 4.4766 (5.1331)  loss_scale: 4096.0000 (4153.0871)  weight_decay: 0.0500 (0.0500)  time: 0.7695  data: 0.1847  max mem: 4509
Epoch: [0]  [ 870/3542]  eta: 0:40:58  lr: 0.000010  min_lr: 0.000010  loss: 4.5039 (5.1265)  loss_scale: 4096.0000 (4152.4317)  weight_decay: 0.0500 (0.0500)  time: 0.7250  data: 0.1403  max mem: 4509
Epoch: [0]  [ 880/3542]  eta: 0:40:45  lr: 0.000010  min_lr: 0.000010  loss: 4.5234 (5.1202)  loss_scale: 4096.0000 (4151.7911)  weight_decay: 0.0500 (0.0500)  time: 0.7648  data: 0.1807  max mem: 4509
Epoch: [0]  [ 890/3542]  eta: 0:40:30  lr: 0.000010  min_lr: 0.000010  loss: 4.4883 (5.1132)  loss_scale: 4096.0000 (4151.1650)  weight_decay: 0.0500 (0.0500)  time: 0.7596  data: 0.1755  max mem: 4509
Epoch: [0]  [ 900/3542]  eta: 0:40:17  lr: 0.000010  min_lr: 0.000010  loss: 4.4688 (5.1073)  loss_scale: 4096.0000 (4150.5527)  weight_decay: 0.0500 (0.0500)  time: 0.7471  data: 0.1628  max mem: 4509
Epoch: [0]  [ 910/3542]  eta: 0:40:00  lr: 0.000010  min_lr: 0.000010  loss: 4.4727 (5.0999)  loss_scale: 4096.0000 (4149.9539)  weight_decay: 0.0500 (0.0500)  time: 0.7110  data: 0.1262  max mem: 4509
Epoch: [0]  [ 920/3542]  eta: 0:39:47  lr: 0.000010  min_lr: 0.000010  loss: 4.4297 (5.0929)  loss_scale: 4096.0000 (4149.3681)  weight_decay: 0.0500 (0.0500)  time: 0.7095  data: 0.1245  max mem: 4509
Epoch: [0]  [ 930/3542]  eta: 0:39:34  lr: 0.000011  min_lr: 0.000011  loss: 4.3984 (5.0858)  loss_scale: 4096.0000 (4148.7948)  weight_decay: 0.0500 (0.0500)  time: 0.7759  data: 0.1908  max mem: 4509
Epoch: [0]  [ 940/3542]  eta: 0:39:20  lr: 0.000011  min_lr: 0.000011  loss: 4.4141 (5.0796)  loss_scale: 4096.0000 (4148.2338)  weight_decay: 0.0500 (0.0500)  time: 0.7603  data: 0.1751  max mem: 4509
Epoch: [0]  [ 950/3542]  eta: 0:39:05  lr: 0.000011  min_lr: 0.000011  loss: 4.4180 (5.0727)  loss_scale: 4096.0000 (4147.6845)  weight_decay: 0.0500 (0.0500)  time: 0.7238  data: 0.1391  max mem: 4509
Epoch: [0]  [ 960/3542]  eta: 0:38:52  lr: 0.000011  min_lr: 0.000011  loss: 4.4375 (5.0666)  loss_scale: 4096.0000 (4147.1467)  weight_decay: 0.0500 (0.0500)  time: 0.7293  data: 0.1443  max mem: 4509
Epoch: [0]  [ 970/3542]  eta: 0:38:39  lr: 0.000011  min_lr: 0.000011  loss: 4.4102 (5.0601)  loss_scale: 4096.0000 (4146.6200)  weight_decay: 0.0500 (0.0500)  time: 0.7396  data: 0.1536  max mem: 4509
Epoch: [0]  [ 980/3542]  eta: 0:38:24  lr: 0.000011  min_lr: 0.000011  loss: 4.3945 (5.0532)  loss_scale: 4096.0000 (4146.1040)  weight_decay: 0.0500 (0.0500)  time: 0.7149  data: 0.1290  max mem: 4509
Epoch: [0]  [ 990/3542]  eta: 0:38:11  lr: 0.000011  min_lr: 0.000011  loss: 4.3398 (5.0458)  loss_scale: 4096.0000 (4145.5984)  weight_decay: 0.0500 (0.0500)  time: 0.7238  data: 0.1390  max mem: 4509
[2023-05-16 05:40:31,846] [INFO] [logging.py:60:log_dist] [Rank 0] step=1000, skipped=4, lr=[1.1284947754871507e-05, 1.1284947754871507e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 05:40:31,849] [INFO] [timer.py:157:stop] 0/1000, SamplesPerSec=55.62901189052717
Epoch: [0]  [1000/3542]  eta: 0:37:57  lr: 0.000011  min_lr: 0.000011  loss: 4.3047 (5.0389)  loss_scale: 4096.0000 (4145.1029)  weight_decay: 0.0500 (0.0500)  time: 0.7205  data: 0.1364  max mem: 4509
[2023-05-16 05:40:36,504] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 05:40:36,504] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
Epoch: [0]  [1010/3542]  eta: 0:37:45  lr: 0.000011  min_lr: 0.000011  loss: 4.3555 (5.0331)  loss_scale: 4096.0000 (4168.9258)  weight_decay: 0.0500 (0.0500)  time: 0.7351  data: 0.1507  max mem: 4509
Epoch: [0]  [1020/3542]  eta: 0:37:33  lr: 0.000012  min_lr: 0.000012  loss: 4.3555 (5.0267)  loss_scale: 8192.0000 (4208.3291)  weight_decay: 0.0500 (0.0500)  time: 0.7714  data: 0.1870  max mem: 4509
Epoch: [0]  [1030/3542]  eta: 0:37:21  lr: 0.000012  min_lr: 0.000012  loss: 4.3203 (5.0197)  loss_scale: 8192.0000 (4246.9680)  weight_decay: 0.0500 (0.0500)  time: 0.7597  data: 0.1747  max mem: 4509
Epoch: [0]  [1040/3542]  eta: 0:37:08  lr: 0.000012  min_lr: 0.000012  loss: 4.3633 (5.0138)  loss_scale: 8192.0000 (4284.8646)  weight_decay: 0.0500 (0.0500)  time: 0.7296  data: 0.1448  max mem: 4509
Epoch: [0]  [1050/3542]  eta: 0:36:54  lr: 0.000012  min_lr: 0.000012  loss: 4.3398 (5.0073)  loss_scale: 8192.0000 (4322.0400)  weight_decay: 0.0500 (0.0500)  time: 0.7080  data: 0.1235  max mem: 4509
Epoch: [0]  [1060/3542]  eta: 0:36:42  lr: 0.000012  min_lr: 0.000012  loss: 4.2578 (5.0000)  loss_scale: 8192.0000 (4358.5146)  weight_decay: 0.0500 (0.0500)  time: 0.7326  data: 0.1478  max mem: 4509
Epoch: [0]  [1070/3542]  eta: 0:36:30  lr: 0.000012  min_lr: 0.000012  loss: 4.1523 (4.9922)  loss_scale: 8192.0000 (4394.3081)  weight_decay: 0.0500 (0.0500)  time: 0.7507  data: 0.1656  max mem: 4509
Epoch: [0]  [1080/3542]  eta: 0:36:18  lr: 0.000012  min_lr: 0.000012  loss: 4.1094 (4.9847)  loss_scale: 8192.0000 (4429.4394)  weight_decay: 0.0500 (0.0500)  time: 0.7342  data: 0.1494  max mem: 4509
Epoch: [0]  [1090/3542]  eta: 0:36:05  lr: 0.000012  min_lr: 0.000012  loss: 4.2188 (4.9785)  loss_scale: 8192.0000 (4463.9267)  weight_decay: 0.0500 (0.0500)  time: 0.7146  data: 0.1296  max mem: 4509
Epoch: [0]  [1100/3542]  eta: 0:35:52  lr: 0.000012  min_lr: 0.000012  loss: 4.2188 (4.9722)  loss_scale: 8192.0000 (4497.7875)  weight_decay: 0.0500 (0.0500)  time: 0.7125  data: 0.1276  max mem: 4509
Epoch: [0]  [1110/3542]  eta: 0:35:43  lr: 0.000013  min_lr: 0.000013  loss: 4.1562 (4.9653)  loss_scale: 8192.0000 (4531.0387)  weight_decay: 0.0500 (0.0500)  time: 0.7840  data: 0.2000  max mem: 4509
Epoch: [0]  [1120/3542]  eta: 0:35:30  lr: 0.000013  min_lr: 0.000013  loss: 4.1445 (4.9582)  loss_scale: 8192.0000 (4563.6967)  weight_decay: 0.0500 (0.0500)  time: 0.7781  data: 0.1939  max mem: 4509
Epoch: [0]  [1130/3542]  eta: 0:35:20  lr: 0.000013  min_lr: 0.000013  loss: 4.2070 (4.9519)  loss_scale: 8192.0000 (4595.7772)  weight_decay: 0.0500 (0.0500)  time: 0.7608  data: 0.1758  max mem: 4509
Epoch: [0]  [1140/3542]  eta: 0:35:09  lr: 0.000013  min_lr: 0.000013  loss: 4.2500 (4.9455)  loss_scale: 8192.0000 (4627.2954)  weight_decay: 0.0500 (0.0500)  time: 0.7866  data: 0.2014  max mem: 4509
Epoch: [0]  [1150/3542]  eta: 0:34:58  lr: 0.000013  min_lr: 0.000013  loss: 4.2500 (4.9393)  loss_scale: 8192.0000 (4658.2659)  weight_decay: 0.0500 (0.0500)  time: 0.7779  data: 0.1934  max mem: 4509
Epoch: [0]  [1160/3542]  eta: 0:34:47  lr: 0.000013  min_lr: 0.000013  loss: 4.2031 (4.9327)  loss_scale: 8192.0000 (4688.7028)  weight_decay: 0.0500 (0.0500)  time: 0.7747  data: 0.1899  max mem: 4509
Epoch: [0]  [1170/3542]  eta: 0:34:36  lr: 0.000013  min_lr: 0.000013  loss: 4.1523 (4.9259)  loss_scale: 8192.0000 (4718.6200)  weight_decay: 0.0500 (0.0500)  time: 0.7729  data: 0.1875  max mem: 4509
Epoch: [0]  [1180/3542]  eta: 0:34:25  lr: 0.000013  min_lr: 0.000013  loss: 4.0859 (4.9188)  loss_scale: 8192.0000 (4748.0305)  weight_decay: 0.0500 (0.0500)  time: 0.7639  data: 0.1791  max mem: 4509
Epoch: [0]  [1190/3542]  eta: 0:34:14  lr: 0.000013  min_lr: 0.000013  loss: 4.0859 (4.9118)  loss_scale: 8192.0000 (4776.9471)  weight_decay: 0.0500 (0.0500)  time: 0.7582  data: 0.1729  max mem: 4509
Epoch: [0]  [1200/3542]  eta: 0:34:03  lr: 0.000014  min_lr: 0.000014  loss: 4.1016 (4.9052)  loss_scale: 8192.0000 (4805.3822)  weight_decay: 0.0500 (0.0500)  time: 0.7724  data: 0.1866  max mem: 4509
Epoch: [0]  [1210/3542]  eta: 0:33:53  lr: 0.000014  min_lr: 0.000014  loss: 4.0625 (4.8982)  loss_scale: 8192.0000 (4833.3476)  weight_decay: 0.0500 (0.0500)  time: 0.7899  data: 0.2048  max mem: 4509
Epoch: [0]  [1220/3542]  eta: 0:33:43  lr: 0.000014  min_lr: 0.000014  loss: 4.0781 (4.8919)  loss_scale: 8192.0000 (4860.8550)  weight_decay: 0.0500 (0.0500)  time: 0.7889  data: 0.2041  max mem: 4509
Epoch: [0]  [1230/3542]  eta: 0:33:32  lr: 0.000014  min_lr: 0.000014  loss: 4.1250 (4.8855)  loss_scale: 8192.0000 (4887.9155)  weight_decay: 0.0500 (0.0500)  time: 0.7674  data: 0.1826  max mem: 4509
Epoch: [0]  [1240/3542]  eta: 0:33:22  lr: 0.000014  min_lr: 0.000014  loss: 4.0703 (4.8793)  loss_scale: 8192.0000 (4914.5399)  weight_decay: 0.0500 (0.0500)  time: 0.7704  data: 0.1855  max mem: 4509
Epoch: [0]  [1250/3542]  eta: 0:33:10  lr: 0.000014  min_lr: 0.000014  loss: 4.0117 (4.8727)  loss_scale: 8192.0000 (4940.7386)  weight_decay: 0.0500 (0.0500)  time: 0.7344  data: 0.1491  max mem: 4509
Epoch: [0]  [1260/3542]  eta: 0:32:59  lr: 0.000014  min_lr: 0.000014  loss: 4.0117 (4.8662)  loss_scale: 8192.0000 (4966.5218)  weight_decay: 0.0500 (0.0500)  time: 0.7355  data: 0.1504  max mem: 4509
Epoch: [0]  [1270/3542]  eta: 0:32:48  lr: 0.000014  min_lr: 0.000014  loss: 4.0430 (4.8593)  loss_scale: 8192.0000 (4991.8993)  weight_decay: 0.0500 (0.0500)  time: 0.7506  data: 0.1656  max mem: 4509
Epoch: [0]  [1280/3542]  eta: 0:32:38  lr: 0.000014  min_lr: 0.000014  loss: 3.9824 (4.8528)  loss_scale: 8192.0000 (5016.8806)  weight_decay: 0.0500 (0.0500)  time: 0.7663  data: 0.1810  max mem: 4509
Epoch: [0]  [1290/3542]  eta: 0:32:29  lr: 0.000015  min_lr: 0.000015  loss: 3.9824 (4.8464)  loss_scale: 8192.0000 (5041.4748)  weight_decay: 0.0500 (0.0500)  time: 0.8195  data: 0.2342  max mem: 4509
Epoch: [0]  [1300/3542]  eta: 0:32:19  lr: 0.000015  min_lr: 0.000015  loss: 3.9199 (4.8393)  loss_scale: 8192.0000 (5065.6910)  weight_decay: 0.0500 (0.0500)  time: 0.8096  data: 0.2247  max mem: 4509
Epoch: [0]  [1310/3542]  eta: 0:32:08  lr: 0.000015  min_lr: 0.000015  loss: 3.9199 (4.8336)  loss_scale: 8192.0000 (5089.5378)  weight_decay: 0.0500 (0.0500)  time: 0.7666  data: 0.1810  max mem: 4509
Epoch: [0]  [1320/3542]  eta: 0:31:58  lr: 0.000015  min_lr: 0.000015  loss: 3.9512 (4.8272)  loss_scale: 8192.0000 (5113.0235)  weight_decay: 0.0500 (0.0500)  time: 0.7579  data: 0.1724  max mem: 4509
Epoch: [0]  [1330/3542]  eta: 0:31:49  lr: 0.000015  min_lr: 0.000015  loss: 3.9688 (4.8207)  loss_scale: 8192.0000 (5136.1563)  weight_decay: 0.0500 (0.0500)  time: 0.7878  data: 0.2037  max mem: 4509
Epoch: [0]  [1340/3542]  eta: 0:31:39  lr: 0.000015  min_lr: 0.000015  loss: 3.9688 (4.8140)  loss_scale: 8192.0000 (5158.9441)  weight_decay: 0.0500 (0.0500)  time: 0.8122  data: 0.2280  max mem: 4509
Epoch: [0]  [1350/3542]  eta: 0:31:30  lr: 0.000015  min_lr: 0.000015  loss: 3.9414 (4.8077)  loss_scale: 8192.0000 (5181.3945)  weight_decay: 0.0500 (0.0500)  time: 0.8061  data: 0.2211  max mem: 4509
Epoch: [0]  [1360/3542]  eta: 0:31:18  lr: 0.000015  min_lr: 0.000015  loss: 3.9180 (4.8010)  loss_scale: 8192.0000 (5203.5151)  weight_decay: 0.0500 (0.0500)  time: 0.7498  data: 0.1649  max mem: 4509
Epoch: [0]  [1370/3542]  eta: 0:31:09  lr: 0.000015  min_lr: 0.000015  loss: 3.8535 (4.7943)  loss_scale: 8192.0000 (5225.3129)  weight_decay: 0.0500 (0.0500)  time: 0.7572  data: 0.1724  max mem: 4509
Epoch: [0]  [1380/3542]  eta: 0:30:58  lr: 0.000016  min_lr: 0.000016  loss: 3.8789 (4.7879)  loss_scale: 8192.0000 (5246.7951)  weight_decay: 0.0500 (0.0500)  time: 0.7681  data: 0.1836  max mem: 4509
Epoch: [0]  [1390/3542]  eta: 0:30:48  lr: 0.000016  min_lr: 0.000016  loss: 3.8770 (4.7813)  loss_scale: 8192.0000 (5267.9684)  weight_decay: 0.0500 (0.0500)  time: 0.7381  data: 0.1537  max mem: 4509
Epoch: [0]  [1400/3542]  eta: 0:30:38  lr: 0.000016  min_lr: 0.000016  loss: 3.7891 (4.7743)  loss_scale: 8192.0000 (5288.8394)  weight_decay: 0.0500 (0.0500)  time: 0.7572  data: 0.1714  max mem: 4509
Epoch: [0]  [1410/3542]  eta: 0:30:27  lr: 0.000016  min_lr: 0.000016  loss: 3.8691 (4.7681)  loss_scale: 8192.0000 (5309.4146)  weight_decay: 0.0500 (0.0500)  time: 0.7346  data: 0.1486  max mem: 4509
Epoch: [0]  [1420/3542]  eta: 0:30:17  lr: 0.000016  min_lr: 0.000016  loss: 3.8828 (4.7617)  loss_scale: 8192.0000 (5329.7002)  weight_decay: 0.0500 (0.0500)  time: 0.7323  data: 0.1477  max mem: 4509
Epoch: [0]  [1430/3542]  eta: 0:30:08  lr: 0.000016  min_lr: 0.000016  loss: 3.7891 (4.7546)  loss_scale: 8192.0000 (5349.7023)  weight_decay: 0.0500 (0.0500)  time: 0.7994  data: 0.2148  max mem: 4509
Epoch: [0]  [1440/3542]  eta: 0:29:57  lr: 0.000016  min_lr: 0.000016  loss: 3.7441 (4.7478)  loss_scale: 8192.0000 (5369.4268)  weight_decay: 0.0500 (0.0500)  time: 0.7645  data: 0.1795  max mem: 4509
Epoch: [0]  [1450/3542]  eta: 0:29:46  lr: 0.000016  min_lr: 0.000016  loss: 3.8477 (4.7416)  loss_scale: 8192.0000 (5388.8794)  weight_decay: 0.0500 (0.0500)  time: 0.6989  data: 0.1141  max mem: 4509
Epoch: [0]  [1460/3542]  eta: 0:29:37  lr: 0.000016  min_lr: 0.000016  loss: 3.8066 (4.7352)  loss_scale: 8192.0000 (5408.0657)  weight_decay: 0.0500 (0.0500)  time: 0.7372  data: 0.1526  max mem: 4509
Epoch: [0]  [1470/3542]  eta: 0:29:26  lr: 0.000017  min_lr: 0.000017  loss: 3.7637 (4.7286)  loss_scale: 8192.0000 (5426.9912)  weight_decay: 0.0500 (0.0500)  time: 0.7426  data: 0.1574  max mem: 4509
Epoch: [0]  [1480/3542]  eta: 0:29:16  lr: 0.000017  min_lr: 0.000017  loss: 3.7676 (4.7229)  loss_scale: 8192.0000 (5445.6610)  weight_decay: 0.0500 (0.0500)  time: 0.7298  data: 0.1447  max mem: 4509
Epoch: [0]  [1490/3542]  eta: 0:29:07  lr: 0.000017  min_lr: 0.000017  loss: 3.8008 (4.7168)  loss_scale: 8192.0000 (5464.0805)  weight_decay: 0.0500 (0.0500)  time: 0.7755  data: 0.1902  max mem: 4509
Epoch: [0]  [1500/3542]  eta: 0:28:57  lr: 0.000017  min_lr: 0.000017  loss: 3.8555 (4.7103)  loss_scale: 8192.0000 (5482.2545)  weight_decay: 0.0500 (0.0500)  time: 0.7637  data: 0.1782  max mem: 4509
Epoch: [0]  [1510/3542]  eta: 0:28:47  lr: 0.000017  min_lr: 0.000017  loss: 3.7168 (4.7035)  loss_scale: 8192.0000 (5500.1880)  weight_decay: 0.0500 (0.0500)  time: 0.7245  data: 0.1400  max mem: 4509
Epoch: [0]  [1520/3542]  eta: 0:28:36  lr: 0.000017  min_lr: 0.000017  loss: 3.6328 (4.6975)  loss_scale: 8192.0000 (5517.8856)  weight_decay: 0.0500 (0.0500)  time: 0.7166  data: 0.1315  max mem: 4509
Epoch: [0]  [1530/3542]  eta: 0:28:26  lr: 0.000017  min_lr: 0.000017  loss: 3.6172 (4.6904)  loss_scale: 8192.0000 (5535.3521)  weight_decay: 0.0500 (0.0500)  time: 0.7155  data: 0.1299  max mem: 4509
Epoch: [0]  [1540/3542]  eta: 0:28:17  lr: 0.000017  min_lr: 0.000017  loss: 3.6289 (4.6839)  loss_scale: 8192.0000 (5552.5918)  weight_decay: 0.0500 (0.0500)  time: 0.7458  data: 0.1612  max mem: 4509
Epoch: [0]  [1550/3542]  eta: 0:28:07  lr: 0.000018  min_lr: 0.000018  loss: 3.6758 (4.6771)  loss_scale: 8192.0000 (5569.6093)  weight_decay: 0.0500 (0.0500)  time: 0.7527  data: 0.1675  max mem: 4509
Epoch: [0]  [1560/3542]  eta: 0:27:56  lr: 0.000018  min_lr: 0.000018  loss: 3.6426 (4.6704)  loss_scale: 8192.0000 (5586.4087)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.1332  max mem: 4509
Epoch: [0]  [1570/3542]  eta: 0:27:46  lr: 0.000018  min_lr: 0.000018  loss: 3.5859 (4.6634)  loss_scale: 8192.0000 (5602.9943)  weight_decay: 0.0500 (0.0500)  time: 0.7118  data: 0.1263  max mem: 4509
Epoch: [0]  [1580/3542]  eta: 0:27:37  lr: 0.000018  min_lr: 0.000018  loss: 3.5801 (4.6568)  loss_scale: 8192.0000 (5619.3700)  weight_decay: 0.0500 (0.0500)  time: 0.7424  data: 0.1569  max mem: 4509
Epoch: [0]  [1590/3542]  eta: 0:27:28  lr: 0.000018  min_lr: 0.000018  loss: 3.6172 (4.6501)  loss_scale: 8192.0000 (5635.5399)  weight_decay: 0.0500 (0.0500)  time: 0.7712  data: 0.1860  max mem: 4509
Epoch: [0]  [1600/3542]  eta: 0:27:18  lr: 0.000018  min_lr: 0.000018  loss: 3.6172 (4.6438)  loss_scale: 8192.0000 (5651.5078)  weight_decay: 0.0500 (0.0500)  time: 0.7493  data: 0.1645  max mem: 4509
Epoch: [0]  [1610/3542]  eta: 0:27:08  lr: 0.000018  min_lr: 0.000018  loss: 3.5762 (4.6370)  loss_scale: 8192.0000 (5667.2775)  weight_decay: 0.0500 (0.0500)  time: 0.7349  data: 0.1504  max mem: 4509
Epoch: [0]  [1620/3542]  eta: 0:26:58  lr: 0.000018  min_lr: 0.000018  loss: 3.5762 (4.6309)  loss_scale: 8192.0000 (5682.8526)  weight_decay: 0.0500 (0.0500)  time: 0.7289  data: 0.1440  max mem: 4509
Epoch: [0]  [1630/3542]  eta: 0:26:49  lr: 0.000018  min_lr: 0.000018  loss: 3.6738 (4.6252)  loss_scale: 8192.0000 (5698.2367)  weight_decay: 0.0500 (0.0500)  time: 0.7467  data: 0.1623  max mem: 4509
Epoch: [0]  [1640/3542]  eta: 0:26:40  lr: 0.000019  min_lr: 0.000019  loss: 3.6719 (4.6194)  loss_scale: 8192.0000 (5713.4333)  weight_decay: 0.0500 (0.0500)  time: 0.7969  data: 0.2121  max mem: 4509
Epoch: [0]  [1650/3542]  eta: 0:26:30  lr: 0.000019  min_lr: 0.000019  loss: 3.6113 (4.6136)  loss_scale: 8192.0000 (5728.4458)  weight_decay: 0.0500 (0.0500)  time: 0.7419  data: 0.1559  max mem: 4509
Epoch: [0]  [1660/3542]  eta: 0:26:20  lr: 0.000019  min_lr: 0.000019  loss: 3.6719 (4.6077)  loss_scale: 8192.0000 (5743.2775)  weight_decay: 0.0500 (0.0500)  time: 0.7138  data: 0.1279  max mem: 4509
Epoch: [0]  [1670/3542]  eta: 0:26:11  lr: 0.000019  min_lr: 0.000019  loss: 3.6191 (4.6016)  loss_scale: 8192.0000 (5757.9318)  weight_decay: 0.0500 (0.0500)  time: 0.7415  data: 0.1562  max mem: 4509
Epoch: [0]  [1680/3542]  eta: 0:26:02  lr: 0.000019  min_lr: 0.000019  loss: 3.5234 (4.5953)  loss_scale: 8192.0000 (5772.4117)  weight_decay: 0.0500 (0.0500)  time: 0.7473  data: 0.1626  max mem: 4509
Epoch: [0]  [1690/3542]  eta: 0:25:52  lr: 0.000019  min_lr: 0.000019  loss: 3.5449 (4.5894)  loss_scale: 8192.0000 (5786.7203)  weight_decay: 0.0500 (0.0500)  time: 0.7390  data: 0.1547  max mem: 4509
Epoch: [0]  [1700/3542]  eta: 0:25:42  lr: 0.000019  min_lr: 0.000019  loss: 3.4805 (4.5828)  loss_scale: 8192.0000 (5800.8607)  weight_decay: 0.0500 (0.0500)  time: 0.7338  data: 0.1501  max mem: 4509
Epoch: [0]  [1710/3542]  eta: 0:25:33  lr: 0.000019  min_lr: 0.000019  loss: 3.4805 (4.5769)  loss_scale: 8192.0000 (5814.8358)  weight_decay: 0.0500 (0.0500)  time: 0.7502  data: 0.1659  max mem: 4509
Epoch: [0]  [1720/3542]  eta: 0:25:24  lr: 0.000019  min_lr: 0.000019  loss: 3.5488 (4.5708)  loss_scale: 8192.0000 (5828.6485)  weight_decay: 0.0500 (0.0500)  time: 0.7507  data: 0.1656  max mem: 4509
Epoch: [0]  [1730/3542]  eta: 0:25:14  lr: 0.000020  min_lr: 0.000020  loss: 3.5020 (4.5647)  loss_scale: 8192.0000 (5842.3016)  weight_decay: 0.0500 (0.0500)  time: 0.7396  data: 0.1548  max mem: 4509
Epoch: [0]  [1740/3542]  eta: 0:25:05  lr: 0.000020  min_lr: 0.000020  loss: 3.6465 (4.5599)  loss_scale: 8192.0000 (5855.7978)  weight_decay: 0.0500 (0.0500)  time: 0.7244  data: 0.1395  max mem: 4509
Epoch: [0]  [1750/3542]  eta: 0:24:56  lr: 0.000020  min_lr: 0.000020  loss: 3.6738 (4.5545)  loss_scale: 8192.0000 (5869.1399)  weight_decay: 0.0500 (0.0500)  time: 0.7311  data: 0.1463  max mem: 4509
Epoch: [0]  [1760/3542]  eta: 0:24:46  lr: 0.000020  min_lr: 0.000020  loss: 3.6738 (4.5493)  loss_scale: 8192.0000 (5882.3305)  weight_decay: 0.0500 (0.0500)  time: 0.7548  data: 0.1702  max mem: 4509
Epoch: [0]  [1770/3542]  eta: 0:24:37  lr: 0.000020  min_lr: 0.000020  loss: 3.6289 (4.5437)  loss_scale: 8192.0000 (5895.3721)  weight_decay: 0.0500 (0.0500)  time: 0.7319  data: 0.1469  max mem: 4509
Epoch: [0]  [1780/3542]  eta: 0:24:28  lr: 0.000020  min_lr: 0.000020  loss: 3.5645 (4.5379)  loss_scale: 8192.0000 (5908.2673)  weight_decay: 0.0500 (0.0500)  time: 0.7416  data: 0.1566  max mem: 4509
Epoch: [0]  [1790/3542]  eta: 0:24:19  lr: 0.000020  min_lr: 0.000020  loss: 3.5391 (4.5321)  loss_scale: 8192.0000 (5921.0184)  weight_decay: 0.0500 (0.0500)  time: 0.7550  data: 0.1706  max mem: 4509
Epoch: [0]  [1800/3542]  eta: 0:24:09  lr: 0.000020  min_lr: 0.000020  loss: 3.4902 (4.5268)  loss_scale: 8192.0000 (5933.6280)  weight_decay: 0.0500 (0.0500)  time: 0.7406  data: 0.1547  max mem: 4509
Epoch: [0]  [1810/3542]  eta: 0:24:00  lr: 0.000020  min_lr: 0.000020  loss: 3.6250 (4.5222)  loss_scale: 8192.0000 (5946.0983)  weight_decay: 0.0500 (0.0500)  time: 0.7498  data: 0.1633  max mem: 4509
Epoch: [0]  [1820/3542]  eta: 0:23:51  lr: 0.000021  min_lr: 0.000021  loss: 3.5586 (4.5166)  loss_scale: 8192.0000 (5958.4316)  weight_decay: 0.0500 (0.0500)  time: 0.7219  data: 0.1361  max mem: 4509
Epoch: [0]  [1830/3542]  eta: 0:23:42  lr: 0.000021  min_lr: 0.000021  loss: 3.4316 (4.5107)  loss_scale: 8192.0000 (5970.6303)  weight_decay: 0.0500 (0.0500)  time: 0.7363  data: 0.1508  max mem: 4509
Epoch: [0]  [1840/3542]  eta: 0:23:33  lr: 0.000021  min_lr: 0.000021  loss: 3.4883 (4.5052)  loss_scale: 8192.0000 (5982.6964)  weight_decay: 0.0500 (0.0500)  time: 0.7906  data: 0.2048  max mem: 4509
Epoch: [0]  [1850/3542]  eta: 0:23:25  lr: 0.000021  min_lr: 0.000021  loss: 3.5059 (4.4998)  loss_scale: 8192.0000 (5994.6321)  weight_decay: 0.0500 (0.0500)  time: 0.8011  data: 0.2148  max mem: 4509
Epoch: [0]  [1860/3542]  eta: 0:23:15  lr: 0.000021  min_lr: 0.000021  loss: 3.4551 (4.4943)  loss_scale: 8192.0000 (6006.4395)  weight_decay: 0.0500 (0.0500)  time: 0.7453  data: 0.1592  max mem: 4509
Epoch: [0]  [1870/3542]  eta: 0:23:06  lr: 0.000021  min_lr: 0.000021  loss: 3.4980 (4.4893)  loss_scale: 8192.0000 (6018.1208)  weight_decay: 0.0500 (0.0500)  time: 0.7288  data: 0.1434  max mem: 4509
Epoch: [0]  [1880/3542]  eta: 0:22:57  lr: 0.000021  min_lr: 0.000021  loss: 3.4980 (4.4839)  loss_scale: 8192.0000 (6029.6778)  weight_decay: 0.0500 (0.0500)  time: 0.7549  data: 0.1702  max mem: 4509
Epoch: [0]  [1890/3542]  eta: 0:22:48  lr: 0.000021  min_lr: 0.000021  loss: 3.5039 (4.4788)  loss_scale: 8192.0000 (6041.1126)  weight_decay: 0.0500 (0.0500)  time: 0.7459  data: 0.1603  max mem: 4509
Epoch: [0]  [1900/3542]  eta: 0:22:39  lr: 0.000021  min_lr: 0.000021  loss: 3.5039 (4.4739)  loss_scale: 8192.0000 (6052.4271)  weight_decay: 0.0500 (0.0500)  time: 0.7444  data: 0.1591  max mem: 4509
Epoch: [0]  [1910/3542]  eta: 0:22:30  lr: 0.000022  min_lr: 0.000022  loss: 3.4648 (4.4684)  loss_scale: 8192.0000 (6063.6232)  weight_decay: 0.0500 (0.0500)  time: 0.7328  data: 0.1471  max mem: 4509
Epoch: [0]  [1920/3542]  eta: 0:22:21  lr: 0.000022  min_lr: 0.000022  loss: 3.4121 (4.4631)  loss_scale: 8192.0000 (6074.7028)  weight_decay: 0.0500 (0.0500)  time: 0.7518  data: 0.1659  max mem: 4509
Epoch: [0]  [1930/3542]  eta: 0:22:13  lr: 0.000022  min_lr: 0.000022  loss: 3.4551 (4.4577)  loss_scale: 8192.0000 (6085.6675)  weight_decay: 0.0500 (0.0500)  time: 0.7740  data: 0.1890  max mem: 4509
Epoch: [0]  [1940/3542]  eta: 0:22:04  lr: 0.000022  min_lr: 0.000022  loss: 3.4922 (4.4527)  loss_scale: 8192.0000 (6096.5193)  weight_decay: 0.0500 (0.0500)  time: 0.7610  data: 0.1759  max mem: 4509
Epoch: [0]  [1950/3542]  eta: 0:21:55  lr: 0.000022  min_lr: 0.000022  loss: 3.4922 (4.4476)  loss_scale: 8192.0000 (6107.2599)  weight_decay: 0.0500 (0.0500)  time: 0.7732  data: 0.1875  max mem: 4509
Epoch: [0]  [1960/3542]  eta: 0:21:47  lr: 0.000022  min_lr: 0.000022  loss: 3.4375 (4.4427)  loss_scale: 8192.0000 (6117.8909)  weight_decay: 0.0500 (0.0500)  time: 0.7925  data: 0.2068  max mem: 4509
Epoch: [0]  [1970/3542]  eta: 0:21:38  lr: 0.000022  min_lr: 0.000022  loss: 3.3691 (4.4373)  loss_scale: 8192.0000 (6128.4140)  weight_decay: 0.0500 (0.0500)  time: 0.7660  data: 0.1804  max mem: 4509
Epoch: [0]  [1980/3542]  eta: 0:21:29  lr: 0.000022  min_lr: 0.000022  loss: 3.3945 (4.4326)  loss_scale: 8192.0000 (6138.8309)  weight_decay: 0.0500 (0.0500)  time: 0.7228  data: 0.1374  max mem: 4509
Epoch: [0]  [1990/3542]  eta: 0:21:20  lr: 0.000022  min_lr: 0.000022  loss: 3.4648 (4.4281)  loss_scale: 8192.0000 (6149.1431)  weight_decay: 0.0500 (0.0500)  time: 0.7472  data: 0.1607  max mem: 4509
[2023-05-16 05:53:06,430] [INFO] [logging.py:60:log_dist] [Rank 0] step=2000, skipped=4, lr=[2.2581191753741882e-05, 2.2581191753741882e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 05:53:06,433] [INFO] [timer.py:157:stop] 0/2000, SamplesPerSec=55.62683386551065
Epoch: [0]  [2000/3542]  eta: 0:21:11  lr: 0.000023  min_lr: 0.000023  loss: 3.4355 (4.4230)  loss_scale: 8192.0000 (6159.3523)  weight_decay: 0.0500 (0.0500)  time: 0.7737  data: 0.1869  max mem: 4509
[2023-05-16 05:53:11,584] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 05:53:11,585] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [0]  [2010/3542]  eta: 0:21:02  lr: 0.000023  min_lr: 0.000023  loss: 3.4414 (4.4188)  loss_scale: 8192.0000 (6193.9015)  weight_decay: 0.0500 (0.0500)  time: 0.7566  data: 0.1709  max mem: 4509
Epoch: [0]  [2020/3542]  eta: 0:20:54  lr: 0.000023  min_lr: 0.000023  loss: 3.5293 (4.4144)  loss_scale: 16384.0000 (6244.3226)  weight_decay: 0.0500 (0.0500)  time: 0.7449  data: 0.1593  max mem: 4509
Epoch: [0]  [2030/3542]  eta: 0:20:45  lr: 0.000023  min_lr: 0.000023  loss: 3.5293 (4.4099)  loss_scale: 16384.0000 (6294.2472)  weight_decay: 0.0500 (0.0500)  time: 0.7319  data: 0.1455  max mem: 4509
[2023-05-16 05:53:30,334] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 2032
[2023-05-16 05:53:30,335] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 05:53:30,335] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [0]  [2040/3542]  eta: 0:20:36  lr: 0.000023  min_lr: 0.000023  loss: 3.5098 (4.4055)  loss_scale: 16384.0000 (6307.5590)  weight_decay: 0.0500 (0.0500)  time: 0.7410  data: 0.1546  max mem: 4509
Epoch: [0]  [2050/3542]  eta: 0:20:27  lr: 0.000023  min_lr: 0.000023  loss: 3.4863 (4.4016)  loss_scale: 8192.0000 (6316.7470)  weight_decay: 0.0500 (0.0500)  time: 0.7420  data: 0.1567  max mem: 4509
Epoch: [0]  [2060/3542]  eta: 0:20:19  lr: 0.000023  min_lr: 0.000023  loss: 3.4785 (4.3968)  loss_scale: 8192.0000 (6325.8457)  weight_decay: 0.0500 (0.0500)  time: 0.8005  data: 0.2141  max mem: 4509
Epoch: [0]  [2070/3542]  eta: 0:20:10  lr: 0.000023  min_lr: 0.000023  loss: 3.3711 (4.3920)  loss_scale: 8192.0000 (6334.8566)  weight_decay: 0.0500 (0.0500)  time: 0.7775  data: 0.1905  max mem: 4509
Epoch: [0]  [2080/3542]  eta: 0:20:01  lr: 0.000023  min_lr: 0.000023  loss: 3.4180 (4.3874)  loss_scale: 8192.0000 (6343.7809)  weight_decay: 0.0500 (0.0500)  time: 0.7186  data: 0.1325  max mem: 4509
Epoch: [0]  [2090/3542]  eta: 0:19:52  lr: 0.000024  min_lr: 0.000024  loss: 3.4473 (4.3828)  loss_scale: 8192.0000 (6352.6198)  weight_decay: 0.0500 (0.0500)  time: 0.7563  data: 0.1708  max mem: 4509
Epoch: [0]  [2100/3542]  eta: 0:19:44  lr: 0.000024  min_lr: 0.000024  loss: 3.4785 (4.3787)  loss_scale: 8192.0000 (6361.3746)  weight_decay: 0.0500 (0.0500)  time: 0.7782  data: 0.1922  max mem: 4509
Epoch: [0]  [2110/3542]  eta: 0:19:35  lr: 0.000024  min_lr: 0.000024  loss: 3.4355 (4.3743)  loss_scale: 8192.0000 (6370.0464)  weight_decay: 0.0500 (0.0500)  time: 0.7703  data: 0.1841  max mem: 4509
Epoch: [0]  [2120/3542]  eta: 0:19:27  lr: 0.000024  min_lr: 0.000024  loss: 3.3887 (4.3699)  loss_scale: 8192.0000 (6378.6365)  weight_decay: 0.0500 (0.0500)  time: 0.7471  data: 0.1609  max mem: 4509
Epoch: [0]  [2130/3542]  eta: 0:19:18  lr: 0.000024  min_lr: 0.000024  loss: 3.4434 (4.3658)  loss_scale: 8192.0000 (6387.1459)  weight_decay: 0.0500 (0.0500)  time: 0.7717  data: 0.1856  max mem: 4509
Epoch: [0]  [2140/3542]  eta: 0:19:09  lr: 0.000024  min_lr: 0.000024  loss: 3.4316 (4.3612)  loss_scale: 8192.0000 (6395.5759)  weight_decay: 0.0500 (0.0500)  time: 0.7504  data: 0.1647  max mem: 4509
Epoch: [0]  [2150/3542]  eta: 0:19:01  lr: 0.000024  min_lr: 0.000024  loss: 3.3301 (4.3564)  loss_scale: 8192.0000 (6403.9275)  weight_decay: 0.0500 (0.0500)  time: 0.7263  data: 0.1410  max mem: 4509
Epoch: [0]  [2160/3542]  eta: 0:18:52  lr: 0.000024  min_lr: 0.000024  loss: 3.3145 (4.3521)  loss_scale: 8192.0000 (6412.2018)  weight_decay: 0.0500 (0.0500)  time: 0.7270  data: 0.1417  max mem: 4509
Epoch: [0]  [2170/3542]  eta: 0:18:43  lr: 0.000025  min_lr: 0.000025  loss: 3.4590 (4.3478)  loss_scale: 8192.0000 (6420.3998)  weight_decay: 0.0500 (0.0500)  time: 0.7372  data: 0.1509  max mem: 4509
Epoch: [0]  [2180/3542]  eta: 0:18:34  lr: 0.000025  min_lr: 0.000025  loss: 3.3535 (4.3432)  loss_scale: 8192.0000 (6428.5227)  weight_decay: 0.0500 (0.0500)  time: 0.7515  data: 0.1647  max mem: 4509
Epoch: [0]  [2190/3542]  eta: 0:18:26  lr: 0.000025  min_lr: 0.000025  loss: 3.4004 (4.3392)  loss_scale: 8192.0000 (6436.5714)  weight_decay: 0.0500 (0.0500)  time: 0.7496  data: 0.1639  max mem: 4509
Epoch: [0]  [2200/3542]  eta: 0:18:18  lr: 0.000025  min_lr: 0.000025  loss: 3.4492 (4.3348)  loss_scale: 8192.0000 (6444.5470)  weight_decay: 0.0500 (0.0500)  time: 0.7909  data: 0.2049  max mem: 4509
Epoch: [0]  [2210/3542]  eta: 0:18:09  lr: 0.000025  min_lr: 0.000025  loss: 3.3789 (4.3307)  loss_scale: 8192.0000 (6452.4505)  weight_decay: 0.0500 (0.0500)  time: 0.7553  data: 0.1697  max mem: 4509
Epoch: [0]  [2220/3542]  eta: 0:18:00  lr: 0.000025  min_lr: 0.000025  loss: 3.4043 (4.3264)  loss_scale: 8192.0000 (6460.2828)  weight_decay: 0.0500 (0.0500)  time: 0.7361  data: 0.1516  max mem: 4509
Epoch: [0]  [2230/3542]  eta: 0:17:52  lr: 0.000025  min_lr: 0.000025  loss: 3.4121 (4.3225)  loss_scale: 8192.0000 (6468.0448)  weight_decay: 0.0500 (0.0500)  time: 0.7504  data: 0.1662  max mem: 4509
Epoch: [0]  [2240/3542]  eta: 0:17:43  lr: 0.000025  min_lr: 0.000025  loss: 3.4199 (4.3183)  loss_scale: 8192.0000 (6475.7376)  weight_decay: 0.0500 (0.0500)  time: 0.7457  data: 0.1609  max mem: 4509
Epoch: [0]  [2250/3542]  eta: 0:17:35  lr: 0.000025  min_lr: 0.000025  loss: 3.3809 (4.3142)  loss_scale: 8192.0000 (6483.3621)  weight_decay: 0.0500 (0.0500)  time: 0.7618  data: 0.1761  max mem: 4509
Epoch: [0]  [2260/3542]  eta: 0:17:26  lr: 0.000026  min_lr: 0.000026  loss: 3.3535 (4.3099)  loss_scale: 8192.0000 (6490.9191)  weight_decay: 0.0500 (0.0500)  time: 0.7631  data: 0.1785  max mem: 4509
Epoch: [0]  [2270/3542]  eta: 0:17:18  lr: 0.000026  min_lr: 0.000026  loss: 3.3535 (4.3058)  loss_scale: 8192.0000 (6498.4095)  weight_decay: 0.0500 (0.0500)  time: 0.7538  data: 0.1689  max mem: 4509
Epoch: [0]  [2280/3542]  eta: 0:17:09  lr: 0.000026  min_lr: 0.000026  loss: 3.4043 (4.3016)  loss_scale: 8192.0000 (6505.8343)  weight_decay: 0.0500 (0.0500)  time: 0.7278  data: 0.1423  max mem: 4509
Epoch: [0]  [2290/3542]  eta: 0:17:01  lr: 0.000026  min_lr: 0.000026  loss: 3.3633 (4.2974)  loss_scale: 8192.0000 (6513.1942)  weight_decay: 0.0500 (0.0500)  time: 0.7579  data: 0.1723  max mem: 4509
Epoch: [0]  [2300/3542]  eta: 0:16:52  lr: 0.000026  min_lr: 0.000026  loss: 3.3711 (4.2936)  loss_scale: 8192.0000 (6520.4902)  weight_decay: 0.0500 (0.0500)  time: 0.7854  data: 0.1995  max mem: 4509
Epoch: [0]  [2310/3542]  eta: 0:16:44  lr: 0.000026  min_lr: 0.000026  loss: 3.5117 (4.2902)  loss_scale: 8192.0000 (6527.7231)  weight_decay: 0.0500 (0.0500)  time: 0.7657  data: 0.1811  max mem: 4509
Epoch: [0]  [2320/3542]  eta: 0:16:35  lr: 0.000026  min_lr: 0.000026  loss: 3.4941 (4.2864)  loss_scale: 8192.0000 (6534.8936)  weight_decay: 0.0500 (0.0500)  time: 0.7263  data: 0.1423  max mem: 4509
Epoch: [0]  [2330/3542]  eta: 0:16:26  lr: 0.000026  min_lr: 0.000026  loss: 3.3574 (4.2826)  loss_scale: 8192.0000 (6542.0026)  weight_decay: 0.0500 (0.0500)  time: 0.7040  data: 0.1197  max mem: 4509
Epoch: [0]  [2340/3542]  eta: 0:16:18  lr: 0.000026  min_lr: 0.000026  loss: 3.3555 (4.2786)  loss_scale: 8192.0000 (6549.0508)  weight_decay: 0.0500 (0.0500)  time: 0.7399  data: 0.1548  max mem: 4509
Epoch: [0]  [2350/3542]  eta: 0:16:10  lr: 0.000027  min_lr: 0.000027  loss: 3.3555 (4.2747)  loss_scale: 8192.0000 (6556.0391)  weight_decay: 0.0500 (0.0500)  time: 0.7856  data: 0.2002  max mem: 4509
Epoch: [0]  [2360/3542]  eta: 0:16:01  lr: 0.000027  min_lr: 0.000027  loss: 3.3457 (4.2709)  loss_scale: 8192.0000 (6562.9682)  weight_decay: 0.0500 (0.0500)  time: 0.7371  data: 0.1520  max mem: 4509
Epoch: [0]  [2370/3542]  eta: 0:15:53  lr: 0.000027  min_lr: 0.000027  loss: 3.3496 (4.2675)  loss_scale: 8192.0000 (6569.8389)  weight_decay: 0.0500 (0.0500)  time: 0.7170  data: 0.1319  max mem: 4509
Epoch: [0]  [2380/3542]  eta: 0:15:44  lr: 0.000027  min_lr: 0.000027  loss: 3.4062 (4.2637)  loss_scale: 8192.0000 (6576.6518)  weight_decay: 0.0500 (0.0500)  time: 0.7340  data: 0.1490  max mem: 4509
Epoch: [0]  [2390/3542]  eta: 0:15:36  lr: 0.000027  min_lr: 0.000027  loss: 3.3203 (4.2599)  loss_scale: 8192.0000 (6583.4078)  weight_decay: 0.0500 (0.0500)  time: 0.7347  data: 0.1487  max mem: 4509
Epoch: [0]  [2400/3542]  eta: 0:15:27  lr: 0.000027  min_lr: 0.000027  loss: 3.3672 (4.2565)  loss_scale: 8192.0000 (6590.1075)  weight_decay: 0.0500 (0.0500)  time: 0.7493  data: 0.1633  max mem: 4509
Epoch: [0]  [2410/3542]  eta: 0:15:18  lr: 0.000027  min_lr: 0.000027  loss: 3.3750 (4.2529)  loss_scale: 8192.0000 (6596.7516)  weight_decay: 0.0500 (0.0500)  time: 0.7227  data: 0.1376  max mem: 4509
Epoch: [0]  [2420/3542]  eta: 0:15:10  lr: 0.000027  min_lr: 0.000027  loss: 3.3047 (4.2491)  loss_scale: 8192.0000 (6603.3408)  weight_decay: 0.0500 (0.0500)  time: 0.7239  data: 0.1389  max mem: 4509
Epoch: [0]  [2430/3542]  eta: 0:15:01  lr: 0.000027  min_lr: 0.000027  loss: 3.3867 (4.2459)  loss_scale: 8192.0000 (6609.8758)  weight_decay: 0.0500 (0.0500)  time: 0.7131  data: 0.1279  max mem: 4509
Epoch: [0]  [2440/3542]  eta: 0:14:53  lr: 0.000028  min_lr: 0.000028  loss: 3.4199 (4.2421)  loss_scale: 8192.0000 (6616.3572)  weight_decay: 0.0500 (0.0500)  time: 0.7294  data: 0.1436  max mem: 4509
Epoch: [0]  [2450/3542]  eta: 0:14:45  lr: 0.000028  min_lr: 0.000028  loss: 3.3223 (4.2385)  loss_scale: 8192.0000 (6622.7858)  weight_decay: 0.0500 (0.0500)  time: 0.7876  data: 0.2019  max mem: 4509
Epoch: [0]  [2460/3542]  eta: 0:14:36  lr: 0.000028  min_lr: 0.000028  loss: 3.3262 (4.2349)  loss_scale: 8192.0000 (6629.1621)  weight_decay: 0.0500 (0.0500)  time: 0.7673  data: 0.1821  max mem: 4509
Epoch: [0]  [2470/3542]  eta: 0:14:28  lr: 0.000028  min_lr: 0.000028  loss: 3.3379 (4.2312)  loss_scale: 8192.0000 (6635.4868)  weight_decay: 0.0500 (0.0500)  time: 0.7793  data: 0.1945  max mem: 4509
Epoch: [0]  [2480/3542]  eta: 0:14:20  lr: 0.000028  min_lr: 0.000028  loss: 3.3457 (4.2280)  loss_scale: 8192.0000 (6641.7606)  weight_decay: 0.0500 (0.0500)  time: 0.7649  data: 0.1797  max mem: 4509
Epoch: [0]  [2490/3542]  eta: 0:14:12  lr: 0.000028  min_lr: 0.000028  loss: 3.3594 (4.2246)  loss_scale: 8192.0000 (6647.9839)  weight_decay: 0.0500 (0.0500)  time: 0.7393  data: 0.1545  max mem: 4509
Epoch: [0]  [2500/3542]  eta: 0:14:03  lr: 0.000028  min_lr: 0.000028  loss: 3.3086 (4.2210)  loss_scale: 8192.0000 (6654.1575)  weight_decay: 0.0500 (0.0500)  time: 0.7633  data: 0.1790  max mem: 4509
Epoch: [0]  [2510/3542]  eta: 0:13:55  lr: 0.000028  min_lr: 0.000028  loss: 3.3086 (4.2175)  loss_scale: 8192.0000 (6660.2820)  weight_decay: 0.0500 (0.0500)  time: 0.7399  data: 0.1546  max mem: 4509
Epoch: [0]  [2520/3542]  eta: 0:13:47  lr: 0.000028  min_lr: 0.000028  loss: 3.3457 (4.2140)  loss_scale: 8192.0000 (6666.3578)  weight_decay: 0.0500 (0.0500)  time: 0.7714  data: 0.1868  max mem: 4509
Epoch: [0]  [2530/3542]  eta: 0:13:38  lr: 0.000029  min_lr: 0.000029  loss: 3.2988 (4.2105)  loss_scale: 8192.0000 (6672.3856)  weight_decay: 0.0500 (0.0500)  time: 0.7768  data: 0.1928  max mem: 4509
Epoch: [0]  [2540/3542]  eta: 0:13:30  lr: 0.000029  min_lr: 0.000029  loss: 3.2930 (4.2070)  loss_scale: 8192.0000 (6678.3660)  weight_decay: 0.0500 (0.0500)  time: 0.7502  data: 0.1658  max mem: 4509
Epoch: [0]  [2550/3542]  eta: 0:13:22  lr: 0.000029  min_lr: 0.000029  loss: 3.3574 (4.2037)  loss_scale: 8192.0000 (6684.2995)  weight_decay: 0.0500 (0.0500)  time: 0.8210  data: 0.2366  max mem: 4509
Epoch: [0]  [2560/3542]  eta: 0:13:14  lr: 0.000029  min_lr: 0.000029  loss: 3.3730 (4.2005)  loss_scale: 8192.0000 (6690.1866)  weight_decay: 0.0500 (0.0500)  time: 0.8363  data: 0.2514  max mem: 4509
Epoch: [0]  [2570/3542]  eta: 0:13:06  lr: 0.000029  min_lr: 0.000029  loss: 3.3125 (4.1969)  loss_scale: 8192.0000 (6696.0280)  weight_decay: 0.0500 (0.0500)  time: 0.7824  data: 0.1973  max mem: 4509
Epoch: [0]  [2580/3542]  eta: 0:12:58  lr: 0.000029  min_lr: 0.000029  loss: 3.3125 (4.1937)  loss_scale: 8192.0000 (6701.8241)  weight_decay: 0.0500 (0.0500)  time: 0.7783  data: 0.1939  max mem: 4509
Epoch: [0]  [2590/3542]  eta: 0:12:50  lr: 0.000029  min_lr: 0.000029  loss: 3.3340 (4.1903)  loss_scale: 8192.0000 (6707.5755)  weight_decay: 0.0500 (0.0500)  time: 0.8525  data: 0.2675  max mem: 4509
Epoch: [0]  [2600/3542]  eta: 0:12:43  lr: 0.000029  min_lr: 0.000029  loss: 3.3340 (4.1870)  loss_scale: 8192.0000 (6713.2826)  weight_decay: 0.0500 (0.0500)  time: 0.9872  data: 0.4025  max mem: 4509
Epoch: [0]  [2610/3542]  eta: 0:12:35  lr: 0.000029  min_lr: 0.000029  loss: 3.2871 (4.1836)  loss_scale: 8192.0000 (6718.9460)  weight_decay: 0.0500 (0.0500)  time: 1.0046  data: 0.4210  max mem: 4509
Epoch: [0]  [2620/3542]  eta: 0:12:28  lr: 0.000030  min_lr: 0.000030  loss: 3.3398 (4.1803)  loss_scale: 8192.0000 (6724.5662)  weight_decay: 0.0500 (0.0500)  time: 0.9505  data: 0.3662  max mem: 4509
Epoch: [0]  [2630/3542]  eta: 0:12:20  lr: 0.000030  min_lr: 0.000030  loss: 3.2969 (4.1769)  loss_scale: 8192.0000 (6730.1437)  weight_decay: 0.0500 (0.0500)  time: 0.9849  data: 0.4008  max mem: 4509
Epoch: [0]  [2640/3542]  eta: 0:12:13  lr: 0.000030  min_lr: 0.000030  loss: 3.2910 (4.1735)  loss_scale: 8192.0000 (6735.6789)  weight_decay: 0.0500 (0.0500)  time: 1.0009  data: 0.4172  max mem: 4509
Epoch: [0]  [2650/3542]  eta: 0:12:05  lr: 0.000030  min_lr: 0.000030  loss: 3.3047 (4.1704)  loss_scale: 8192.0000 (6741.1724)  weight_decay: 0.0500 (0.0500)  time: 0.9615  data: 0.3779  max mem: 4509
Epoch: [0]  [2660/3542]  eta: 0:11:57  lr: 0.000030  min_lr: 0.000030  loss: 3.2852 (4.1673)  loss_scale: 8192.0000 (6746.6246)  weight_decay: 0.0500 (0.0500)  time: 0.9504  data: 0.3670  max mem: 4509
Epoch: [0]  [2670/3542]  eta: 0:11:50  lr: 0.000030  min_lr: 0.000030  loss: 3.2852 (4.1641)  loss_scale: 8192.0000 (6752.0359)  weight_decay: 0.0500 (0.0500)  time: 0.9366  data: 0.3532  max mem: 4509
Epoch: [0]  [2680/3542]  eta: 0:11:42  lr: 0.000030  min_lr: 0.000030  loss: 3.3184 (4.1610)  loss_scale: 8192.0000 (6757.4069)  weight_decay: 0.0500 (0.0500)  time: 0.9293  data: 0.3461  max mem: 4509
Epoch: [0]  [2690/3542]  eta: 0:11:34  lr: 0.000030  min_lr: 0.000030  loss: 3.3184 (4.1579)  loss_scale: 8192.0000 (6762.7380)  weight_decay: 0.0500 (0.0500)  time: 0.9233  data: 0.3397  max mem: 4509
Epoch: [0]  [2700/3542]  eta: 0:11:26  lr: 0.000030  min_lr: 0.000030  loss: 3.3262 (4.1546)  loss_scale: 8192.0000 (6768.0296)  weight_decay: 0.0500 (0.0500)  time: 0.8672  data: 0.2833  max mem: 4509
Epoch: [0]  [2710/3542]  eta: 0:11:18  lr: 0.000031  min_lr: 0.000031  loss: 3.3262 (4.1512)  loss_scale: 8192.0000 (6773.2822)  weight_decay: 0.0500 (0.0500)  time: 0.8390  data: 0.2551  max mem: 4509
Epoch: [0]  [2720/3542]  eta: 0:11:10  lr: 0.000031  min_lr: 0.000031  loss: 3.3340 (4.1483)  loss_scale: 8192.0000 (6778.4961)  weight_decay: 0.0500 (0.0500)  time: 0.8224  data: 0.2385  max mem: 4509
Epoch: [0]  [2730/3542]  eta: 0:11:01  lr: 0.000031  min_lr: 0.000031  loss: 3.3027 (4.1450)  loss_scale: 8192.0000 (6783.6719)  weight_decay: 0.0500 (0.0500)  time: 0.7759  data: 0.1919  max mem: 4509
Epoch: [0]  [2740/3542]  eta: 0:10:53  lr: 0.000031  min_lr: 0.000031  loss: 3.2109 (4.1418)  loss_scale: 8192.0000 (6788.8099)  weight_decay: 0.0500 (0.0500)  time: 0.7747  data: 0.1905  max mem: 4509
Epoch: [0]  [2750/3542]  eta: 0:10:45  lr: 0.000031  min_lr: 0.000031  loss: 3.2695 (4.1390)  loss_scale: 8192.0000 (6793.9106)  weight_decay: 0.0500 (0.0500)  time: 0.7876  data: 0.2038  max mem: 4509
Epoch: [0]  [2760/3542]  eta: 0:10:37  lr: 0.000031  min_lr: 0.000031  loss: 3.3438 (4.1360)  loss_scale: 8192.0000 (6798.9743)  weight_decay: 0.0500 (0.0500)  time: 0.8285  data: 0.2447  max mem: 4509
Epoch: [0]  [2770/3542]  eta: 0:10:28  lr: 0.000031  min_lr: 0.000031  loss: 3.2168 (4.1328)  loss_scale: 8192.0000 (6804.0014)  weight_decay: 0.0500 (0.0500)  time: 0.8079  data: 0.2240  max mem: 4509
Epoch: [0]  [2780/3542]  eta: 0:10:20  lr: 0.000031  min_lr: 0.000031  loss: 3.2109 (4.1296)  loss_scale: 8192.0000 (6808.9924)  weight_decay: 0.0500 (0.0500)  time: 0.7874  data: 0.2030  max mem: 4509
Epoch: [0]  [2790/3542]  eta: 0:10:12  lr: 0.000032  min_lr: 0.000032  loss: 3.2246 (4.1266)  loss_scale: 8192.0000 (6813.9477)  weight_decay: 0.0500 (0.0500)  time: 0.7858  data: 0.2006  max mem: 4509
Epoch: [0]  [2800/3542]  eta: 0:10:04  lr: 0.000032  min_lr: 0.000032  loss: 3.3086 (4.1237)  loss_scale: 8192.0000 (6818.8675)  weight_decay: 0.0500 (0.0500)  time: 0.7508  data: 0.1663  max mem: 4509
Epoch: [0]  [2810/3542]  eta: 0:09:55  lr: 0.000032  min_lr: 0.000032  loss: 3.3359 (4.1210)  loss_scale: 8192.0000 (6823.7524)  weight_decay: 0.0500 (0.0500)  time: 0.7693  data: 0.1849  max mem: 4509
Epoch: [0]  [2820/3542]  eta: 0:09:47  lr: 0.000032  min_lr: 0.000032  loss: 3.3027 (4.1180)  loss_scale: 8192.0000 (6828.6026)  weight_decay: 0.0500 (0.0500)  time: 0.7941  data: 0.2092  max mem: 4509
Epoch: [0]  [2830/3542]  eta: 0:09:39  lr: 0.000032  min_lr: 0.000032  loss: 3.3184 (4.1152)  loss_scale: 8192.0000 (6833.4186)  weight_decay: 0.0500 (0.0500)  time: 0.7900  data: 0.2059  max mem: 4509
Epoch: [0]  [2840/3542]  eta: 0:09:31  lr: 0.000032  min_lr: 0.000032  loss: 3.3184 (4.1123)  loss_scale: 8192.0000 (6838.2006)  weight_decay: 0.0500 (0.0500)  time: 0.7528  data: 0.1687  max mem: 4509
Epoch: [0]  [2850/3542]  eta: 0:09:23  lr: 0.000032  min_lr: 0.000032  loss: 3.2539 (4.1093)  loss_scale: 8192.0000 (6842.9491)  weight_decay: 0.0500 (0.0500)  time: 0.7835  data: 0.1993  max mem: 4509
Epoch: [0]  [2860/3542]  eta: 0:09:14  lr: 0.000032  min_lr: 0.000032  loss: 3.2793 (4.1066)  loss_scale: 8192.0000 (6847.6645)  weight_decay: 0.0500 (0.0500)  time: 0.7980  data: 0.2133  max mem: 4509
Epoch: [0]  [2870/3542]  eta: 0:09:06  lr: 0.000032  min_lr: 0.000032  loss: 3.3652 (4.1037)  loss_scale: 8192.0000 (6852.3469)  weight_decay: 0.0500 (0.0500)  time: 0.7907  data: 0.2049  max mem: 4509
Epoch: [0]  [2880/3542]  eta: 0:08:58  lr: 0.000033  min_lr: 0.000033  loss: 3.2910 (4.1008)  loss_scale: 8192.0000 (6856.9969)  weight_decay: 0.0500 (0.0500)  time: 0.8359  data: 0.2500  max mem: 4509
Epoch: [0]  [2890/3542]  eta: 0:08:50  lr: 0.000033  min_lr: 0.000033  loss: 3.2812 (4.0981)  loss_scale: 8192.0000 (6861.6147)  weight_decay: 0.0500 (0.0500)  time: 0.8349  data: 0.2501  max mem: 4509
Epoch: [0]  [2900/3542]  eta: 0:08:42  lr: 0.000033  min_lr: 0.000033  loss: 3.3125 (4.0955)  loss_scale: 8192.0000 (6866.2006)  weight_decay: 0.0500 (0.0500)  time: 0.7950  data: 0.2111  max mem: 4509
Epoch: [0]  [2910/3542]  eta: 0:08:34  lr: 0.000033  min_lr: 0.000033  loss: 3.3125 (4.0927)  loss_scale: 8192.0000 (6870.7551)  weight_decay: 0.0500 (0.0500)  time: 0.8002  data: 0.2162  max mem: 4509
Epoch: [0]  [2920/3542]  eta: 0:08:26  lr: 0.000033  min_lr: 0.000033  loss: 3.2500 (4.0897)  loss_scale: 8192.0000 (6875.2783)  weight_decay: 0.0500 (0.0500)  time: 0.8553  data: 0.2707  max mem: 4509
Epoch: [0]  [2930/3542]  eta: 0:08:18  lr: 0.000033  min_lr: 0.000033  loss: 3.1777 (4.0867)  loss_scale: 8192.0000 (6879.7707)  weight_decay: 0.0500 (0.0500)  time: 0.9210  data: 0.3349  max mem: 4509
Epoch: [0]  [2940/3542]  eta: 0:08:10  lr: 0.000033  min_lr: 0.000033  loss: 3.2754 (4.0841)  loss_scale: 8192.0000 (6884.2326)  weight_decay: 0.0500 (0.0500)  time: 0.8608  data: 0.2753  max mem: 4509
Epoch: [0]  [2950/3542]  eta: 0:08:02  lr: 0.000033  min_lr: 0.000033  loss: 3.2754 (4.0813)  loss_scale: 8192.0000 (6888.6642)  weight_decay: 0.0500 (0.0500)  time: 0.7845  data: 0.2008  max mem: 4509
Epoch: [0]  [2960/3542]  eta: 0:07:54  lr: 0.000033  min_lr: 0.000033  loss: 3.2539 (4.0787)  loss_scale: 8192.0000 (6893.0659)  weight_decay: 0.0500 (0.0500)  time: 0.8483  data: 0.2639  max mem: 4509
Epoch: [0]  [2970/3542]  eta: 0:07:45  lr: 0.000034  min_lr: 0.000034  loss: 3.2695 (4.0761)  loss_scale: 8192.0000 (6897.4379)  weight_decay: 0.0500 (0.0500)  time: 0.8647  data: 0.2801  max mem: 4509
Epoch: [0]  [2980/3542]  eta: 0:07:37  lr: 0.000034  min_lr: 0.000034  loss: 3.3770 (4.0740)  loss_scale: 8192.0000 (6901.7806)  weight_decay: 0.0500 (0.0500)  time: 0.8410  data: 0.2566  max mem: 4509
Epoch: [0]  [2990/3542]  eta: 0:07:29  lr: 0.000034  min_lr: 0.000034  loss: 3.3281 (4.0711)  loss_scale: 8192.0000 (6906.0943)  weight_decay: 0.0500 (0.0500)  time: 0.8355  data: 0.2513  max mem: 4509
[2023-05-16 06:06:20,717] [INFO] [logging.py:60:log_dist] [Rank 0] step=3000, skipped=5, lr=[3.387743575261226e-05, 3.387743575261226e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 06:06:20,720] [INFO] [timer.py:157:stop] 0/3000, SamplesPerSec=55.6357889173943
Epoch: [0]  [3000/3542]  eta: 0:07:21  lr: 0.000034  min_lr: 0.000034  loss: 3.2715 (4.0685)  loss_scale: 8192.0000 (6910.3792)  weight_decay: 0.0500 (0.0500)  time: 0.7958  data: 0.2115  max mem: 4509
Epoch: [0]  [3010/3542]  eta: 0:07:13  lr: 0.000034  min_lr: 0.000034  loss: 3.2754 (4.0659)  loss_scale: 8192.0000 (6914.6357)  weight_decay: 0.0500 (0.0500)  time: 0.8041  data: 0.2193  max mem: 4509
Epoch: [0]  [3020/3542]  eta: 0:07:05  lr: 0.000034  min_lr: 0.000034  loss: 3.2734 (4.0634)  loss_scale: 8192.0000 (6918.8640)  weight_decay: 0.0500 (0.0500)  time: 0.8206  data: 0.2362  max mem: 4509
Epoch: [0]  [3030/3542]  eta: 0:06:57  lr: 0.000034  min_lr: 0.000034  loss: 3.2734 (4.0608)  loss_scale: 8192.0000 (6923.0643)  weight_decay: 0.0500 (0.0500)  time: 0.8336  data: 0.2497  max mem: 4509
[2023-05-16 06:06:48,753] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 06:06:48,754] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [0]  [3040/3542]  eta: 0:06:49  lr: 0.000034  min_lr: 0.000034  loss: 3.2324 (4.0583)  loss_scale: 8192.0000 (6948.7879)  weight_decay: 0.0500 (0.0500)  time: 0.8419  data: 0.2577  max mem: 4509
Epoch: [0]  [3050/3542]  eta: 0:06:40  lr: 0.000034  min_lr: 0.000034  loss: 3.2148 (4.0554)  loss_scale: 16384.0000 (6979.7129)  weight_decay: 0.0500 (0.0500)  time: 0.8103  data: 0.2256  max mem: 4509
Epoch: [0]  [3060/3542]  eta: 0:06:32  lr: 0.000035  min_lr: 0.000035  loss: 3.1250 (4.0523)  loss_scale: 16384.0000 (7010.4358)  weight_decay: 0.0500 (0.0500)  time: 0.7716  data: 0.1874  max mem: 4509
Epoch: [0]  [3070/3542]  eta: 0:06:24  lr: 0.000035  min_lr: 0.000035  loss: 3.1406 (4.0496)  loss_scale: 16384.0000 (7040.9586)  weight_decay: 0.0500 (0.0500)  time: 0.7736  data: 0.1901  max mem: 4509
Epoch: [0]  [3080/3542]  eta: 0:06:16  lr: 0.000035  min_lr: 0.000035  loss: 3.1582 (4.0467)  loss_scale: 16384.0000 (7071.2833)  weight_decay: 0.0500 (0.0500)  time: 0.8623  data: 0.2768  max mem: 4509
Epoch: [0]  [3090/3542]  eta: 0:06:08  lr: 0.000035  min_lr: 0.000035  loss: 3.2988 (4.0446)  loss_scale: 16384.0000 (7101.4118)  weight_decay: 0.0500 (0.0500)  time: 0.8429  data: 0.2565  max mem: 4509
Epoch: [0]  [3100/3542]  eta: 0:06:00  lr: 0.000035  min_lr: 0.000035  loss: 3.3750 (4.0423)  loss_scale: 16384.0000 (7131.3460)  weight_decay: 0.0500 (0.0500)  time: 0.8079  data: 0.2224  max mem: 4509
Epoch: [0]  [3110/3542]  eta: 0:05:51  lr: 0.000035  min_lr: 0.000035  loss: 3.3262 (4.0397)  loss_scale: 16384.0000 (7161.0878)  weight_decay: 0.0500 (0.0500)  time: 0.8238  data: 0.2387  max mem: 4509
Epoch: [0]  [3120/3542]  eta: 0:05:43  lr: 0.000035  min_lr: 0.000035  loss: 3.3262 (4.0377)  loss_scale: 16384.0000 (7190.6389)  weight_decay: 0.0500 (0.0500)  time: 0.7596  data: 0.1752  max mem: 4509
Epoch: [0]  [3130/3542]  eta: 0:05:35  lr: 0.000035  min_lr: 0.000035  loss: 3.2559 (4.0350)  loss_scale: 16384.0000 (7220.0013)  weight_decay: 0.0500 (0.0500)  time: 0.7421  data: 0.1578  max mem: 4509
Epoch: [0]  [3140/3542]  eta: 0:05:27  lr: 0.000035  min_lr: 0.000035  loss: 3.2402 (4.0326)  loss_scale: 16384.0000 (7249.1767)  weight_decay: 0.0500 (0.0500)  time: 0.7851  data: 0.1997  max mem: 4509
Epoch: [0]  [3150/3542]  eta: 0:05:19  lr: 0.000036  min_lr: 0.000036  loss: 3.2402 (4.0300)  loss_scale: 16384.0000 (7278.1669)  weight_decay: 0.0500 (0.0500)  time: 0.8329  data: 0.2484  max mem: 4509
Epoch: [0]  [3160/3542]  eta: 0:05:11  lr: 0.000036  min_lr: 0.000036  loss: 3.1367 (4.0271)  loss_scale: 16384.0000 (7306.9737)  weight_decay: 0.0500 (0.0500)  time: 0.8236  data: 0.2402  max mem: 4509
Epoch: [0]  [3170/3542]  eta: 0:05:02  lr: 0.000036  min_lr: 0.000036  loss: 3.1309 (4.0245)  loss_scale: 16384.0000 (7335.5989)  weight_decay: 0.0500 (0.0500)  time: 0.8436  data: 0.2598  max mem: 4509
Epoch: [0]  [3180/3542]  eta: 0:04:54  lr: 0.000036  min_lr: 0.000036  loss: 3.1602 (4.0220)  loss_scale: 16384.0000 (7364.0440)  weight_decay: 0.0500 (0.0500)  time: 0.8596  data: 0.2757  max mem: 4509
Epoch: [0]  [3190/3542]  eta: 0:04:46  lr: 0.000036  min_lr: 0.000036  loss: 3.2578 (4.0197)  loss_scale: 16384.0000 (7392.3109)  weight_decay: 0.0500 (0.0500)  time: 0.8565  data: 0.2705  max mem: 4509
Epoch: [0]  [3200/3542]  eta: 0:04:38  lr: 0.000036  min_lr: 0.000036  loss: 3.2910 (4.0173)  loss_scale: 16384.0000 (7420.4011)  weight_decay: 0.0500 (0.0500)  time: 0.8943  data: 0.3078  max mem: 4509
Epoch: [0]  [3210/3542]  eta: 0:04:30  lr: 0.000036  min_lr: 0.000036  loss: 3.1582 (4.0148)  loss_scale: 16384.0000 (7448.3164)  weight_decay: 0.0500 (0.0500)  time: 0.9360  data: 0.3517  max mem: 4509
Epoch: [0]  [3220/3542]  eta: 0:04:22  lr: 0.000036  min_lr: 0.000036  loss: 3.1543 (4.0121)  loss_scale: 16384.0000 (7476.0584)  weight_decay: 0.0500 (0.0500)  time: 0.9905  data: 0.4066  max mem: 4509
Epoch: [0]  [3230/3542]  eta: 0:04:14  lr: 0.000036  min_lr: 0.000036  loss: 3.2285 (4.0101)  loss_scale: 16384.0000 (7503.6286)  weight_decay: 0.0500 (0.0500)  time: 1.0115  data: 0.4281  max mem: 4509
Epoch: [0]  [3240/3542]  eta: 0:04:06  lr: 0.000037  min_lr: 0.000037  loss: 3.2461 (4.0078)  loss_scale: 16384.0000 (7531.0287)  weight_decay: 0.0500 (0.0500)  time: 0.9450  data: 0.3608  max mem: 4509
Epoch: [0]  [3250/3542]  eta: 0:03:58  lr: 0.000037  min_lr: 0.000037  loss: 3.2070 (4.0056)  loss_scale: 16384.0000 (7558.2602)  weight_decay: 0.0500 (0.0500)  time: 0.8992  data: 0.3137  max mem: 4509
Epoch: [0]  [3260/3542]  eta: 0:03:50  lr: 0.000037  min_lr: 0.000037  loss: 3.2207 (4.0031)  loss_scale: 16384.0000 (7585.3247)  weight_decay: 0.0500 (0.0500)  time: 0.9373  data: 0.3533  max mem: 4509
Epoch: [0]  [3270/3542]  eta: 0:03:42  lr: 0.000037  min_lr: 0.000037  loss: 3.2012 (4.0009)  loss_scale: 16384.0000 (7612.2238)  weight_decay: 0.0500 (0.0500)  time: 0.9440  data: 0.3608  max mem: 4509
Epoch: [0]  [3280/3542]  eta: 0:03:34  lr: 0.000037  min_lr: 0.000037  loss: 3.2910 (3.9988)  loss_scale: 16384.0000 (7638.9589)  weight_decay: 0.0500 (0.0500)  time: 0.8583  data: 0.2742  max mem: 4509
Epoch: [0]  [3290/3542]  eta: 0:03:26  lr: 0.000037  min_lr: 0.000037  loss: 3.3125 (3.9966)  loss_scale: 16384.0000 (7665.5314)  weight_decay: 0.0500 (0.0500)  time: 0.8379  data: 0.2522  max mem: 4509
Epoch: [0]  [3300/3542]  eta: 0:03:18  lr: 0.000037  min_lr: 0.000037  loss: 3.3301 (3.9946)  loss_scale: 16384.0000 (7691.9430)  weight_decay: 0.0500 (0.0500)  time: 0.9267  data: 0.3420  max mem: 4509
Epoch: [0]  [3310/3542]  eta: 0:03:09  lr: 0.000037  min_lr: 0.000037  loss: 3.3184 (3.9924)  loss_scale: 16384.0000 (7718.1951)  weight_decay: 0.0500 (0.0500)  time: 0.8783  data: 0.2954  max mem: 4509
Epoch: [0]  [3320/3542]  eta: 0:03:01  lr: 0.000038  min_lr: 0.000038  loss: 3.2363 (3.9901)  loss_scale: 16384.0000 (7744.2891)  weight_decay: 0.0500 (0.0500)  time: 0.8169  data: 0.2332  max mem: 4509
Epoch: [0]  [3330/3542]  eta: 0:02:53  lr: 0.000038  min_lr: 0.000038  loss: 3.2090 (3.9876)  loss_scale: 16384.0000 (7770.2264)  weight_decay: 0.0500 (0.0500)  time: 0.8105  data: 0.2257  max mem: 4509
Epoch: [0]  [3340/3542]  eta: 0:02:45  lr: 0.000038  min_lr: 0.000038  loss: 3.2344 (3.9854)  loss_scale: 16384.0000 (7796.0084)  weight_decay: 0.0500 (0.0500)  time: 0.7709  data: 0.1860  max mem: 4509
Epoch: [0]  [3350/3542]  eta: 0:02:37  lr: 0.000038  min_lr: 0.000038  loss: 3.1973 (3.9831)  loss_scale: 16384.0000 (7821.6365)  weight_decay: 0.0500 (0.0500)  time: 0.7669  data: 0.1828  max mem: 4509
Epoch: [0]  [3360/3542]  eta: 0:02:28  lr: 0.000038  min_lr: 0.000038  loss: 3.1758 (3.9809)  loss_scale: 16384.0000 (7847.1122)  weight_decay: 0.0500 (0.0500)  time: 0.7986  data: 0.2153  max mem: 4509
Epoch: [0]  [3370/3542]  eta: 0:02:20  lr: 0.000038  min_lr: 0.000038  loss: 3.0762 (3.9785)  loss_scale: 16384.0000 (7872.4367)  weight_decay: 0.0500 (0.0500)  time: 0.8240  data: 0.2405  max mem: 4509
Epoch: [0]  [3380/3542]  eta: 0:02:12  lr: 0.000038  min_lr: 0.000038  loss: 3.2598 (3.9764)  loss_scale: 16384.0000 (7897.6114)  weight_decay: 0.0500 (0.0500)  time: 0.8184  data: 0.2344  max mem: 4509
Epoch: [0]  [3390/3542]  eta: 0:02:04  lr: 0.000038  min_lr: 0.000038  loss: 3.1992 (3.9740)  loss_scale: 16384.0000 (7922.6376)  weight_decay: 0.0500 (0.0500)  time: 0.8151  data: 0.2307  max mem: 4509
Epoch: [0]  [3400/3542]  eta: 0:01:56  lr: 0.000038  min_lr: 0.000038  loss: 3.1504 (3.9716)  loss_scale: 16384.0000 (7947.5166)  weight_decay: 0.0500 (0.0500)  time: 0.8326  data: 0.2486  max mem: 4509
Epoch: [0]  [3410/3542]  eta: 0:01:48  lr: 0.000039  min_lr: 0.000039  loss: 3.1191 (3.9695)  loss_scale: 16384.0000 (7972.2498)  weight_decay: 0.0500 (0.0500)  time: 0.8377  data: 0.2541  max mem: 4509
Epoch: [0]  [3420/3542]  eta: 0:01:39  lr: 0.000039  min_lr: 0.000039  loss: 3.1387 (3.9672)  loss_scale: 16384.0000 (7996.8384)  weight_decay: 0.0500 (0.0500)  time: 0.7817  data: 0.1972  max mem: 4509
Epoch: [0]  [3430/3542]  eta: 0:01:31  lr: 0.000039  min_lr: 0.000039  loss: 3.1387 (3.9649)  loss_scale: 16384.0000 (8021.2836)  weight_decay: 0.0500 (0.0500)  time: 0.7621  data: 0.1777  max mem: 4509
Epoch: [0]  [3440/3542]  eta: 0:01:23  lr: 0.000039  min_lr: 0.000039  loss: 3.0820 (3.9624)  loss_scale: 16384.0000 (8045.5867)  weight_decay: 0.0500 (0.0500)  time: 0.7829  data: 0.1994  max mem: 4509
[2023-05-16 06:12:38,447] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 3450
[2023-05-16 06:12:38,448] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 06:12:38,448] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [0]  [3450/3542]  eta: 0:01:15  lr: 0.000039  min_lr: 0.000039  loss: 3.0820 (3.9602)  loss_scale: 16384.0000 (8067.3753)  weight_decay: 0.0500 (0.0500)  time: 0.7532  data: 0.1698  max mem: 4509
Epoch: [0]  [3460/3542]  eta: 0:01:07  lr: 0.000039  min_lr: 0.000039  loss: 3.1855 (3.9581)  loss_scale: 8192.0000 (8067.7353)  weight_decay: 0.0500 (0.0500)  time: 0.7414  data: 0.1574  max mem: 4509
Epoch: [0]  [3470/3542]  eta: 0:00:58  lr: 0.000039  min_lr: 0.000039  loss: 3.1855 (3.9560)  loss_scale: 8192.0000 (8068.0933)  weight_decay: 0.0500 (0.0500)  time: 0.7431  data: 0.1587  max mem: 4509
Epoch: [0]  [3480/3542]  eta: 0:00:50  lr: 0.000039  min_lr: 0.000039  loss: 3.1387 (3.9538)  loss_scale: 8192.0000 (8068.4493)  weight_decay: 0.0500 (0.0500)  time: 0.7491  data: 0.1644  max mem: 4509
Epoch: [0]  [3490/3542]  eta: 0:00:42  lr: 0.000039  min_lr: 0.000039  loss: 3.1738 (3.9517)  loss_scale: 8192.0000 (8068.8032)  weight_decay: 0.0500 (0.0500)  time: 0.7515  data: 0.1664  max mem: 4509
Epoch: [0]  [3500/3542]  eta: 0:00:34  lr: 0.000040  min_lr: 0.000040  loss: 3.2090 (3.9496)  loss_scale: 8192.0000 (8069.1551)  weight_decay: 0.0500 (0.0500)  time: 0.7527  data: 0.1682  max mem: 4509
Epoch: [0]  [3510/3542]  eta: 0:00:26  lr: 0.000040  min_lr: 0.000040  loss: 3.1777 (3.9472)  loss_scale: 8192.0000 (8069.5050)  weight_decay: 0.0500 (0.0500)  time: 0.7608  data: 0.1768  max mem: 4509
Epoch: [0]  [3520/3542]  eta: 0:00:17  lr: 0.000040  min_lr: 0.000040  loss: 3.1973 (3.9452)  loss_scale: 8192.0000 (8069.8529)  weight_decay: 0.0500 (0.0500)  time: 0.7586  data: 0.1739  max mem: 4509
Epoch: [0]  [3530/3542]  eta: 0:00:09  lr: 0.000040  min_lr: 0.000040  loss: 3.2480 (3.9432)  loss_scale: 8192.0000 (8070.1988)  weight_decay: 0.0500 (0.0500)  time: 0.7557  data: 0.1707  max mem: 4509
Epoch: [0]  [3540/3542]  eta: 0:00:01  lr: 0.000040  min_lr: 0.000040  loss: 3.2441 (3.9413)  loss_scale: 8192.0000 (8070.5428)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.0848  max mem: 4509
Epoch: [0]  [3541/3542]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000040  loss: 3.2422 (3.9411)  loss_scale: 8192.0000 (8070.5771)  weight_decay: 0.0500 (0.0500)  time: 0.6680  data: 0.0848  max mem: 4509
Epoch: [0] Total time: 0:48:09 (0.8157 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000040  loss: 3.2422 (3.9411)  loss_scale: 8192.0000 (8070.5771)  weight_decay: 0.0500 (0.0500)
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 06:13:45,656] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-0/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:40:08    time: 15.4386  data: 10.3908  max mem: 13006
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:11:52    time: 4.8812  data: 0.9448  max mem: 13007
Test:  [ 20/156]  eta: 0:09:48    time: 3.7697  data: 0.0002  max mem: 13007
Test:  [ 30/156]  eta: 0:08:41    time: 3.7294  data: 0.0002  max mem: 13007
Test:  [ 40/156]  eta: 0:07:47    time: 3.7224  data: 0.0002  max mem: 13007
Test:  [ 50/156]  eta: 0:07:01    time: 3.7236  data: 0.0002  max mem: 13007
Test:  [ 60/156]  eta: 0:06:17    time: 3.7403  data: 0.0002  max mem: 13007
Test:  [ 70/156]  eta: 0:05:36    time: 3.7579  data: 0.0002  max mem: 13007
Test:  [ 80/156]  eta: 0:04:55    time: 3.7256  data: 0.0002  max mem: 13007
Test:  [ 90/156]  eta: 0:04:15    time: 3.7247  data: 0.0002  max mem: 13007
Test:  [100/156]  eta: 0:03:35    time: 3.7371  data: 0.0002  max mem: 13007
Test:  [110/156]  eta: 0:02:57    time: 3.7413  data: 0.0002  max mem: 13007
Test:  [120/156]  eta: 0:02:18    time: 3.7704  data: 0.0002  max mem: 13007
Test:  [130/156]  eta: 0:01:39    time: 3.7427  data: 0.0002  max mem: 13007
Test:  [140/156]  eta: 0:01:01    time: 3.7585  data: 0.0002  max mem: 13007
Test:  [150/156]  eta: 0:00:22    time: 3.7781  data: 0.0002  max mem: 13007
Test:  [155/156]  eta: 0:00:03    time: 3.7793  data: 0.0002  max mem: 13007
Test: Total time: 0:09:57 (3.8281 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e0.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e0.json and result file is ./output_freeze/coco_captioning_result_val_e0.json
Downloading https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val_gt.json to ./output_freeze/coco_karpathy_val_gt.json
  0%|          | 0/2657641 [00:00<?, ?it/s]100%|| 2657641/2657641 [00:00<00:00, 92983494.27it/s]
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e0.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1111062.06 tokens per second.
PTBTokenizer tokenized 58340 tokens at 405788.20 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 47217, 'reflen': 47247, 'guess': [47217, 42225, 37233, 32242], 'correct': [29412, 14068, 5750, 2058]}
ratio: 0.9993650390500773
Bleu_1: 0.623
Bleu_2: 0.455
Bleu_3: 0.317
Bleu_4: 0.213
computing METEOR score...
METEOR: 0.213
computing Rouge score...
ROUGE_L: 0.476
computing CIDEr score...
CIDEr: 0.691
computing SPICE score...
Parsing reference captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
Threads( StanfordCoreNLP ) [47.647 seconds]
Threads( StanfordCoreNLP ) [43.735 seconds]
Threads( StanfordCoreNLP ) [20.844 seconds]
Parsing test captions
Threads( StanfordCoreNLP ) [22.783 seconds]
SPICE evaluation took: 2.527 min
SPICE: 0.158
Performance of the network on the 5000 val images: 0.7%
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 06:26:41,657] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-best/mp_rank_00_model_states.pt
Max performance: 0.69%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [1]  [   0/3542]  eta: 22:04:00  lr: 0.000040  min_lr: 0.000040  loss: 3.2598 (3.2598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 22.4283  data: 21.7952  max mem: 13007
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [1]  [  10/3542]  eta: 3:53:50  lr: 0.000040  min_lr: 0.000040  loss: 3.1895 (3.2006)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 3.9725  data: 3.3846  max mem: 13007
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [1]  [  20/3542]  eta: 2:44:27  lr: 0.000040  min_lr: 0.000040  loss: 3.1895 (3.1936)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.8203  data: 1.2379  max mem: 13007
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [1]  [  30/3542]  eta: 2:19:00  lr: 0.000040  min_lr: 0.000040  loss: 3.2031 (3.2021)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4963  data: 0.9148  max mem: 13007
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [1]  [  40/3542]  eta: 2:08:00  lr: 0.000040  min_lr: 0.000040  loss: 3.2227 (3.2141)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5542  data: 0.9705  max mem: 13007
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [1]  [  50/3542]  eta: 1:55:54  lr: 0.000040  min_lr: 0.000040  loss: 3.2227 (3.2165)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3976  data: 0.8140  max mem: 13007
Epoch: [1]  [  60/3542]  eta: 1:47:56  lr: 0.000040  min_lr: 0.000040  loss: 3.1816 (3.2109)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1775  data: 0.5957  max mem: 13007
Epoch: [1]  [  70/3542]  eta: 1:42:49  lr: 0.000040  min_lr: 0.000040  loss: 3.1934 (3.2099)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2290  data: 0.6470  max mem: 13007
Epoch: [1]  [  80/3542]  eta: 1:36:17  lr: 0.000040  min_lr: 0.000040  loss: 3.1504 (3.2039)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0848  data: 0.5006  max mem: 13007
Epoch: [1]  [  90/3542]  eta: 1:31:08  lr: 0.000040  min_lr: 0.000040  loss: 3.1504 (3.1989)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9002  data: 0.3155  max mem: 13007
Epoch: [1]  [ 100/3542]  eta: 1:26:36  lr: 0.000040  min_lr: 0.000040  loss: 3.1660 (3.1995)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8657  data: 0.2831  max mem: 13007
Epoch: [1]  [ 110/3542]  eta: 1:23:04  lr: 0.000040  min_lr: 0.000040  loss: 3.2305 (3.2015)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8522  data: 0.2696  max mem: 13007
Epoch: [1]  [ 120/3542]  eta: 1:19:52  lr: 0.000040  min_lr: 0.000040  loss: 3.2168 (3.2007)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.2665  max mem: 13007
Epoch: [1]  [ 130/3542]  eta: 1:17:10  lr: 0.000040  min_lr: 0.000040  loss: 3.2109 (3.1979)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8289  data: 0.2452  max mem: 13007
Epoch: [1]  [ 140/3542]  eta: 1:14:52  lr: 0.000040  min_lr: 0.000040  loss: 3.0957 (3.1915)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8365  data: 0.2531  max mem: 13007
Epoch: [1]  [ 150/3542]  eta: 1:12:37  lr: 0.000040  min_lr: 0.000040  loss: 3.1250 (3.1892)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8094  data: 0.2265  max mem: 13007
Epoch: [1]  [ 160/3542]  eta: 1:10:44  lr: 0.000040  min_lr: 0.000040  loss: 3.1895 (3.1909)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7925  data: 0.2089  max mem: 13007
Epoch: [1]  [ 170/3542]  eta: 1:08:52  lr: 0.000040  min_lr: 0.000040  loss: 3.1621 (3.1893)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7791  data: 0.1946  max mem: 13007
Epoch: [1]  [ 180/3542]  eta: 1:07:09  lr: 0.000040  min_lr: 0.000040  loss: 3.1816 (3.1925)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7437  data: 0.1586  max mem: 13007
Epoch: [1]  [ 190/3542]  eta: 1:05:41  lr: 0.000040  min_lr: 0.000040  loss: 3.2402 (3.1952)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7529  data: 0.1684  max mem: 13007
Epoch: [1]  [ 200/3542]  eta: 1:04:13  lr: 0.000040  min_lr: 0.000040  loss: 3.2949 (3.1991)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7426  data: 0.1581  max mem: 13007
Epoch: [1]  [ 210/3542]  eta: 1:02:51  lr: 0.000040  min_lr: 0.000040  loss: 3.1816 (3.1985)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7102  data: 0.1251  max mem: 13007
Epoch: [1]  [ 220/3542]  eta: 1:01:35  lr: 0.000040  min_lr: 0.000040  loss: 3.1230 (3.1935)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7048  data: 0.1199  max mem: 13007
Epoch: [1]  [ 230/3542]  eta: 1:00:23  lr: 0.000040  min_lr: 0.000040  loss: 3.0977 (3.1903)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6971  data: 0.1124  max mem: 13007
Epoch: [1]  [ 240/3542]  eta: 0:59:25  lr: 0.000040  min_lr: 0.000040  loss: 3.0879 (3.1856)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7174  data: 0.1322  max mem: 13007
Epoch: [1]  [ 250/3542]  eta: 0:58:27  lr: 0.000040  min_lr: 0.000040  loss: 3.1191 (3.1836)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7320  data: 0.1463  max mem: 13007
Epoch: [1]  [ 260/3542]  eta: 0:57:33  lr: 0.000040  min_lr: 0.000040  loss: 3.1484 (3.1839)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7209  data: 0.1361  max mem: 13007
Epoch: [1]  [ 270/3542]  eta: 0:56:39  lr: 0.000040  min_lr: 0.000040  loss: 3.1035 (3.1832)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7087  data: 0.1230  max mem: 13007
Epoch: [1]  [ 280/3542]  eta: 0:55:55  lr: 0.000040  min_lr: 0.000040  loss: 3.1035 (3.1816)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7196  data: 0.1332  max mem: 13007
Epoch: [1]  [ 290/3542]  eta: 0:55:06  lr: 0.000040  min_lr: 0.000040  loss: 3.1836 (3.1847)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7168  data: 0.1310  max mem: 13007
Epoch: [1]  [ 300/3542]  eta: 0:54:26  lr: 0.000040  min_lr: 0.000040  loss: 3.2090 (3.1846)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7135  data: 0.1277  max mem: 13007
Epoch: [1]  [ 310/3542]  eta: 0:53:42  lr: 0.000040  min_lr: 0.000040  loss: 3.0898 (3.1841)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7101  data: 0.1244  max mem: 13007
Epoch: [1]  [ 320/3542]  eta: 0:53:09  lr: 0.000040  min_lr: 0.000040  loss: 3.0898 (3.1809)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7209  data: 0.1353  max mem: 13007
Epoch: [1]  [ 330/3542]  eta: 0:52:26  lr: 0.000040  min_lr: 0.000040  loss: 3.1191 (3.1825)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7053  data: 0.1190  max mem: 13007
Epoch: [1]  [ 340/3542]  eta: 0:51:54  lr: 0.000040  min_lr: 0.000040  loss: 3.1953 (3.1833)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6957  data: 0.1093  max mem: 13007
Epoch: [1]  [ 350/3542]  eta: 0:51:19  lr: 0.000040  min_lr: 0.000040  loss: 3.1953 (3.1851)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7192  data: 0.1327  max mem: 13007
Epoch: [1]  [ 360/3542]  eta: 0:50:46  lr: 0.000040  min_lr: 0.000040  loss: 3.1914 (3.1860)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7019  data: 0.1155  max mem: 13007
Epoch: [1]  [ 370/3542]  eta: 0:50:15  lr: 0.000040  min_lr: 0.000040  loss: 3.1914 (3.1857)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7040  data: 0.1184  max mem: 13007
Epoch: [1]  [ 380/3542]  eta: 0:49:45  lr: 0.000040  min_lr: 0.000040  loss: 3.1523 (3.1844)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7058  data: 0.1202  max mem: 13007
Epoch: [1]  [ 390/3542]  eta: 0:49:17  lr: 0.000040  min_lr: 0.000040  loss: 3.0742 (3.1834)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7079  data: 0.1220  max mem: 13007
Epoch: [1]  [ 400/3542]  eta: 0:48:48  lr: 0.000040  min_lr: 0.000040  loss: 3.1426 (3.1830)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6999  data: 0.1141  max mem: 13007
Epoch: [1]  [ 410/3542]  eta: 0:48:31  lr: 0.000040  min_lr: 0.000040  loss: 3.1465 (3.1818)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7618  data: 0.1763  max mem: 13007
Epoch: [1]  [ 420/3542]  eta: 0:48:06  lr: 0.000040  min_lr: 0.000040  loss: 3.1465 (3.1820)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7742  data: 0.1894  max mem: 13007
Epoch: [1]  [ 430/3542]  eta: 0:47:41  lr: 0.000040  min_lr: 0.000040  loss: 3.1289 (3.1804)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7096  data: 0.1242  max mem: 13007
Epoch: [1]  [ 440/3542]  eta: 0:47:15  lr: 0.000040  min_lr: 0.000040  loss: 3.1250 (3.1799)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.1058  max mem: 13007
Epoch: [1]  [ 450/3542]  eta: 0:46:45  lr: 0.000040  min_lr: 0.000040  loss: 3.1035 (3.1782)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6485  data: 0.0630  max mem: 13007
[2023-05-16 06:33:37,447] [INFO] [logging.py:60:log_dist] [Rank 0] step=4000, skipped=6, lr=[3.998022660610628e-05, 3.998022660610628e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 06:33:37,449] [INFO] [timer.py:157:stop] 0/4000, SamplesPerSec=55.64587347496529
Epoch: [1]  [ 460/3542]  eta: 0:46:19  lr: 0.000040  min_lr: 0.000040  loss: 3.0527 (3.1763)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0486  max mem: 13007
Epoch: [1]  [ 470/3542]  eta: 0:46:11  lr: 0.000040  min_lr: 0.000040  loss: 3.0801 (3.1758)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7820  data: 0.1942  max mem: 13007
Epoch: [1]  [ 480/3542]  eta: 0:45:47  lr: 0.000040  min_lr: 0.000040  loss: 3.1855 (3.1768)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7883  data: 0.2011  max mem: 13007
Epoch: [1]  [ 490/3542]  eta: 0:45:24  lr: 0.000040  min_lr: 0.000040  loss: 3.1680 (3.1768)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6696  data: 0.0839  max mem: 13007
Epoch: [1]  [ 500/3542]  eta: 0:45:02  lr: 0.000040  min_lr: 0.000040  loss: 3.1680 (3.1759)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6800  data: 0.0934  max mem: 13007
Epoch: [1]  [ 510/3542]  eta: 0:44:45  lr: 0.000040  min_lr: 0.000040  loss: 3.1953 (3.1765)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7133  data: 0.1271  max mem: 13007
Epoch: [1]  [ 520/3542]  eta: 0:44:26  lr: 0.000040  min_lr: 0.000040  loss: 3.2246 (3.1772)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7303  data: 0.1447  max mem: 13007
Epoch: [1]  [ 530/3542]  eta: 0:44:08  lr: 0.000040  min_lr: 0.000040  loss: 3.1055 (3.1756)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7166  data: 0.1308  max mem: 13007
Epoch: [1]  [ 540/3542]  eta: 0:43:53  lr: 0.000040  min_lr: 0.000040  loss: 3.0840 (3.1745)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7431  data: 0.1574  max mem: 13007
Epoch: [1]  [ 550/3542]  eta: 0:43:33  lr: 0.000040  min_lr: 0.000040  loss: 3.0801 (3.1730)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7219  data: 0.1359  max mem: 13007
Epoch: [1]  [ 560/3542]  eta: 0:43:17  lr: 0.000040  min_lr: 0.000040  loss: 3.1230 (3.1744)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6985  data: 0.1118  max mem: 13007
Epoch: [1]  [ 570/3542]  eta: 0:42:55  lr: 0.000040  min_lr: 0.000040  loss: 3.1992 (3.1747)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0848  max mem: 13007
Epoch: [1]  [ 580/3542]  eta: 0:42:35  lr: 0.000040  min_lr: 0.000040  loss: 3.1660 (3.1750)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.0430  max mem: 13007
Epoch: [1]  [ 590/3542]  eta: 0:42:15  lr: 0.000040  min_lr: 0.000040  loss: 3.1660 (3.1743)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.0593  max mem: 13007
Epoch: [1]  [ 600/3542]  eta: 0:42:00  lr: 0.000040  min_lr: 0.000040  loss: 3.1074 (3.1734)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6861  data: 0.1001  max mem: 13007
Epoch: [1]  [ 610/3542]  eta: 0:41:42  lr: 0.000040  min_lr: 0.000040  loss: 3.1250 (3.1728)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6878  data: 0.1024  max mem: 13007
Epoch: [1]  [ 620/3542]  eta: 0:41:26  lr: 0.000040  min_lr: 0.000040  loss: 3.1250 (3.1721)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6798  data: 0.0943  max mem: 13007
Epoch: [1]  [ 630/3542]  eta: 0:41:10  lr: 0.000040  min_lr: 0.000040  loss: 3.1816 (3.1729)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6940  data: 0.1081  max mem: 13007
Epoch: [1]  [ 640/3542]  eta: 0:40:56  lr: 0.000040  min_lr: 0.000040  loss: 3.1914 (3.1724)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6993  data: 0.1129  max mem: 13007
Epoch: [1]  [ 650/3542]  eta: 0:40:40  lr: 0.000040  min_lr: 0.000040  loss: 3.1816 (3.1723)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7066  data: 0.1200  max mem: 13007
Epoch: [1]  [ 660/3542]  eta: 0:40:25  lr: 0.000040  min_lr: 0.000040  loss: 3.1387 (3.1713)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6937  data: 0.1079  max mem: 13007
Epoch: [1]  [ 670/3542]  eta: 0:40:11  lr: 0.000040  min_lr: 0.000040  loss: 3.1504 (3.1716)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7015  data: 0.1154  max mem: 13007
Epoch: [1]  [ 680/3542]  eta: 0:39:54  lr: 0.000040  min_lr: 0.000040  loss: 3.1504 (3.1714)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.0875  max mem: 13007
Epoch: [1]  [ 690/3542]  eta: 0:39:38  lr: 0.000040  min_lr: 0.000040  loss: 3.1758 (3.1712)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0564  max mem: 13007
Epoch: [1]  [ 700/3542]  eta: 0:39:21  lr: 0.000040  min_lr: 0.000040  loss: 3.1758 (3.1709)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.0470  max mem: 13007
Epoch: [1]  [ 710/3542]  eta: 0:39:07  lr: 0.000040  min_lr: 0.000040  loss: 3.0762 (3.1697)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6564  data: 0.0702  max mem: 13007
Epoch: [1]  [ 720/3542]  eta: 0:38:54  lr: 0.000040  min_lr: 0.000040  loss: 3.0371 (3.1691)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6919  data: 0.1065  max mem: 13007
Epoch: [1]  [ 730/3542]  eta: 0:38:41  lr: 0.000040  min_lr: 0.000040  loss: 3.1523 (3.1694)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7078  data: 0.1224  max mem: 13007
Epoch: [1]  [ 740/3542]  eta: 0:38:27  lr: 0.000040  min_lr: 0.000040  loss: 3.1895 (3.1698)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6942  data: 0.1083  max mem: 13007
Epoch: [1]  [ 750/3542]  eta: 0:38:14  lr: 0.000040  min_lr: 0.000040  loss: 3.1895 (3.1712)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6858  data: 0.1001  max mem: 13007
Epoch: [1]  [ 760/3542]  eta: 0:38:02  lr: 0.000040  min_lr: 0.000040  loss: 3.1875 (3.1718)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7103  data: 0.1244  max mem: 13007
Epoch: [1]  [ 770/3542]  eta: 0:37:51  lr: 0.000040  min_lr: 0.000040  loss: 3.1836 (3.1726)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7190  data: 0.1324  max mem: 13007
Epoch: [1]  [ 780/3542]  eta: 0:37:39  lr: 0.000040  min_lr: 0.000040  loss: 3.2031 (3.1728)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7167  data: 0.1307  max mem: 13007
Epoch: [1]  [ 790/3542]  eta: 0:37:26  lr: 0.000040  min_lr: 0.000040  loss: 3.1016 (3.1717)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6949  data: 0.1085  max mem: 13007
Epoch: [1]  [ 800/3542]  eta: 0:37:11  lr: 0.000040  min_lr: 0.000040  loss: 3.1016 (3.1715)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.0722  max mem: 13007
Epoch: [1]  [ 810/3542]  eta: 0:36:59  lr: 0.000040  min_lr: 0.000040  loss: 3.0918 (3.1707)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6682  data: 0.0805  max mem: 13007
Epoch: [1]  [ 820/3542]  eta: 0:36:48  lr: 0.000040  min_lr: 0.000040  loss: 3.0918 (3.1704)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7047  data: 0.1174  max mem: 13007
Epoch: [1]  [ 830/3542]  eta: 0:36:36  lr: 0.000040  min_lr: 0.000040  loss: 3.1797 (3.1707)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6980  data: 0.1114  max mem: 13007
Epoch: [1]  [ 840/3542]  eta: 0:36:25  lr: 0.000040  min_lr: 0.000040  loss: 3.1504 (3.1703)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7053  data: 0.1194  max mem: 13007
Epoch: [1]  [ 850/3542]  eta: 0:36:13  lr: 0.000040  min_lr: 0.000040  loss: 3.1738 (3.1703)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7083  data: 0.1223  max mem: 13007
Epoch: [1]  [ 860/3542]  eta: 0:36:03  lr: 0.000040  min_lr: 0.000040  loss: 3.1797 (3.1699)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7136  data: 0.1278  max mem: 13007
Epoch: [1]  [ 870/3542]  eta: 0:35:52  lr: 0.000040  min_lr: 0.000040  loss: 3.1504 (3.1703)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7217  data: 0.1361  max mem: 13007
Epoch: [1]  [ 880/3542]  eta: 0:35:38  lr: 0.000040  min_lr: 0.000040  loss: 3.2168 (3.1710)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6691  data: 0.0827  max mem: 13007
Epoch: [1]  [ 890/3542]  eta: 0:35:28  lr: 0.000040  min_lr: 0.000040  loss: 3.2051 (3.1712)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6751  data: 0.0885  max mem: 13007
Epoch: [1]  [ 900/3542]  eta: 0:35:16  lr: 0.000040  min_lr: 0.000040  loss: 3.1328 (3.1703)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.1163  max mem: 13007
[2023-05-16 06:38:51,288] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 06:38:51,289] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [1]  [ 910/3542]  eta: 0:35:04  lr: 0.000040  min_lr: 0.000040  loss: 3.0586 (3.1699)  loss_scale: 8192.0000 (8209.9846)  weight_decay: 0.0500 (0.0500)  time: 0.6704  data: 0.0843  max mem: 13007
Epoch: [1]  [ 920/3542]  eta: 0:34:53  lr: 0.000040  min_lr: 0.000040  loss: 3.0586 (3.1681)  loss_scale: 16384.0000 (8298.7362)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.0797  max mem: 13007
Epoch: [1]  [ 930/3542]  eta: 0:34:40  lr: 0.000040  min_lr: 0.000040  loss: 3.0801 (3.1675)  loss_scale: 16384.0000 (8385.5811)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.0705  max mem: 13007
Epoch: [1]  [ 940/3542]  eta: 0:34:30  lr: 0.000040  min_lr: 0.000040  loss: 3.0996 (3.1672)  loss_scale: 16384.0000 (8470.5802)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.0945  max mem: 13007
Epoch: [1]  [ 950/3542]  eta: 0:34:19  lr: 0.000040  min_lr: 0.000040  loss: 3.1230 (3.1669)  loss_scale: 16384.0000 (8553.7918)  weight_decay: 0.0500 (0.0500)  time: 0.7033  data: 0.1175  max mem: 13007
Epoch: [1]  [ 960/3542]  eta: 0:34:08  lr: 0.000040  min_lr: 0.000040  loss: 3.0742 (3.1657)  loss_scale: 16384.0000 (8635.2716)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.0911  max mem: 13007
Epoch: [1]  [ 970/3542]  eta: 0:33:57  lr: 0.000040  min_lr: 0.000040  loss: 3.0703 (3.1657)  loss_scale: 16384.0000 (8715.0731)  weight_decay: 0.0500 (0.0500)  time: 0.6888  data: 0.1021  max mem: 13007
Epoch: [1]  [ 980/3542]  eta: 0:33:46  lr: 0.000040  min_lr: 0.000040  loss: 3.0859 (3.1650)  loss_scale: 16384.0000 (8793.2477)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.0986  max mem: 13007
Epoch: [1]  [ 990/3542]  eta: 0:33:35  lr: 0.000040  min_lr: 0.000040  loss: 3.1406 (3.1651)  loss_scale: 16384.0000 (8869.8446)  weight_decay: 0.0500 (0.0500)  time: 0.6685  data: 0.0827  max mem: 13007
Epoch: [1]  [1000/3542]  eta: 0:33:24  lr: 0.000040  min_lr: 0.000040  loss: 3.1699 (3.1660)  loss_scale: 16384.0000 (8944.9111)  weight_decay: 0.0500 (0.0500)  time: 0.6576  data: 0.0719  max mem: 13007
Epoch: [1]  [1010/3542]  eta: 0:33:14  lr: 0.000040  min_lr: 0.000040  loss: 3.1777 (3.1663)  loss_scale: 16384.0000 (9018.4926)  weight_decay: 0.0500 (0.0500)  time: 0.6722  data: 0.0869  max mem: 13007
Epoch: [1]  [1020/3542]  eta: 0:33:02  lr: 0.000040  min_lr: 0.000040  loss: 3.1777 (3.1662)  loss_scale: 16384.0000 (9090.6327)  weight_decay: 0.0500 (0.0500)  time: 0.6743  data: 0.0881  max mem: 13007
Epoch: [1]  [1030/3542]  eta: 0:32:52  lr: 0.000040  min_lr: 0.000040  loss: 3.0312 (3.1653)  loss_scale: 16384.0000 (9161.3734)  weight_decay: 0.0500 (0.0500)  time: 0.6698  data: 0.0840  max mem: 13007
Epoch: [1]  [1040/3542]  eta: 0:32:43  lr: 0.000040  min_lr: 0.000040  loss: 3.0312 (3.1644)  loss_scale: 16384.0000 (9230.7550)  weight_decay: 0.0500 (0.0500)  time: 0.7084  data: 0.1228  max mem: 13007
Epoch: [1]  [1050/3542]  eta: 0:32:33  lr: 0.000040  min_lr: 0.000040  loss: 3.0684 (3.1635)  loss_scale: 16384.0000 (9298.8164)  weight_decay: 0.0500 (0.0500)  time: 0.7154  data: 0.1291  max mem: 13007
Epoch: [1]  [1060/3542]  eta: 0:32:24  lr: 0.000040  min_lr: 0.000040  loss: 3.1621 (3.1642)  loss_scale: 16384.0000 (9365.5947)  weight_decay: 0.0500 (0.0500)  time: 0.7210  data: 0.1342  max mem: 13007
Epoch: [1]  [1070/3542]  eta: 0:32:13  lr: 0.000040  min_lr: 0.000040  loss: 3.1934 (3.1637)  loss_scale: 16384.0000 (9431.1261)  weight_decay: 0.0500 (0.0500)  time: 0.6969  data: 0.1103  max mem: 13007
Epoch: [1]  [1080/3542]  eta: 0:32:03  lr: 0.000040  min_lr: 0.000040  loss: 3.1816 (3.1644)  loss_scale: 16384.0000 (9495.4450)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0732  max mem: 13007
Epoch: [1]  [1090/3542]  eta: 0:31:52  lr: 0.000040  min_lr: 0.000040  loss: 3.1172 (3.1638)  loss_scale: 16384.0000 (9558.5848)  weight_decay: 0.0500 (0.0500)  time: 0.6659  data: 0.0804  max mem: 13007
Epoch: [1]  [1100/3542]  eta: 0:31:43  lr: 0.000040  min_lr: 0.000040  loss: 3.0469 (3.1625)  loss_scale: 16384.0000 (9620.5777)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.0924  max mem: 13007
Epoch: [1]  [1110/3542]  eta: 0:31:32  lr: 0.000040  min_lr: 0.000040  loss: 3.1035 (3.1622)  loss_scale: 16384.0000 (9681.4545)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.0914  max mem: 13007
Epoch: [1]  [1120/3542]  eta: 0:31:23  lr: 0.000040  min_lr: 0.000040  loss: 3.1309 (3.1620)  loss_scale: 16384.0000 (9741.2453)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.0961  max mem: 13007
Epoch: [1]  [1130/3542]  eta: 0:31:12  lr: 0.000040  min_lr: 0.000040  loss: 3.1309 (3.1618)  loss_scale: 16384.0000 (9799.9788)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.0872  max mem: 13007
Epoch: [1]  [1140/3542]  eta: 0:31:03  lr: 0.000040  min_lr: 0.000040  loss: 3.0488 (3.1610)  loss_scale: 16384.0000 (9857.6827)  weight_decay: 0.0500 (0.0500)  time: 0.6812  data: 0.0959  max mem: 13007
Epoch: [1]  [1150/3542]  eta: 0:30:53  lr: 0.000040  min_lr: 0.000040  loss: 3.0488 (3.1604)  loss_scale: 16384.0000 (9914.3840)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.1068  max mem: 13007
Epoch: [1]  [1160/3542]  eta: 0:30:44  lr: 0.000040  min_lr: 0.000040  loss: 3.0938 (3.1602)  loss_scale: 16384.0000 (9970.1085)  weight_decay: 0.0500 (0.0500)  time: 0.6791  data: 0.0928  max mem: 13007
Epoch: [1]  [1170/3542]  eta: 0:30:34  lr: 0.000040  min_lr: 0.000040  loss: 3.0938 (3.1598)  loss_scale: 16384.0000 (10024.8813)  weight_decay: 0.0500 (0.0500)  time: 0.6927  data: 0.1071  max mem: 13007
Epoch: [1]  [1180/3542]  eta: 0:30:24  lr: 0.000040  min_lr: 0.000040  loss: 3.1738 (3.1605)  loss_scale: 16384.0000 (10078.7265)  weight_decay: 0.0500 (0.0500)  time: 0.6726  data: 0.0869  max mem: 13007
Epoch: [1]  [1190/3542]  eta: 0:30:14  lr: 0.000040  min_lr: 0.000040  loss: 3.1738 (3.1600)  loss_scale: 16384.0000 (10131.6675)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.0685  max mem: 13007
Epoch: [1]  [1200/3542]  eta: 0:30:04  lr: 0.000040  min_lr: 0.000040  loss: 3.1465 (3.1597)  loss_scale: 16384.0000 (10183.7269)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.0781  max mem: 13007
Epoch: [1]  [1210/3542]  eta: 0:29:56  lr: 0.000040  min_lr: 0.000040  loss: 3.1094 (3.1591)  loss_scale: 16384.0000 (10234.9265)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.1030  max mem: 13007
Epoch: [1]  [1220/3542]  eta: 0:29:46  lr: 0.000040  min_lr: 0.000040  loss: 2.9922 (3.1583)  loss_scale: 16384.0000 (10285.2875)  weight_decay: 0.0500 (0.0500)  time: 0.6987  data: 0.1133  max mem: 13007
Epoch: [1]  [1230/3542]  eta: 0:29:37  lr: 0.000040  min_lr: 0.000040  loss: 3.1191 (3.1582)  loss_scale: 16384.0000 (10334.8302)  weight_decay: 0.0500 (0.0500)  time: 0.6961  data: 0.1105  max mem: 13007
Epoch: [1]  [1240/3542]  eta: 0:29:28  lr: 0.000040  min_lr: 0.000040  loss: 3.1719 (3.1587)  loss_scale: 16384.0000 (10383.5745)  weight_decay: 0.0500 (0.0500)  time: 0.7046  data: 0.1186  max mem: 13007
Epoch: [1]  [1250/3542]  eta: 0:29:20  lr: 0.000040  min_lr: 0.000040  loss: 3.1836 (3.1590)  loss_scale: 16384.0000 (10431.5396)  weight_decay: 0.0500 (0.0500)  time: 0.7130  data: 0.1262  max mem: 13007
Epoch: [1]  [1260/3542]  eta: 0:29:11  lr: 0.000040  min_lr: 0.000040  loss: 3.1738 (3.1590)  loss_scale: 16384.0000 (10478.7439)  weight_decay: 0.0500 (0.0500)  time: 0.7019  data: 0.1152  max mem: 13007
Epoch: [1]  [1270/3542]  eta: 0:29:02  lr: 0.000040  min_lr: 0.000040  loss: 3.1504 (3.1589)  loss_scale: 16384.0000 (10525.2054)  weight_decay: 0.0500 (0.0500)  time: 0.7052  data: 0.1186  max mem: 13007
Epoch: [1]  [1280/3542]  eta: 0:28:53  lr: 0.000040  min_lr: 0.000040  loss: 3.0996 (3.1581)  loss_scale: 16384.0000 (10570.9415)  weight_decay: 0.0500 (0.0500)  time: 0.6996  data: 0.1124  max mem: 13007
Epoch: [1]  [1290/3542]  eta: 0:28:44  lr: 0.000040  min_lr: 0.000040  loss: 3.0684 (3.1580)  loss_scale: 16384.0000 (10615.9690)  weight_decay: 0.0500 (0.0500)  time: 0.6929  data: 0.1061  max mem: 13007
Epoch: [1]  [1300/3542]  eta: 0:28:36  lr: 0.000040  min_lr: 0.000040  loss: 3.1191 (3.1578)  loss_scale: 16384.0000 (10660.3044)  weight_decay: 0.0500 (0.0500)  time: 0.7214  data: 0.1346  max mem: 13007
Epoch: [1]  [1310/3542]  eta: 0:28:29  lr: 0.000040  min_lr: 0.000040  loss: 3.1191 (3.1575)  loss_scale: 16384.0000 (10703.9634)  weight_decay: 0.0500 (0.0500)  time: 0.7494  data: 0.1629  max mem: 13007
Epoch: [1]  [1320/3542]  eta: 0:28:20  lr: 0.000040  min_lr: 0.000040  loss: 3.1719 (3.1576)  loss_scale: 16384.0000 (10746.9614)  weight_decay: 0.0500 (0.0500)  time: 0.7450  data: 0.1587  max mem: 13007
Epoch: [1]  [1330/3542]  eta: 0:28:12  lr: 0.000040  min_lr: 0.000040  loss: 3.1816 (3.1572)  loss_scale: 16384.0000 (10789.3133)  weight_decay: 0.0500 (0.0500)  time: 0.7151  data: 0.1279  max mem: 13007
Epoch: [1]  [1340/3542]  eta: 0:28:03  lr: 0.000040  min_lr: 0.000040  loss: 3.0430 (3.1565)  loss_scale: 16384.0000 (10831.0336)  weight_decay: 0.0500 (0.0500)  time: 0.7227  data: 0.1359  max mem: 13007
Epoch: [1]  [1350/3542]  eta: 0:27:55  lr: 0.000040  min_lr: 0.000040  loss: 3.0938 (3.1563)  loss_scale: 16384.0000 (10872.1362)  weight_decay: 0.0500 (0.0500)  time: 0.7113  data: 0.1250  max mem: 13007
Epoch: [1]  [1360/3542]  eta: 0:27:47  lr: 0.000040  min_lr: 0.000040  loss: 3.1465 (3.1565)  loss_scale: 16384.0000 (10912.6348)  weight_decay: 0.0500 (0.0500)  time: 0.7155  data: 0.1289  max mem: 13007
Epoch: [1]  [1370/3542]  eta: 0:27:38  lr: 0.000040  min_lr: 0.000040  loss: 3.1523 (3.1565)  loss_scale: 16384.0000 (10952.5427)  weight_decay: 0.0500 (0.0500)  time: 0.7144  data: 0.1287  max mem: 13007
[2023-05-16 06:44:11,397] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 4914
[2023-05-16 06:44:11,397] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 06:44:11,397] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [1]  [1380/3542]  eta: 0:27:29  lr: 0.000040  min_lr: 0.000040  loss: 3.1133 (3.1560)  loss_scale: 16384.0000 (10938.4852)  weight_decay: 0.0500 (0.0500)  time: 0.7045  data: 0.1191  max mem: 13007
Epoch: [1]  [1390/3542]  eta: 0:27:21  lr: 0.000040  min_lr: 0.000040  loss: 3.0312 (3.1553)  loss_scale: 8192.0000 (10918.7405)  weight_decay: 0.0500 (0.0500)  time: 0.7111  data: 0.1248  max mem: 13007
Epoch: [1]  [1400/3542]  eta: 0:27:12  lr: 0.000040  min_lr: 0.000040  loss: 3.0156 (3.1543)  loss_scale: 8192.0000 (10899.2777)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.1164  max mem: 13007
Epoch: [1]  [1410/3542]  eta: 0:27:04  lr: 0.000040  min_lr: 0.000040  loss: 3.0820 (3.1543)  loss_scale: 8192.0000 (10880.0907)  weight_decay: 0.0500 (0.0500)  time: 0.7094  data: 0.1230  max mem: 13007
Epoch: [1]  [1420/3542]  eta: 0:26:56  lr: 0.000040  min_lr: 0.000040  loss: 3.1348 (3.1543)  loss_scale: 8192.0000 (10861.1738)  weight_decay: 0.0500 (0.0500)  time: 0.7250  data: 0.1389  max mem: 13007
Epoch: [1]  [1430/3542]  eta: 0:26:48  lr: 0.000040  min_lr: 0.000040  loss: 3.0977 (3.1531)  loss_scale: 8192.0000 (10842.5213)  weight_decay: 0.0500 (0.0500)  time: 0.7441  data: 0.1581  max mem: 13007
Epoch: [1]  [1440/3542]  eta: 0:26:39  lr: 0.000040  min_lr: 0.000040  loss: 3.0195 (3.1531)  loss_scale: 8192.0000 (10824.1277)  weight_decay: 0.0500 (0.0500)  time: 0.7104  data: 0.1238  max mem: 13007
Epoch: [1]  [1450/3542]  eta: 0:26:31  lr: 0.000040  min_lr: 0.000040  loss: 3.1172 (3.1528)  loss_scale: 8192.0000 (10805.9876)  weight_decay: 0.0500 (0.0500)  time: 0.7013  data: 0.1150  max mem: 13007
[2023-05-16 06:45:11,661] [INFO] [logging.py:60:log_dist] [Rank 0] step=5000, skipped=7, lr=[3.9799324187346614e-05, 3.9799324187346614e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 06:45:11,663] [INFO] [timer.py:157:stop] 0/5000, SamplesPerSec=55.62118779547444
Epoch: [1]  [1460/3542]  eta: 0:26:23  lr: 0.000040  min_lr: 0.000040  loss: 3.1094 (3.1527)  loss_scale: 8192.0000 (10788.0958)  weight_decay: 0.0500 (0.0500)  time: 0.7334  data: 0.1476  max mem: 13007
Epoch: [1]  [1470/3542]  eta: 0:26:15  lr: 0.000040  min_lr: 0.000040  loss: 3.1250 (3.1524)  loss_scale: 8192.0000 (10770.4473)  weight_decay: 0.0500 (0.0500)  time: 0.7194  data: 0.1337  max mem: 13007
Epoch: [1]  [1480/3542]  eta: 0:26:06  lr: 0.000040  min_lr: 0.000040  loss: 3.1035 (3.1522)  loss_scale: 8192.0000 (10753.0371)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.1174  max mem: 13007
Epoch: [1]  [1490/3542]  eta: 0:25:59  lr: 0.000040  min_lr: 0.000040  loss: 3.1172 (3.1521)  loss_scale: 8192.0000 (10735.8605)  weight_decay: 0.0500 (0.0500)  time: 0.7223  data: 0.1367  max mem: 13007
Epoch: [1]  [1500/3542]  eta: 0:25:50  lr: 0.000040  min_lr: 0.000040  loss: 3.1270 (3.1514)  loss_scale: 8192.0000 (10718.9127)  weight_decay: 0.0500 (0.0500)  time: 0.7111  data: 0.1250  max mem: 13007
Epoch: [1]  [1510/3542]  eta: 0:25:42  lr: 0.000040  min_lr: 0.000040  loss: 2.9805 (3.1512)  loss_scale: 8192.0000 (10702.1893)  weight_decay: 0.0500 (0.0500)  time: 0.6949  data: 0.1087  max mem: 13007
Epoch: [1]  [1520/3542]  eta: 0:25:34  lr: 0.000040  min_lr: 0.000040  loss: 3.0898 (3.1510)  loss_scale: 8192.0000 (10685.6857)  weight_decay: 0.0500 (0.0500)  time: 0.7278  data: 0.1420  max mem: 13007
Epoch: [1]  [1530/3542]  eta: 0:25:26  lr: 0.000040  min_lr: 0.000040  loss: 3.1035 (3.1506)  loss_scale: 8192.0000 (10669.3978)  weight_decay: 0.0500 (0.0500)  time: 0.7382  data: 0.1522  max mem: 13007
Epoch: [1]  [1540/3542]  eta: 0:25:17  lr: 0.000040  min_lr: 0.000040  loss: 3.1406 (3.1507)  loss_scale: 8192.0000 (10653.3212)  weight_decay: 0.0500 (0.0500)  time: 0.7105  data: 0.1242  max mem: 13007
Epoch: [1]  [1550/3542]  eta: 0:25:09  lr: 0.000040  min_lr: 0.000040  loss: 3.1133 (3.1504)  loss_scale: 8192.0000 (10637.4520)  weight_decay: 0.0500 (0.0500)  time: 0.6982  data: 0.1117  max mem: 13007
Epoch: [1]  [1560/3542]  eta: 0:25:01  lr: 0.000040  min_lr: 0.000040  loss: 3.0957 (3.1501)  loss_scale: 8192.0000 (10621.7860)  weight_decay: 0.0500 (0.0500)  time: 0.7049  data: 0.1175  max mem: 13007
Epoch: [1]  [1570/3542]  eta: 0:24:52  lr: 0.000040  min_lr: 0.000040  loss: 3.0957 (3.1499)  loss_scale: 8192.0000 (10606.3195)  weight_decay: 0.0500 (0.0500)  time: 0.6922  data: 0.1057  max mem: 13007
Epoch: [1]  [1580/3542]  eta: 0:24:44  lr: 0.000040  min_lr: 0.000040  loss: 3.0254 (3.1489)  loss_scale: 8192.0000 (10591.0487)  weight_decay: 0.0500 (0.0500)  time: 0.6935  data: 0.1082  max mem: 13007
Epoch: [1]  [1590/3542]  eta: 0:24:36  lr: 0.000040  min_lr: 0.000040  loss: 3.0430 (3.1487)  loss_scale: 8192.0000 (10575.9698)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.1217  max mem: 13007
Epoch: [1]  [1600/3542]  eta: 0:24:28  lr: 0.000040  min_lr: 0.000040  loss: 3.0703 (3.1483)  loss_scale: 8192.0000 (10561.0793)  weight_decay: 0.0500 (0.0500)  time: 0.7089  data: 0.1224  max mem: 13007
Epoch: [1]  [1610/3542]  eta: 0:24:20  lr: 0.000040  min_lr: 0.000040  loss: 3.0781 (3.1481)  loss_scale: 8192.0000 (10546.3737)  weight_decay: 0.0500 (0.0500)  time: 0.7025  data: 0.1168  max mem: 13007
Epoch: [1]  [1620/3542]  eta: 0:24:11  lr: 0.000040  min_lr: 0.000040  loss: 3.1797 (3.1486)  loss_scale: 8192.0000 (10531.8495)  weight_decay: 0.0500 (0.0500)  time: 0.6934  data: 0.1077  max mem: 13007
Epoch: [1]  [1630/3542]  eta: 0:24:03  lr: 0.000040  min_lr: 0.000040  loss: 3.1973 (3.1487)  loss_scale: 8192.0000 (10517.5034)  weight_decay: 0.0500 (0.0500)  time: 0.6947  data: 0.1088  max mem: 13007
Epoch: [1]  [1640/3542]  eta: 0:23:55  lr: 0.000040  min_lr: 0.000040  loss: 3.1230 (3.1481)  loss_scale: 8192.0000 (10503.3321)  weight_decay: 0.0500 (0.0500)  time: 0.7217  data: 0.1363  max mem: 13007
Epoch: [1]  [1650/3542]  eta: 0:23:47  lr: 0.000040  min_lr: 0.000040  loss: 3.0625 (3.1479)  loss_scale: 8192.0000 (10489.3325)  weight_decay: 0.0500 (0.0500)  time: 0.7176  data: 0.1322  max mem: 13007
Epoch: [1]  [1660/3542]  eta: 0:23:39  lr: 0.000040  min_lr: 0.000040  loss: 3.0918 (3.1474)  loss_scale: 8192.0000 (10475.5015)  weight_decay: 0.0500 (0.0500)  time: 0.7116  data: 0.1258  max mem: 13007
Epoch: [1]  [1670/3542]  eta: 0:23:32  lr: 0.000040  min_lr: 0.000040  loss: 3.0449 (3.1468)  loss_scale: 8192.0000 (10461.8360)  weight_decay: 0.0500 (0.0500)  time: 0.7323  data: 0.1475  max mem: 13007
Epoch: [1]  [1680/3542]  eta: 0:23:24  lr: 0.000040  min_lr: 0.000040  loss: 3.0957 (3.1470)  loss_scale: 8192.0000 (10448.3331)  weight_decay: 0.0500 (0.0500)  time: 0.7409  data: 0.1563  max mem: 13007
Epoch: [1]  [1690/3542]  eta: 0:23:16  lr: 0.000040  min_lr: 0.000040  loss: 2.9648 (3.1463)  loss_scale: 8192.0000 (10434.9899)  weight_decay: 0.0500 (0.0500)  time: 0.7204  data: 0.1353  max mem: 13007
Epoch: [1]  [1700/3542]  eta: 0:23:08  lr: 0.000040  min_lr: 0.000040  loss: 3.0059 (3.1462)  loss_scale: 8192.0000 (10421.8036)  weight_decay: 0.0500 (0.0500)  time: 0.7412  data: 0.1562  max mem: 13007
Epoch: [1]  [1710/3542]  eta: 0:23:01  lr: 0.000040  min_lr: 0.000040  loss: 3.0566 (3.1457)  loss_scale: 8192.0000 (10408.7715)  weight_decay: 0.0500 (0.0500)  time: 0.7531  data: 0.1681  max mem: 13007
Epoch: [1]  [1720/3542]  eta: 0:22:53  lr: 0.000040  min_lr: 0.000040  loss: 3.0547 (3.1450)  loss_scale: 8192.0000 (10395.8908)  weight_decay: 0.0500 (0.0500)  time: 0.7124  data: 0.1267  max mem: 13007
Epoch: [1]  [1730/3542]  eta: 0:22:45  lr: 0.000040  min_lr: 0.000040  loss: 3.1211 (3.1452)  loss_scale: 8192.0000 (10383.1589)  weight_decay: 0.0500 (0.0500)  time: 0.7142  data: 0.1278  max mem: 13007
Epoch: [1]  [1740/3542]  eta: 0:22:36  lr: 0.000040  min_lr: 0.000040  loss: 3.1133 (3.1450)  loss_scale: 8192.0000 (10370.5732)  weight_decay: 0.0500 (0.0500)  time: 0.6787  data: 0.0931  max mem: 13007
Epoch: [1]  [1750/3542]  eta: 0:22:28  lr: 0.000040  min_lr: 0.000040  loss: 3.0684 (3.1451)  loss_scale: 8192.0000 (10358.1314)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.1052  max mem: 13007
Epoch: [1]  [1760/3542]  eta: 0:22:21  lr: 0.000040  min_lr: 0.000040  loss: 2.9746 (3.1444)  loss_scale: 8192.0000 (10345.8308)  weight_decay: 0.0500 (0.0500)  time: 0.7477  data: 0.1613  max mem: 13007
Epoch: [1]  [1770/3542]  eta: 0:22:13  lr: 0.000040  min_lr: 0.000040  loss: 3.0801 (3.1443)  loss_scale: 8192.0000 (10333.6691)  weight_decay: 0.0500 (0.0500)  time: 0.7418  data: 0.1554  max mem: 13007
Epoch: [1]  [1780/3542]  eta: 0:22:05  lr: 0.000040  min_lr: 0.000040  loss: 3.0859 (3.1443)  loss_scale: 8192.0000 (10321.6440)  weight_decay: 0.0500 (0.0500)  time: 0.7180  data: 0.1324  max mem: 13007
Epoch: [1]  [1790/3542]  eta: 0:21:57  lr: 0.000040  min_lr: 0.000040  loss: 3.1367 (3.1443)  loss_scale: 8192.0000 (10309.7532)  weight_decay: 0.0500 (0.0500)  time: 0.7002  data: 0.1153  max mem: 13007
Epoch: [1]  [1800/3542]  eta: 0:21:49  lr: 0.000040  min_lr: 0.000040  loss: 3.1230 (3.1437)  loss_scale: 8192.0000 (10297.9944)  weight_decay: 0.0500 (0.0500)  time: 0.7075  data: 0.1219  max mem: 13007
Epoch: [1]  [1810/3542]  eta: 0:21:42  lr: 0.000040  min_lr: 0.000040  loss: 3.1230 (3.1437)  loss_scale: 8192.0000 (10286.3655)  weight_decay: 0.0500 (0.0500)  time: 0.7292  data: 0.1433  max mem: 13007
Epoch: [1]  [1820/3542]  eta: 0:21:34  lr: 0.000040  min_lr: 0.000040  loss: 3.1328 (3.1435)  loss_scale: 8192.0000 (10274.8644)  weight_decay: 0.0500 (0.0500)  time: 0.7254  data: 0.1397  max mem: 13007
Epoch: [1]  [1830/3542]  eta: 0:21:26  lr: 0.000040  min_lr: 0.000040  loss: 3.0840 (3.1430)  loss_scale: 8192.0000 (10263.4888)  weight_decay: 0.0500 (0.0500)  time: 0.7078  data: 0.1217  max mem: 13007
Epoch: [1]  [1840/3542]  eta: 0:21:18  lr: 0.000040  min_lr: 0.000040  loss: 3.0352 (3.1425)  loss_scale: 8192.0000 (10252.2368)  weight_decay: 0.0500 (0.0500)  time: 0.6972  data: 0.1111  max mem: 13007
Epoch: [1]  [1850/3542]  eta: 0:21:10  lr: 0.000040  min_lr: 0.000040  loss: 3.0000 (3.1414)  loss_scale: 8192.0000 (10241.1064)  weight_decay: 0.0500 (0.0500)  time: 0.6955  data: 0.1099  max mem: 13007
Epoch: [1]  [1860/3542]  eta: 0:21:01  lr: 0.000040  min_lr: 0.000040  loss: 3.0000 (3.1409)  loss_scale: 8192.0000 (10230.0956)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.0953  max mem: 13007
Epoch: [1]  [1870/3542]  eta: 0:20:53  lr: 0.000040  min_lr: 0.000040  loss: 3.0625 (3.1406)  loss_scale: 8192.0000 (10219.2026)  weight_decay: 0.0500 (0.0500)  time: 0.6740  data: 0.0882  max mem: 13007
Epoch: [1]  [1880/3542]  eta: 0:20:46  lr: 0.000040  min_lr: 0.000040  loss: 3.0723 (3.1403)  loss_scale: 8192.0000 (10208.4253)  weight_decay: 0.0500 (0.0500)  time: 0.7238  data: 0.1373  max mem: 13007
Epoch: [1]  [1890/3542]  eta: 0:20:38  lr: 0.000040  min_lr: 0.000040  loss: 3.0566 (3.1399)  loss_scale: 8192.0000 (10197.7620)  weight_decay: 0.0500 (0.0500)  time: 0.7267  data: 0.1406  max mem: 13007
Epoch: [1]  [1900/3542]  eta: 0:20:30  lr: 0.000040  min_lr: 0.000040  loss: 3.0566 (3.1395)  loss_scale: 8192.0000 (10187.2109)  weight_decay: 0.0500 (0.0500)  time: 0.6991  data: 0.1133  max mem: 13007
Epoch: [1]  [1910/3542]  eta: 0:20:22  lr: 0.000040  min_lr: 0.000040  loss: 3.0703 (3.1393)  loss_scale: 8192.0000 (10176.7703)  weight_decay: 0.0500 (0.0500)  time: 0.7083  data: 0.1215  max mem: 13007
Epoch: [1]  [1920/3542]  eta: 0:20:14  lr: 0.000040  min_lr: 0.000040  loss: 3.1543 (3.1399)  loss_scale: 8192.0000 (10166.4383)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.1157  max mem: 13007
Epoch: [1]  [1930/3542]  eta: 0:20:06  lr: 0.000040  min_lr: 0.000040  loss: 3.1543 (3.1398)  loss_scale: 8192.0000 (10156.2134)  weight_decay: 0.0500 (0.0500)  time: 0.7037  data: 0.1168  max mem: 13007
Epoch: [1]  [1940/3542]  eta: 0:19:58  lr: 0.000040  min_lr: 0.000040  loss: 3.0859 (3.1393)  loss_scale: 8192.0000 (10146.0938)  weight_decay: 0.0500 (0.0500)  time: 0.6941  data: 0.1073  max mem: 13007
Epoch: [1]  [1950/3542]  eta: 0:19:50  lr: 0.000040  min_lr: 0.000040  loss: 3.0391 (3.1387)  loss_scale: 8192.0000 (10136.0779)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.0734  max mem: 13007
Epoch: [1]  [1960/3542]  eta: 0:19:42  lr: 0.000040  min_lr: 0.000040  loss: 3.0371 (3.1386)  loss_scale: 8192.0000 (10126.1642)  weight_decay: 0.0500 (0.0500)  time: 0.6918  data: 0.1049  max mem: 13007
Epoch: [1]  [1970/3542]  eta: 0:19:34  lr: 0.000040  min_lr: 0.000040  loss: 3.1055 (3.1383)  loss_scale: 8192.0000 (10116.3511)  weight_decay: 0.0500 (0.0500)  time: 0.7050  data: 0.1191  max mem: 13007
Epoch: [1]  [1980/3542]  eta: 0:19:26  lr: 0.000040  min_lr: 0.000040  loss: 3.1367 (3.1382)  loss_scale: 8192.0000 (10106.6371)  weight_decay: 0.0500 (0.0500)  time: 0.6873  data: 0.1022  max mem: 13007
Epoch: [1]  [1990/3542]  eta: 0:19:19  lr: 0.000040  min_lr: 0.000040  loss: 3.0723 (3.1383)  loss_scale: 8192.0000 (10097.0206)  weight_decay: 0.0500 (0.0500)  time: 0.7034  data: 0.1173  max mem: 13007
Epoch: [1]  [2000/3542]  eta: 0:19:11  lr: 0.000040  min_lr: 0.000040  loss: 3.0352 (3.1381)  loss_scale: 8192.0000 (10087.5002)  weight_decay: 0.0500 (0.0500)  time: 0.7108  data: 0.1253  max mem: 13007
Epoch: [1]  [2010/3542]  eta: 0:19:03  lr: 0.000040  min_lr: 0.000040  loss: 3.1035 (3.1381)  loss_scale: 8192.0000 (10078.0746)  weight_decay: 0.0500 (0.0500)  time: 0.7170  data: 0.1319  max mem: 13007
Epoch: [1]  [2020/3542]  eta: 0:18:56  lr: 0.000040  min_lr: 0.000040  loss: 3.0391 (3.1379)  loss_scale: 8192.0000 (10068.7422)  weight_decay: 0.0500 (0.0500)  time: 0.7433  data: 0.1576  max mem: 13007
Epoch: [1]  [2030/3542]  eta: 0:18:48  lr: 0.000040  min_lr: 0.000040  loss: 3.0410 (3.1375)  loss_scale: 8192.0000 (10059.5017)  weight_decay: 0.0500 (0.0500)  time: 0.7426  data: 0.1573  max mem: 13007
Epoch: [1]  [2040/3542]  eta: 0:18:41  lr: 0.000040  min_lr: 0.000040  loss: 3.0742 (3.1373)  loss_scale: 8192.0000 (10050.3518)  weight_decay: 0.0500 (0.0500)  time: 0.7176  data: 0.1319  max mem: 13007
Epoch: [1]  [2050/3542]  eta: 0:18:33  lr: 0.000040  min_lr: 0.000040  loss: 3.1211 (3.1373)  loss_scale: 8192.0000 (10041.2911)  weight_decay: 0.0500 (0.0500)  time: 0.7265  data: 0.1406  max mem: 13007
Epoch: [1]  [2060/3542]  eta: 0:18:25  lr: 0.000040  min_lr: 0.000040  loss: 3.1211 (3.1371)  loss_scale: 8192.0000 (10032.3183)  weight_decay: 0.0500 (0.0500)  time: 0.7158  data: 0.1298  max mem: 13007
Epoch: [1]  [2070/3542]  eta: 0:18:17  lr: 0.000040  min_lr: 0.000040  loss: 3.1289 (3.1373)  loss_scale: 8192.0000 (10023.4322)  weight_decay: 0.0500 (0.0500)  time: 0.6859  data: 0.1006  max mem: 13007
Epoch: [1]  [2080/3542]  eta: 0:18:09  lr: 0.000040  min_lr: 0.000040  loss: 3.1230 (3.1370)  loss_scale: 8192.0000 (10014.6314)  weight_decay: 0.0500 (0.0500)  time: 0.6871  data: 0.1024  max mem: 13007
Epoch: [1]  [2090/3542]  eta: 0:18:02  lr: 0.000040  min_lr: 0.000040  loss: 3.0527 (3.1365)  loss_scale: 8192.0000 (10005.9149)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.1052  max mem: 13007
Epoch: [1]  [2100/3542]  eta: 0:17:54  lr: 0.000040  min_lr: 0.000040  loss: 3.0898 (3.1365)  loss_scale: 8192.0000 (9997.2813)  weight_decay: 0.0500 (0.0500)  time: 0.7069  data: 0.1209  max mem: 13007
Epoch: [1]  [2110/3542]  eta: 0:17:47  lr: 0.000040  min_lr: 0.000040  loss: 3.1367 (3.1365)  loss_scale: 8192.0000 (9988.7295)  weight_decay: 0.0500 (0.0500)  time: 0.7422  data: 0.1561  max mem: 13007
Epoch: [1]  [2120/3542]  eta: 0:17:39  lr: 0.000040  min_lr: 0.000040  loss: 3.1504 (3.1364)  loss_scale: 8192.0000 (9980.2584)  weight_decay: 0.0500 (0.0500)  time: 0.7415  data: 0.1558  max mem: 13007
Epoch: [1]  [2130/3542]  eta: 0:17:32  lr: 0.000040  min_lr: 0.000040  loss: 3.0352 (3.1356)  loss_scale: 8192.0000 (9971.8667)  weight_decay: 0.0500 (0.0500)  time: 0.7406  data: 0.1551  max mem: 13007
Epoch: [1]  [2140/3542]  eta: 0:17:24  lr: 0.000040  min_lr: 0.000040  loss: 3.0449 (3.1356)  loss_scale: 8192.0000 (9963.5535)  weight_decay: 0.0500 (0.0500)  time: 0.7532  data: 0.1681  max mem: 13007
Epoch: [1]  [2150/3542]  eta: 0:17:17  lr: 0.000040  min_lr: 0.000040  loss: 3.2012 (3.1359)  loss_scale: 8192.0000 (9955.3175)  weight_decay: 0.0500 (0.0500)  time: 0.7254  data: 0.1406  max mem: 13007
Epoch: [1]  [2160/3542]  eta: 0:17:09  lr: 0.000040  min_lr: 0.000040  loss: 3.1328 (3.1356)  loss_scale: 8192.0000 (9947.1578)  weight_decay: 0.0500 (0.0500)  time: 0.7367  data: 0.1514  max mem: 13007
Epoch: [1]  [2170/3542]  eta: 0:17:02  lr: 0.000040  min_lr: 0.000040  loss: 3.0586 (3.1352)  loss_scale: 8192.0000 (9939.0732)  weight_decay: 0.0500 (0.0500)  time: 0.7499  data: 0.1639  max mem: 13007
Epoch: [1]  [2180/3542]  eta: 0:16:54  lr: 0.000040  min_lr: 0.000040  loss: 3.0625 (3.1351)  loss_scale: 8192.0000 (9931.0628)  weight_decay: 0.0500 (0.0500)  time: 0.7340  data: 0.1474  max mem: 13007
Epoch: [1]  [2190/3542]  eta: 0:16:46  lr: 0.000040  min_lr: 0.000040  loss: 3.0176 (3.1348)  loss_scale: 8192.0000 (9923.1255)  weight_decay: 0.0500 (0.0500)  time: 0.7193  data: 0.1330  max mem: 13007
Epoch: [1]  [2200/3542]  eta: 0:16:39  lr: 0.000040  min_lr: 0.000040  loss: 3.0117 (3.1346)  loss_scale: 8192.0000 (9915.2603)  weight_decay: 0.0500 (0.0500)  time: 0.7330  data: 0.1469  max mem: 13007
Epoch: [1]  [2210/3542]  eta: 0:16:32  lr: 0.000040  min_lr: 0.000040  loss: 3.0879 (3.1344)  loss_scale: 8192.0000 (9907.4663)  weight_decay: 0.0500 (0.0500)  time: 0.7664  data: 0.1804  max mem: 13007
Epoch: [1]  [2220/3542]  eta: 0:16:24  lr: 0.000040  min_lr: 0.000040  loss: 3.0859 (3.1341)  loss_scale: 8192.0000 (9899.7425)  weight_decay: 0.0500 (0.0500)  time: 0.7166  data: 0.1310  max mem: 13007
Epoch: [1]  [2230/3542]  eta: 0:16:17  lr: 0.000040  min_lr: 0.000040  loss: 3.0566 (3.1338)  loss_scale: 8192.0000 (9892.0879)  weight_decay: 0.0500 (0.0500)  time: 0.7113  data: 0.1260  max mem: 13007
Epoch: [1]  [2240/3542]  eta: 0:16:09  lr: 0.000040  min_lr: 0.000040  loss: 3.0449 (3.1334)  loss_scale: 8192.0000 (9884.5016)  weight_decay: 0.0500 (0.0500)  time: 0.7435  data: 0.1583  max mem: 13007
Epoch: [1]  [2250/3542]  eta: 0:16:01  lr: 0.000040  min_lr: 0.000040  loss: 3.0332 (3.1330)  loss_scale: 8192.0000 (9876.9827)  weight_decay: 0.0500 (0.0500)  time: 0.7276  data: 0.1418  max mem: 13007
Epoch: [1]  [2260/3542]  eta: 0:15:54  lr: 0.000040  min_lr: 0.000040  loss: 3.0762 (3.1331)  loss_scale: 8192.0000 (9869.5303)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.1425  max mem: 13007
Epoch: [1]  [2270/3542]  eta: 0:15:46  lr: 0.000040  min_lr: 0.000040  loss: 3.0762 (3.1324)  loss_scale: 8192.0000 (9862.1435)  weight_decay: 0.0500 (0.0500)  time: 0.7180  data: 0.1331  max mem: 13007
Epoch: [1]  [2280/3542]  eta: 0:15:38  lr: 0.000040  min_lr: 0.000040  loss: 3.0059 (3.1320)  loss_scale: 8192.0000 (9854.8216)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.1005  max mem: 13007
Epoch: [1]  [2290/3542]  eta: 0:15:31  lr: 0.000040  min_lr: 0.000040  loss: 3.1270 (3.1324)  loss_scale: 8192.0000 (9847.5635)  weight_decay: 0.0500 (0.0500)  time: 0.6936  data: 0.1075  max mem: 13007
Epoch: [1]  [2300/3542]  eta: 0:15:23  lr: 0.000040  min_lr: 0.000040  loss: 3.1250 (3.1321)  loss_scale: 8192.0000 (9840.3685)  weight_decay: 0.0500 (0.0500)  time: 0.7351  data: 0.1495  max mem: 13007
Epoch: [1]  [2310/3542]  eta: 0:15:16  lr: 0.000039  min_lr: 0.000039  loss: 3.0820 (3.1321)  loss_scale: 8192.0000 (9833.2358)  weight_decay: 0.0500 (0.0500)  time: 0.7271  data: 0.1413  max mem: 13007
Epoch: [1]  [2320/3542]  eta: 0:15:08  lr: 0.000039  min_lr: 0.000039  loss: 3.1465 (3.1321)  loss_scale: 8192.0000 (9826.1646)  weight_decay: 0.0500 (0.0500)  time: 0.7145  data: 0.1289  max mem: 13007
Epoch: [1]  [2330/3542]  eta: 0:15:01  lr: 0.000039  min_lr: 0.000039  loss: 3.1133 (3.1318)  loss_scale: 8192.0000 (9819.1540)  weight_decay: 0.0500 (0.0500)  time: 0.7265  data: 0.1410  max mem: 13007
Epoch: [1]  [2340/3542]  eta: 0:14:53  lr: 0.000039  min_lr: 0.000039  loss: 3.0840 (3.1316)  loss_scale: 8192.0000 (9812.2033)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.0960  max mem: 13007
Epoch: [1]  [2350/3542]  eta: 0:14:46  lr: 0.000039  min_lr: 0.000039  loss: 3.1016 (3.1317)  loss_scale: 8192.0000 (9805.3118)  weight_decay: 0.0500 (0.0500)  time: 0.7111  data: 0.1256  max mem: 13007
Epoch: [1]  [2360/3542]  eta: 0:14:38  lr: 0.000039  min_lr: 0.000039  loss: 3.1426 (3.1317)  loss_scale: 8192.0000 (9798.4786)  weight_decay: 0.0500 (0.0500)  time: 0.7308  data: 0.1449  max mem: 13007
Epoch: [1]  [2370/3542]  eta: 0:14:30  lr: 0.000039  min_lr: 0.000039  loss: 3.0703 (3.1315)  loss_scale: 8192.0000 (9791.7031)  weight_decay: 0.0500 (0.0500)  time: 0.7025  data: 0.1163  max mem: 13007
[2023-05-16 06:56:07,031] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 06:56:07,031] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [1]  [2380/3542]  eta: 0:14:23  lr: 0.000039  min_lr: 0.000039  loss: 3.0684 (3.1312)  loss_scale: 8192.0000 (9812.5090)  weight_decay: 0.0500 (0.0500)  time: 0.7193  data: 0.1336  max mem: 13007
Epoch: [1]  [2390/3542]  eta: 0:14:15  lr: 0.000039  min_lr: 0.000039  loss: 3.0684 (3.1310)  loss_scale: 16384.0000 (9839.9933)  weight_decay: 0.0500 (0.0500)  time: 0.7171  data: 0.1317  max mem: 13007
Epoch: [1]  [2400/3542]  eta: 0:14:08  lr: 0.000039  min_lr: 0.000039  loss: 3.0176 (3.1303)  loss_scale: 16384.0000 (9867.2486)  weight_decay: 0.0500 (0.0500)  time: 0.7222  data: 0.1361  max mem: 13007
Epoch: [1]  [2410/3542]  eta: 0:14:00  lr: 0.000039  min_lr: 0.000039  loss: 2.9824 (3.1302)  loss_scale: 16384.0000 (9894.2779)  weight_decay: 0.0500 (0.0500)  time: 0.7285  data: 0.1420  max mem: 13007
Epoch: [1]  [2420/3542]  eta: 0:13:53  lr: 0.000039  min_lr: 0.000039  loss: 3.1191 (3.1305)  loss_scale: 16384.0000 (9921.0838)  weight_decay: 0.0500 (0.0500)  time: 0.7493  data: 0.1636  max mem: 13007
Epoch: [1]  [2430/3542]  eta: 0:13:45  lr: 0.000039  min_lr: 0.000039  loss: 3.1289 (3.1304)  loss_scale: 16384.0000 (9947.6693)  weight_decay: 0.0500 (0.0500)  time: 0.7511  data: 0.1657  max mem: 13007
Epoch: [1]  [2440/3542]  eta: 0:13:38  lr: 0.000039  min_lr: 0.000039  loss: 3.0957 (3.1302)  loss_scale: 16384.0000 (9974.0369)  weight_decay: 0.0500 (0.0500)  time: 0.7383  data: 0.1528  max mem: 13007
Epoch: [1]  [2450/3542]  eta: 0:13:30  lr: 0.000039  min_lr: 0.000039  loss: 3.0547 (3.1299)  loss_scale: 16384.0000 (10000.1893)  weight_decay: 0.0500 (0.0500)  time: 0.7330  data: 0.1478  max mem: 13007
[2023-05-16 06:57:09,436] [INFO] [logging.py:60:log_dist] [Rank 0] step=6000, skipped=7, lr=[3.943113452863373e-05, 3.943113452863373e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 06:57:09,439] [INFO] [timer.py:157:stop] 0/6000, SamplesPerSec=55.61085495452993
Epoch: [1]  [2460/3542]  eta: 0:13:23  lr: 0.000039  min_lr: 0.000039  loss: 3.0547 (3.1299)  loss_scale: 16384.0000 (10026.1292)  weight_decay: 0.0500 (0.0500)  time: 0.7293  data: 0.1442  max mem: 13007
Epoch: [1]  [2470/3542]  eta: 0:13:15  lr: 0.000039  min_lr: 0.000039  loss: 3.0352 (3.1294)  loss_scale: 16384.0000 (10051.8592)  weight_decay: 0.0500 (0.0500)  time: 0.7203  data: 0.1343  max mem: 13007
Epoch: [1]  [2480/3542]  eta: 0:13:08  lr: 0.000039  min_lr: 0.000039  loss: 3.0078 (3.1292)  loss_scale: 16384.0000 (10077.3817)  weight_decay: 0.0500 (0.0500)  time: 0.7008  data: 0.1145  max mem: 13007
Epoch: [1]  [2490/3542]  eta: 0:13:00  lr: 0.000039  min_lr: 0.000039  loss: 3.0254 (3.1288)  loss_scale: 16384.0000 (10102.6993)  weight_decay: 0.0500 (0.0500)  time: 0.7076  data: 0.1213  max mem: 13007
Epoch: [1]  [2500/3542]  eta: 0:12:53  lr: 0.000039  min_lr: 0.000039  loss: 3.0332 (3.1285)  loss_scale: 16384.0000 (10127.8145)  weight_decay: 0.0500 (0.0500)  time: 0.7307  data: 0.1446  max mem: 13007
[2023-05-16 06:57:40,421] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 6043
[2023-05-16 06:57:40,421] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 06:57:40,421] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [1]  [2510/3542]  eta: 0:12:45  lr: 0.000039  min_lr: 0.000039  loss: 3.0332 (3.1281)  loss_scale: 8192.0000 (10120.1051)  weight_decay: 0.0500 (0.0500)  time: 0.7429  data: 0.1580  max mem: 13007
Epoch: [1]  [2520/3542]  eta: 0:12:38  lr: 0.000039  min_lr: 0.000039  loss: 3.0781 (3.1280)  loss_scale: 8192.0000 (10112.4570)  weight_decay: 0.0500 (0.0500)  time: 0.7318  data: 0.1470  max mem: 13007
Epoch: [1]  [2530/3542]  eta: 0:12:30  lr: 0.000039  min_lr: 0.000039  loss: 3.0781 (3.1276)  loss_scale: 8192.0000 (10104.8692)  weight_decay: 0.0500 (0.0500)  time: 0.7125  data: 0.1277  max mem: 13007
Epoch: [1]  [2540/3542]  eta: 0:12:23  lr: 0.000039  min_lr: 0.000039  loss: 3.0566 (3.1274)  loss_scale: 8192.0000 (10097.3412)  weight_decay: 0.0500 (0.0500)  time: 0.7205  data: 0.1349  max mem: 13007
Epoch: [1]  [2550/3542]  eta: 0:12:16  lr: 0.000039  min_lr: 0.000039  loss: 3.0488 (3.1272)  loss_scale: 8192.0000 (10089.8722)  weight_decay: 0.0500 (0.0500)  time: 0.7506  data: 0.1643  max mem: 13007
Epoch: [1]  [2560/3542]  eta: 0:12:08  lr: 0.000039  min_lr: 0.000039  loss: 3.0117 (3.1269)  loss_scale: 8192.0000 (10082.4615)  weight_decay: 0.0500 (0.0500)  time: 0.7285  data: 0.1423  max mem: 13007
Epoch: [1]  [2570/3542]  eta: 0:12:01  lr: 0.000039  min_lr: 0.000039  loss: 3.0273 (3.1266)  loss_scale: 8192.0000 (10075.1085)  weight_decay: 0.0500 (0.0500)  time: 0.7280  data: 0.1417  max mem: 13007
Epoch: [1]  [2580/3542]  eta: 0:11:53  lr: 0.000039  min_lr: 0.000039  loss: 3.0898 (3.1267)  loss_scale: 8192.0000 (10067.8125)  weight_decay: 0.0500 (0.0500)  time: 0.6976  data: 0.1116  max mem: 13007
Epoch: [1]  [2590/3542]  eta: 0:11:45  lr: 0.000039  min_lr: 0.000039  loss: 3.0898 (3.1264)  loss_scale: 8192.0000 (10060.5728)  weight_decay: 0.0500 (0.0500)  time: 0.6663  data: 0.0807  max mem: 13007
Epoch: [1]  [2600/3542]  eta: 0:11:38  lr: 0.000039  min_lr: 0.000039  loss: 3.0000 (3.1260)  loss_scale: 8192.0000 (10053.3887)  weight_decay: 0.0500 (0.0500)  time: 0.7219  data: 0.1356  max mem: 13007
Epoch: [1]  [2610/3542]  eta: 0:11:30  lr: 0.000039  min_lr: 0.000039  loss: 3.0371 (3.1258)  loss_scale: 8192.0000 (10046.2597)  weight_decay: 0.0500 (0.0500)  time: 0.7061  data: 0.1198  max mem: 13007
Epoch: [1]  [2620/3542]  eta: 0:11:22  lr: 0.000039  min_lr: 0.000039  loss: 3.1211 (3.1259)  loss_scale: 8192.0000 (10039.1850)  weight_decay: 0.0500 (0.0500)  time: 0.6703  data: 0.0844  max mem: 13007
Epoch: [1]  [2630/3542]  eta: 0:11:15  lr: 0.000039  min_lr: 0.000039  loss: 3.1211 (3.1256)  loss_scale: 8192.0000 (10032.1642)  weight_decay: 0.0500 (0.0500)  time: 0.6992  data: 0.1146  max mem: 13007
Epoch: [1]  [2640/3542]  eta: 0:11:08  lr: 0.000039  min_lr: 0.000039  loss: 2.9277 (3.1250)  loss_scale: 8192.0000 (10025.1965)  weight_decay: 0.0500 (0.0500)  time: 0.7261  data: 0.1420  max mem: 13007
Epoch: [1]  [2650/3542]  eta: 0:11:00  lr: 0.000039  min_lr: 0.000039  loss: 2.9570 (3.1248)  loss_scale: 8192.0000 (10018.2814)  weight_decay: 0.0500 (0.0500)  time: 0.7159  data: 0.1306  max mem: 13007
Epoch: [1]  [2660/3542]  eta: 0:10:52  lr: 0.000039  min_lr: 0.000039  loss: 3.0723 (3.1247)  loss_scale: 8192.0000 (10011.4183)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.1049  max mem: 13007
Epoch: [1]  [2670/3542]  eta: 0:10:45  lr: 0.000039  min_lr: 0.000039  loss: 3.1445 (3.1248)  loss_scale: 8192.0000 (10004.6065)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.0679  max mem: 13007
Epoch: [1]  [2680/3542]  eta: 0:10:37  lr: 0.000039  min_lr: 0.000039  loss: 3.0391 (3.1244)  loss_scale: 8192.0000 (9997.8456)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.0728  max mem: 13007
Epoch: [1]  [2690/3542]  eta: 0:10:30  lr: 0.000039  min_lr: 0.000039  loss: 3.0352 (3.1243)  loss_scale: 8192.0000 (9991.1349)  weight_decay: 0.0500 (0.0500)  time: 0.6873  data: 0.1011  max mem: 13007
Epoch: [1]  [2700/3542]  eta: 0:10:22  lr: 0.000039  min_lr: 0.000039  loss: 3.0527 (3.1241)  loss_scale: 8192.0000 (9984.4739)  weight_decay: 0.0500 (0.0500)  time: 0.6940  data: 0.1069  max mem: 13007
Epoch: [1]  [2710/3542]  eta: 0:10:15  lr: 0.000039  min_lr: 0.000039  loss: 3.0273 (3.1239)  loss_scale: 8192.0000 (9977.8620)  weight_decay: 0.0500 (0.0500)  time: 0.7130  data: 0.1254  max mem: 13007
Epoch: [1]  [2720/3542]  eta: 0:10:07  lr: 0.000039  min_lr: 0.000039  loss: 3.1016 (3.1237)  loss_scale: 8192.0000 (9971.2988)  weight_decay: 0.0500 (0.0500)  time: 0.7389  data: 0.1533  max mem: 13007
Epoch: [1]  [2730/3542]  eta: 0:10:00  lr: 0.000039  min_lr: 0.000039  loss: 3.0762 (3.1237)  loss_scale: 8192.0000 (9964.7836)  weight_decay: 0.0500 (0.0500)  time: 0.7154  data: 0.1299  max mem: 13007
Epoch: [1]  [2740/3542]  eta: 0:09:52  lr: 0.000039  min_lr: 0.000039  loss: 3.0254 (3.1232)  loss_scale: 8192.0000 (9958.3159)  weight_decay: 0.0500 (0.0500)  time: 0.7132  data: 0.1272  max mem: 13007
Epoch: [1]  [2750/3542]  eta: 0:09:45  lr: 0.000039  min_lr: 0.000039  loss: 2.9512 (3.1228)  loss_scale: 8192.0000 (9951.8953)  weight_decay: 0.0500 (0.0500)  time: 0.7312  data: 0.1455  max mem: 13007
Epoch: [1]  [2760/3542]  eta: 0:09:37  lr: 0.000039  min_lr: 0.000039  loss: 3.0195 (3.1225)  loss_scale: 8192.0000 (9945.5212)  weight_decay: 0.0500 (0.0500)  time: 0.7042  data: 0.1183  max mem: 13007
Epoch: [1]  [2770/3542]  eta: 0:09:30  lr: 0.000039  min_lr: 0.000039  loss: 3.0625 (3.1224)  loss_scale: 8192.0000 (9939.1931)  weight_decay: 0.0500 (0.0500)  time: 0.6920  data: 0.1060  max mem: 13007
Epoch: [1]  [2780/3542]  eta: 0:09:22  lr: 0.000039  min_lr: 0.000039  loss: 3.0332 (3.1221)  loss_scale: 8192.0000 (9932.9105)  weight_decay: 0.0500 (0.0500)  time: 0.6979  data: 0.1122  max mem: 13007
Epoch: [1]  [2790/3542]  eta: 0:09:15  lr: 0.000039  min_lr: 0.000039  loss: 2.9688 (3.1217)  loss_scale: 8192.0000 (9926.6729)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.1000  max mem: 13007
Epoch: [1]  [2800/3542]  eta: 0:09:07  lr: 0.000039  min_lr: 0.000039  loss: 2.9648 (3.1213)  loss_scale: 8192.0000 (9920.4798)  weight_decay: 0.0500 (0.0500)  time: 0.6888  data: 0.1032  max mem: 13007
Epoch: [1]  [2810/3542]  eta: 0:09:00  lr: 0.000039  min_lr: 0.000039  loss: 2.9922 (3.1211)  loss_scale: 8192.0000 (9914.3308)  weight_decay: 0.0500 (0.0500)  time: 0.6976  data: 0.1119  max mem: 13007
Epoch: [1]  [2820/3542]  eta: 0:08:52  lr: 0.000039  min_lr: 0.000039  loss: 3.0039 (3.1207)  loss_scale: 8192.0000 (9908.2255)  weight_decay: 0.0500 (0.0500)  time: 0.6848  data: 0.1000  max mem: 13007
Epoch: [1]  [2830/3542]  eta: 0:08:45  lr: 0.000039  min_lr: 0.000039  loss: 3.1152 (3.1208)  loss_scale: 8192.0000 (9902.1632)  weight_decay: 0.0500 (0.0500)  time: 0.6992  data: 0.1149  max mem: 13007
Epoch: [1]  [2840/3542]  eta: 0:08:37  lr: 0.000039  min_lr: 0.000039  loss: 3.1152 (3.1207)  loss_scale: 8192.0000 (9896.1436)  weight_decay: 0.0500 (0.0500)  time: 0.7258  data: 0.1409  max mem: 13007
Epoch: [1]  [2850/3542]  eta: 0:08:30  lr: 0.000039  min_lr: 0.000039  loss: 2.9883 (3.1201)  loss_scale: 8192.0000 (9890.1663)  weight_decay: 0.0500 (0.0500)  time: 0.7128  data: 0.1272  max mem: 13007
Epoch: [1]  [2860/3542]  eta: 0:08:23  lr: 0.000039  min_lr: 0.000039  loss: 3.0254 (3.1199)  loss_scale: 8192.0000 (9884.2307)  weight_decay: 0.0500 (0.0500)  time: 0.7069  data: 0.1213  max mem: 13007
Epoch: [1]  [2870/3542]  eta: 0:08:15  lr: 0.000039  min_lr: 0.000039  loss: 3.0527 (3.1195)  loss_scale: 8192.0000 (9878.3365)  weight_decay: 0.0500 (0.0500)  time: 0.7331  data: 0.1467  max mem: 13007
Epoch: [1]  [2880/3542]  eta: 0:08:08  lr: 0.000039  min_lr: 0.000039  loss: 3.0234 (3.1192)  loss_scale: 8192.0000 (9872.4832)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.1081  max mem: 13007
Epoch: [1]  [2890/3542]  eta: 0:08:00  lr: 0.000039  min_lr: 0.000039  loss: 3.0234 (3.1188)  loss_scale: 8192.0000 (9866.6704)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.1029  max mem: 13007
Epoch: [1]  [2900/3542]  eta: 0:07:53  lr: 0.000039  min_lr: 0.000039  loss: 3.0586 (3.1186)  loss_scale: 8192.0000 (9860.8976)  weight_decay: 0.0500 (0.0500)  time: 0.7365  data: 0.1506  max mem: 13007
Epoch: [1]  [2910/3542]  eta: 0:07:45  lr: 0.000039  min_lr: 0.000039  loss: 3.0781 (3.1187)  loss_scale: 8192.0000 (9855.1645)  weight_decay: 0.0500 (0.0500)  time: 0.7392  data: 0.1534  max mem: 13007
Epoch: [1]  [2920/3542]  eta: 0:07:38  lr: 0.000039  min_lr: 0.000039  loss: 3.0781 (3.1185)  loss_scale: 8192.0000 (9849.4707)  weight_decay: 0.0500 (0.0500)  time: 0.7244  data: 0.1383  max mem: 13007
Epoch: [1]  [2930/3542]  eta: 0:07:31  lr: 0.000039  min_lr: 0.000039  loss: 3.0781 (3.1184)  loss_scale: 8192.0000 (9843.8158)  weight_decay: 0.0500 (0.0500)  time: 0.7382  data: 0.1524  max mem: 13007
Epoch: [1]  [2940/3542]  eta: 0:07:23  lr: 0.000039  min_lr: 0.000039  loss: 3.0605 (3.1181)  loss_scale: 8192.0000 (9838.1993)  weight_decay: 0.0500 (0.0500)  time: 0.7531  data: 0.1673  max mem: 13007
Epoch: [1]  [2950/3542]  eta: 0:07:16  lr: 0.000039  min_lr: 0.000039  loss: 3.0566 (3.1180)  loss_scale: 8192.0000 (9832.6208)  weight_decay: 0.0500 (0.0500)  time: 0.7707  data: 0.1851  max mem: 13007
Epoch: [1]  [2960/3542]  eta: 0:07:09  lr: 0.000039  min_lr: 0.000039  loss: 3.0137 (3.1176)  loss_scale: 8192.0000 (9827.0800)  weight_decay: 0.0500 (0.0500)  time: 0.7903  data: 0.2052  max mem: 13007
Epoch: [1]  [2970/3542]  eta: 0:07:01  lr: 0.000039  min_lr: 0.000039  loss: 2.9648 (3.1173)  loss_scale: 8192.0000 (9821.5766)  weight_decay: 0.0500 (0.0500)  time: 0.7414  data: 0.1561  max mem: 13007
Epoch: [1]  [2980/3542]  eta: 0:06:54  lr: 0.000039  min_lr: 0.000039  loss: 3.0000 (3.1169)  loss_scale: 8192.0000 (9816.1100)  weight_decay: 0.0500 (0.0500)  time: 0.7363  data: 0.1509  max mem: 13007
Epoch: [1]  [2990/3542]  eta: 0:06:47  lr: 0.000039  min_lr: 0.000039  loss: 3.0469 (3.1167)  loss_scale: 8192.0000 (9810.6800)  weight_decay: 0.0500 (0.0500)  time: 0.7665  data: 0.1806  max mem: 13007
Epoch: [1]  [3000/3542]  eta: 0:06:39  lr: 0.000039  min_lr: 0.000039  loss: 3.0469 (3.1165)  loss_scale: 8192.0000 (9805.2862)  weight_decay: 0.0500 (0.0500)  time: 0.7226  data: 0.1371  max mem: 13007
Epoch: [1]  [3010/3542]  eta: 0:06:32  lr: 0.000039  min_lr: 0.000039  loss: 3.0742 (3.1167)  loss_scale: 8192.0000 (9799.9283)  weight_decay: 0.0500 (0.0500)  time: 0.7042  data: 0.1196  max mem: 13007
Epoch: [1]  [3020/3542]  eta: 0:06:25  lr: 0.000039  min_lr: 0.000039  loss: 3.1621 (3.1169)  loss_scale: 8192.0000 (9794.6058)  weight_decay: 0.0500 (0.0500)  time: 0.7557  data: 0.1709  max mem: 13007
Epoch: [1]  [3030/3542]  eta: 0:06:17  lr: 0.000039  min_lr: 0.000039  loss: 3.0840 (3.1167)  loss_scale: 8192.0000 (9789.3184)  weight_decay: 0.0500 (0.0500)  time: 0.8035  data: 0.2180  max mem: 13007
Epoch: [1]  [3040/3542]  eta: 0:06:10  lr: 0.000039  min_lr: 0.000039  loss: 3.0020 (3.1163)  loss_scale: 8192.0000 (9784.0658)  weight_decay: 0.0500 (0.0500)  time: 0.7578  data: 0.1723  max mem: 13007
Epoch: [1]  [3050/3542]  eta: 0:06:02  lr: 0.000039  min_lr: 0.000039  loss: 3.0117 (3.1163)  loss_scale: 8192.0000 (9778.8476)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.1033  max mem: 13007
Epoch: [1]  [3060/3542]  eta: 0:05:55  lr: 0.000039  min_lr: 0.000039  loss: 3.0820 (3.1161)  loss_scale: 8192.0000 (9773.6635)  weight_decay: 0.0500 (0.0500)  time: 0.7376  data: 0.1514  max mem: 13007
Epoch: [1]  [3070/3542]  eta: 0:05:48  lr: 0.000039  min_lr: 0.000039  loss: 3.0879 (3.1160)  loss_scale: 8192.0000 (9768.5132)  weight_decay: 0.0500 (0.0500)  time: 0.7406  data: 0.1550  max mem: 13007
Epoch: [1]  [3080/3542]  eta: 0:05:40  lr: 0.000039  min_lr: 0.000039  loss: 3.0703 (3.1159)  loss_scale: 8192.0000 (9763.3963)  weight_decay: 0.0500 (0.0500)  time: 0.7000  data: 0.1150  max mem: 13007
Epoch: [1]  [3090/3542]  eta: 0:05:33  lr: 0.000039  min_lr: 0.000039  loss: 3.0625 (3.1157)  loss_scale: 8192.0000 (9758.3125)  weight_decay: 0.0500 (0.0500)  time: 0.7048  data: 0.1193  max mem: 13007
Epoch: [1]  [3100/3542]  eta: 0:05:25  lr: 0.000039  min_lr: 0.000039  loss: 3.0898 (3.1156)  loss_scale: 8192.0000 (9753.2615)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.1094  max mem: 13007
Epoch: [1]  [3110/3542]  eta: 0:05:18  lr: 0.000039  min_lr: 0.000039  loss: 3.0762 (3.1152)  loss_scale: 8192.0000 (9748.2430)  weight_decay: 0.0500 (0.0500)  time: 0.6684  data: 0.0825  max mem: 13007
Epoch: [1]  [3120/3542]  eta: 0:05:11  lr: 0.000039  min_lr: 0.000039  loss: 3.0684 (3.1150)  loss_scale: 8192.0000 (9743.2566)  weight_decay: 0.0500 (0.0500)  time: 0.7030  data: 0.1174  max mem: 13007
Epoch: [1]  [3130/3542]  eta: 0:05:03  lr: 0.000039  min_lr: 0.000039  loss: 2.9980 (3.1148)  loss_scale: 8192.0000 (9738.3021)  weight_decay: 0.0500 (0.0500)  time: 0.7146  data: 0.1293  max mem: 13007
Epoch: [1]  [3140/3542]  eta: 0:04:56  lr: 0.000039  min_lr: 0.000039  loss: 3.0234 (3.1147)  loss_scale: 8192.0000 (9733.3792)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.0941  max mem: 13007
Epoch: [1]  [3150/3542]  eta: 0:04:48  lr: 0.000039  min_lr: 0.000039  loss: 3.0352 (3.1146)  loss_scale: 8192.0000 (9728.4875)  weight_decay: 0.0500 (0.0500)  time: 0.6993  data: 0.1138  max mem: 13007
Epoch: [1]  [3160/3542]  eta: 0:04:41  lr: 0.000039  min_lr: 0.000039  loss: 3.0039 (3.1143)  loss_scale: 8192.0000 (9723.6267)  weight_decay: 0.0500 (0.0500)  time: 0.7107  data: 0.1255  max mem: 13007
Epoch: [1]  [3170/3542]  eta: 0:04:34  lr: 0.000039  min_lr: 0.000039  loss: 3.0293 (3.1142)  loss_scale: 8192.0000 (9718.7966)  weight_decay: 0.0500 (0.0500)  time: 0.7385  data: 0.1537  max mem: 13007
Epoch: [1]  [3180/3542]  eta: 0:04:26  lr: 0.000039  min_lr: 0.000039  loss: 3.0684 (3.1142)  loss_scale: 8192.0000 (9713.9969)  weight_decay: 0.0500 (0.0500)  time: 0.7639  data: 0.1786  max mem: 13007
Epoch: [1]  [3190/3542]  eta: 0:04:19  lr: 0.000039  min_lr: 0.000039  loss: 3.0684 (3.1141)  loss_scale: 8192.0000 (9709.2272)  weight_decay: 0.0500 (0.0500)  time: 0.7133  data: 0.1275  max mem: 13007
Epoch: [1]  [3200/3542]  eta: 0:04:11  lr: 0.000039  min_lr: 0.000039  loss: 3.0273 (3.1139)  loss_scale: 8192.0000 (9704.4873)  weight_decay: 0.0500 (0.0500)  time: 0.6801  data: 0.0948  max mem: 13007
Epoch: [1]  [3210/3542]  eta: 0:04:04  lr: 0.000039  min_lr: 0.000039  loss: 3.0371 (3.1135)  loss_scale: 8192.0000 (9699.7770)  weight_decay: 0.0500 (0.0500)  time: 0.7082  data: 0.1228  max mem: 13007
Epoch: [1]  [3220/3542]  eta: 0:03:57  lr: 0.000039  min_lr: 0.000039  loss: 3.0703 (3.1134)  loss_scale: 8192.0000 (9695.0959)  weight_decay: 0.0500 (0.0500)  time: 0.7063  data: 0.1211  max mem: 13007
Epoch: [1]  [3230/3542]  eta: 0:03:49  lr: 0.000039  min_lr: 0.000039  loss: 3.1367 (3.1135)  loss_scale: 8192.0000 (9690.4438)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.1105  max mem: 13007
Epoch: [1]  [3240/3542]  eta: 0:03:42  lr: 0.000039  min_lr: 0.000039  loss: 3.1328 (3.1133)  loss_scale: 8192.0000 (9685.8204)  weight_decay: 0.0500 (0.0500)  time: 0.7314  data: 0.1458  max mem: 13007
Epoch: [1]  [3250/3542]  eta: 0:03:34  lr: 0.000039  min_lr: 0.000039  loss: 2.9844 (3.1130)  loss_scale: 8192.0000 (9681.2255)  weight_decay: 0.0500 (0.0500)  time: 0.7336  data: 0.1477  max mem: 13007
Epoch: [1]  [3260/3542]  eta: 0:03:27  lr: 0.000039  min_lr: 0.000039  loss: 2.9863 (3.1130)  loss_scale: 8192.0000 (9676.6587)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0791  max mem: 13007
Epoch: [1]  [3270/3542]  eta: 0:03:20  lr: 0.000039  min_lr: 0.000039  loss: 3.0273 (3.1129)  loss_scale: 8192.0000 (9672.1198)  weight_decay: 0.0500 (0.0500)  time: 0.7046  data: 0.1197  max mem: 13007
Epoch: [1]  [3280/3542]  eta: 0:03:12  lr: 0.000039  min_lr: 0.000039  loss: 3.0078 (3.1126)  loss_scale: 8192.0000 (9667.6087)  weight_decay: 0.0500 (0.0500)  time: 0.7441  data: 0.1594  max mem: 13007
Epoch: [1]  [3290/3542]  eta: 0:03:05  lr: 0.000039  min_lr: 0.000039  loss: 3.0234 (3.1123)  loss_scale: 8192.0000 (9663.1249)  weight_decay: 0.0500 (0.0500)  time: 0.7177  data: 0.1322  max mem: 13007
Epoch: [1]  [3300/3542]  eta: 0:02:58  lr: 0.000039  min_lr: 0.000039  loss: 2.9668 (3.1119)  loss_scale: 8192.0000 (9658.6683)  weight_decay: 0.0500 (0.0500)  time: 0.7270  data: 0.1416  max mem: 13007
Epoch: [1]  [3310/3542]  eta: 0:02:50  lr: 0.000039  min_lr: 0.000039  loss: 3.0117 (3.1115)  loss_scale: 8192.0000 (9654.2386)  weight_decay: 0.0500 (0.0500)  time: 0.7262  data: 0.1412  max mem: 13007
Epoch: [1]  [3320/3542]  eta: 0:02:43  lr: 0.000039  min_lr: 0.000039  loss: 3.0254 (3.1112)  loss_scale: 8192.0000 (9649.8356)  weight_decay: 0.0500 (0.0500)  time: 0.7268  data: 0.1411  max mem: 13007
Epoch: [1]  [3330/3542]  eta: 0:02:35  lr: 0.000039  min_lr: 0.000039  loss: 2.9707 (3.1107)  loss_scale: 8192.0000 (9645.4590)  weight_decay: 0.0500 (0.0500)  time: 0.6786  data: 0.0924  max mem: 13007
Epoch: [1]  [3340/3542]  eta: 0:02:28  lr: 0.000039  min_lr: 0.000039  loss: 2.9590 (3.1104)  loss_scale: 8192.0000 (9641.1087)  weight_decay: 0.0500 (0.0500)  time: 0.6970  data: 0.1108  max mem: 13007
Epoch: [1]  [3350/3542]  eta: 0:02:21  lr: 0.000039  min_lr: 0.000039  loss: 3.0332 (3.1102)  loss_scale: 8192.0000 (9636.7842)  weight_decay: 0.0500 (0.0500)  time: 0.7320  data: 0.1467  max mem: 13007
Epoch: [1]  [3360/3542]  eta: 0:02:13  lr: 0.000039  min_lr: 0.000039  loss: 3.0527 (3.1099)  loss_scale: 8192.0000 (9632.4856)  weight_decay: 0.0500 (0.0500)  time: 0.7358  data: 0.1503  max mem: 13007
Epoch: [1]  [3370/3542]  eta: 0:02:06  lr: 0.000039  min_lr: 0.000039  loss: 2.9902 (3.1098)  loss_scale: 8192.0000 (9628.2124)  weight_decay: 0.0500 (0.0500)  time: 0.7442  data: 0.1580  max mem: 13007
Epoch: [1]  [3380/3542]  eta: 0:01:59  lr: 0.000039  min_lr: 0.000039  loss: 2.9004 (3.1095)  loss_scale: 8192.0000 (9623.9645)  weight_decay: 0.0500 (0.0500)  time: 0.7442  data: 0.1590  max mem: 13007
Epoch: [1]  [3390/3542]  eta: 0:01:51  lr: 0.000039  min_lr: 0.000039  loss: 3.0742 (3.1095)  loss_scale: 8192.0000 (9619.7417)  weight_decay: 0.0500 (0.0500)  time: 0.7299  data: 0.1447  max mem: 13007
Epoch: [1]  [3400/3542]  eta: 0:01:44  lr: 0.000039  min_lr: 0.000039  loss: 3.0742 (3.1092)  loss_scale: 8192.0000 (9615.5437)  weight_decay: 0.0500 (0.0500)  time: 0.7108  data: 0.1253  max mem: 13007
Epoch: [1]  [3410/3542]  eta: 0:01:37  lr: 0.000039  min_lr: 0.000039  loss: 3.0781 (3.1093)  loss_scale: 8192.0000 (9611.3703)  weight_decay: 0.0500 (0.0500)  time: 0.7451  data: 0.1599  max mem: 13007
Epoch: [1]  [3420/3542]  eta: 0:01:29  lr: 0.000039  min_lr: 0.000039  loss: 3.0254 (3.1089)  loss_scale: 8192.0000 (9607.2213)  weight_decay: 0.0500 (0.0500)  time: 0.7628  data: 0.1776  max mem: 13007
Epoch: [1]  [3430/3542]  eta: 0:01:22  lr: 0.000039  min_lr: 0.000039  loss: 3.0254 (3.1089)  loss_scale: 8192.0000 (9603.0965)  weight_decay: 0.0500 (0.0500)  time: 0.7642  data: 0.1786  max mem: 13007
Epoch: [1]  [3440/3542]  eta: 0:01:15  lr: 0.000039  min_lr: 0.000039  loss: 3.0957 (3.1088)  loss_scale: 8192.0000 (9598.9956)  weight_decay: 0.0500 (0.0500)  time: 0.7356  data: 0.1499  max mem: 13007
Epoch: [1]  [3450/3542]  eta: 0:01:07  lr: 0.000039  min_lr: 0.000039  loss: 3.0586 (3.1085)  loss_scale: 8192.0000 (9594.9186)  weight_decay: 0.0500 (0.0500)  time: 0.7367  data: 0.1511  max mem: 13007
[2023-05-16 07:09:06,318] [INFO] [logging.py:60:log_dist] [Rank 0] step=7000, skipped=8, lr=[3.88792306682922e-05, 3.88792306682922e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 07:09:06,321] [INFO] [timer.py:157:stop] 0/7000, SamplesPerSec=55.60509253790707
Epoch: [1]  [3460/3542]  eta: 0:01:00  lr: 0.000039  min_lr: 0.000039  loss: 3.0352 (3.1084)  loss_scale: 8192.0000 (9590.8651)  weight_decay: 0.0500 (0.0500)  time: 0.7427  data: 0.1578  max mem: 13007
Epoch: [1]  [3470/3542]  eta: 0:00:52  lr: 0.000039  min_lr: 0.000039  loss: 3.0996 (3.1083)  loss_scale: 8192.0000 (9586.8349)  weight_decay: 0.0500 (0.0500)  time: 0.7056  data: 0.1202  max mem: 13007
Epoch: [1]  [3480/3542]  eta: 0:00:45  lr: 0.000039  min_lr: 0.000039  loss: 3.0996 (3.1083)  loss_scale: 8192.0000 (9582.8279)  weight_decay: 0.0500 (0.0500)  time: 0.7153  data: 0.1296  max mem: 13007
Epoch: [1]  [3490/3542]  eta: 0:00:38  lr: 0.000039  min_lr: 0.000039  loss: 3.0391 (3.1081)  loss_scale: 8192.0000 (9578.8439)  weight_decay: 0.0500 (0.0500)  time: 0.7140  data: 0.1293  max mem: 13007
Epoch: [1]  [3500/3542]  eta: 0:00:30  lr: 0.000039  min_lr: 0.000039  loss: 3.0371 (3.1078)  loss_scale: 8192.0000 (9574.8826)  weight_decay: 0.0500 (0.0500)  time: 0.7455  data: 0.1601  max mem: 13007
[2023-05-16 07:09:39,848] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 07:09:39,848] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [1]  [3510/3542]  eta: 0:00:23  lr: 0.000039  min_lr: 0.000039  loss: 3.0625 (3.1081)  loss_scale: 8192.0000 (9591.9430)  weight_decay: 0.0500 (0.0500)  time: 0.7581  data: 0.1722  max mem: 13007
Epoch: [1]  [3520/3542]  eta: 0:00:16  lr: 0.000039  min_lr: 0.000039  loss: 3.0723 (3.1078)  loss_scale: 16384.0000 (9611.2332)  weight_decay: 0.0500 (0.0500)  time: 0.7726  data: 0.1872  max mem: 13007
Epoch: [1]  [3530/3542]  eta: 0:00:08  lr: 0.000039  min_lr: 0.000039  loss: 3.0234 (3.1078)  loss_scale: 16384.0000 (9630.4140)  weight_decay: 0.0500 (0.0500)  time: 0.7101  data: 0.1248  max mem: 13007
Epoch: [1]  [3540/3542]  eta: 0:00:01  lr: 0.000039  min_lr: 0.000039  loss: 3.0234 (3.1076)  loss_scale: 16384.0000 (9649.4866)  weight_decay: 0.0500 (0.0500)  time: 0.5851  data: 0.0003  max mem: 13007
Epoch: [1]  [3541/3542]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000039  loss: 3.0234 (3.1076)  loss_scale: 16384.0000 (9651.3879)  weight_decay: 0.0500 (0.0500)  time: 0.5848  data: 0.0003  max mem: 13007
Epoch: [1] Total time: 0:43:23 (0.7351 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000039  loss: 3.0234 (3.1076)  loss_scale: 16384.0000 (9651.3879)  weight_decay: 0.0500 (0.0500)
[2023-05-16 07:10:07,195] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-1/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:35:42    time: 13.7331  data: 9.9033  max mem: 13008
Test:  [ 10/156]  eta: 0:11:20    time: 4.6620  data: 0.9005  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 20/156]  eta: 0:09:35    time: 3.7534  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 30/156]  eta: 0:08:34    time: 3.7619  data: 0.0002  max mem: 13008
Test:  [ 40/156]  eta: 0:07:42    time: 3.7324  data: 0.0002  max mem: 13008
Test:  [ 50/156]  eta: 0:06:54    time: 3.6503  data: 0.0002  max mem: 13008
Test:  [ 60/156]  eta: 0:06:12    time: 3.6562  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 70/156]  eta: 0:05:31    time: 3.7059  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:51    time: 3.6795  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 90/156]  eta: 0:04:11    time: 3.6759  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:32    time: 3.6869  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:54    time: 3.7199  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:16    time: 3.7329  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:38    time: 3.7223  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:01:00    time: 3.7233  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:22    time: 3.6969  data: 0.0002  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.7224  data: 0.0002  max mem: 13008
Test: Total time: 0:09:49 (3.7773 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e1.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e1.json and result file is ./output_freeze/coco_captioning_result_val_e1.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e1.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1166469.35 tokens per second.
PTBTokenizer tokenized 60708 tokens at 422892.92 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 49129, 'reflen': 48286, 'guess': [49129, 44137, 39145, 34154], 'correct': [31660, 15797, 6799, 2576]}
ratio: 1.0174584765770407
Bleu_1: 0.644
Bleu_2: 0.480
Bleu_3: 0.342
Bleu_4: 0.234
computing METEOR score...
METEOR: 0.227
computing Rouge score...
ROUGE_L: 0.494
computing CIDEr score...
CIDEr: 0.761
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [23.26 seconds]
SPICE evaluation took: 35.12 s
SPICE: 0.165
Performance of the network on the 5000 val images: 0.8%
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 07:20:57,538] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-best/mp_rank_00_model_states.pt
Max performance: 0.76%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [2]  [   0/3542]  eta: 18:02:44  lr: 0.000039  min_lr: 0.000039  loss: 3.0781 (3.0781)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 18.3412  data: 17.7018  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [2]  [  10/3542]  eta: 3:00:19  lr: 0.000039  min_lr: 0.000039  loss: 3.0645 (3.0884)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 3.0632  data: 2.4735  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [2]  [  20/3542]  eta: 2:13:54  lr: 0.000039  min_lr: 0.000039  loss: 3.0059 (2.9965)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4783  data: 0.8955  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [2]  [  30/3542]  eta: 1:56:43  lr: 0.000039  min_lr: 0.000039  loss: 2.9941 (3.0341)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4064  data: 0.8246  max mem: 13008
Epoch: [2]  [  40/3542]  eta: 1:43:56  lr: 0.000039  min_lr: 0.000039  loss: 3.0957 (3.0505)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2552  data: 0.6725  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [2]  [  50/3542]  eta: 1:38:20  lr: 0.000039  min_lr: 0.000039  loss: 3.0684 (3.0483)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2179  data: 0.6337  max mem: 13008
Epoch: [2]  [  60/3542]  eta: 1:32:52  lr: 0.000039  min_lr: 0.000039  loss: 2.9492 (3.0285)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2304  data: 0.6464  max mem: 13008
Epoch: [2]  [  70/3542]  eta: 1:25:22  lr: 0.000039  min_lr: 0.000039  loss: 2.9922 (3.0289)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9286  data: 0.3459  max mem: 13008
Epoch: [2]  [  80/3542]  eta: 1:21:13  lr: 0.000039  min_lr: 0.000039  loss: 3.0195 (3.0241)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8197  data: 0.2370  max mem: 13008
Epoch: [2]  [  90/3542]  eta: 1:18:41  lr: 0.000039  min_lr: 0.000039  loss: 2.9570 (3.0199)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9859  data: 0.4030  max mem: 13008
Epoch: [2]  [ 100/3542]  eta: 1:15:54  lr: 0.000039  min_lr: 0.000039  loss: 3.0176 (3.0301)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9811  data: 0.3952  max mem: 13008
Epoch: [2]  [ 110/3542]  eta: 1:13:31  lr: 0.000039  min_lr: 0.000039  loss: 3.0176 (3.0214)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9110  data: 0.3240  max mem: 13008
Epoch: [2]  [ 120/3542]  eta: 1:10:39  lr: 0.000039  min_lr: 0.000039  loss: 2.9336 (3.0156)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8131  data: 0.2282  max mem: 13008
Epoch: [2]  [ 130/3542]  eta: 1:08:27  lr: 0.000039  min_lr: 0.000039  loss: 3.0332 (3.0221)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7508  data: 0.1665  max mem: 13008
Epoch: [2]  [ 140/3542]  eta: 1:06:37  lr: 0.000039  min_lr: 0.000039  loss: 3.0332 (3.0206)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7897  data: 0.2058  max mem: 13008
Epoch: [2]  [ 150/3542]  eta: 1:04:38  lr: 0.000039  min_lr: 0.000039  loss: 2.9199 (3.0152)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7472  data: 0.1636  max mem: 13008
Epoch: [2]  [ 160/3542]  eta: 1:02:48  lr: 0.000039  min_lr: 0.000039  loss: 2.9277 (3.0135)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6851  data: 0.1009  max mem: 13008
Epoch: [2]  [ 170/3542]  eta: 1:01:16  lr: 0.000039  min_lr: 0.000039  loss: 2.9883 (3.0121)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6908  data: 0.1057  max mem: 13008
Epoch: [2]  [ 180/3542]  eta: 0:59:50  lr: 0.000039  min_lr: 0.000039  loss: 2.9902 (3.0105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6951  data: 0.1102  max mem: 13008
Epoch: [2]  [ 190/3542]  eta: 0:58:48  lr: 0.000039  min_lr: 0.000039  loss: 2.9824 (3.0098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7285  data: 0.1430  max mem: 13008
Epoch: [2]  [ 200/3542]  eta: 0:57:31  lr: 0.000039  min_lr: 0.000039  loss: 2.9727 (3.0083)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7132  data: 0.1276  max mem: 13008
Epoch: [2]  [ 210/3542]  eta: 0:56:35  lr: 0.000039  min_lr: 0.000039  loss: 2.9375 (3.0095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6992  data: 0.1139  max mem: 13008
Epoch: [2]  [ 220/3542]  eta: 0:55:32  lr: 0.000039  min_lr: 0.000039  loss: 2.9414 (3.0066)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7083  data: 0.1227  max mem: 13008
Epoch: [2]  [ 230/3542]  eta: 0:54:37  lr: 0.000039  min_lr: 0.000039  loss: 2.9004 (3.0025)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6790  data: 0.0939  max mem: 13008
Epoch: [2]  [ 240/3542]  eta: 0:53:48  lr: 0.000039  min_lr: 0.000039  loss: 2.9980 (3.0045)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.1101  max mem: 13008
Epoch: [2]  [ 250/3542]  eta: 0:53:01  lr: 0.000039  min_lr: 0.000039  loss: 3.0273 (3.0063)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7002  data: 0.1137  max mem: 13008
Epoch: [2]  [ 260/3542]  eta: 0:52:14  lr: 0.000039  min_lr: 0.000039  loss: 3.0273 (3.0066)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.0960  max mem: 13008
Epoch: [2]  [ 270/3542]  eta: 0:51:30  lr: 0.000039  min_lr: 0.000039  loss: 2.9102 (3.0041)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.0831  max mem: 13008
Epoch: [2]  [ 280/3542]  eta: 0:50:50  lr: 0.000039  min_lr: 0.000039  loss: 2.9922 (3.0052)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.0884  max mem: 13008
Epoch: [2]  [ 290/3542]  eta: 0:50:17  lr: 0.000039  min_lr: 0.000039  loss: 3.0000 (3.0043)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.1185  max mem: 13008
Epoch: [2]  [ 300/3542]  eta: 0:49:47  lr: 0.000039  min_lr: 0.000039  loss: 3.0430 (3.0054)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7304  data: 0.1447  max mem: 13008
Epoch: [2]  [ 310/3542]  eta: 0:49:08  lr: 0.000039  min_lr: 0.000039  loss: 3.0508 (3.0079)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.0968  max mem: 13008
Epoch: [2]  [ 320/3542]  eta: 0:48:34  lr: 0.000039  min_lr: 0.000039  loss: 2.9941 (3.0081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.0658  max mem: 13008
Epoch: [2]  [ 330/3542]  eta: 0:48:02  lr: 0.000039  min_lr: 0.000039  loss: 2.9941 (3.0085)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6675  data: 0.0824  max mem: 13008
Epoch: [2]  [ 340/3542]  eta: 0:47:36  lr: 0.000039  min_lr: 0.000039  loss: 3.0625 (3.0075)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6886  data: 0.1025  max mem: 13008
Epoch: [2]  [ 350/3542]  eta: 0:47:07  lr: 0.000039  min_lr: 0.000039  loss: 2.9688 (3.0068)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6936  data: 0.1080  max mem: 13008
Epoch: [2]  [ 360/3542]  eta: 0:46:41  lr: 0.000039  min_lr: 0.000039  loss: 2.9688 (3.0069)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6811  data: 0.0958  max mem: 13008
Epoch: [2]  [ 370/3542]  eta: 0:46:13  lr: 0.000039  min_lr: 0.000039  loss: 3.0039 (3.0087)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6715  data: 0.0851  max mem: 13008
Epoch: [2]  [ 380/3542]  eta: 0:45:43  lr: 0.000039  min_lr: 0.000039  loss: 3.0039 (3.0090)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.0503  max mem: 13008
Epoch: [2]  [ 390/3542]  eta: 0:45:20  lr: 0.000039  min_lr: 0.000039  loss: 2.9902 (3.0089)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6554  data: 0.0687  max mem: 13008
Epoch: [2]  [ 400/3542]  eta: 0:44:59  lr: 0.000039  min_lr: 0.000039  loss: 2.9785 (3.0066)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7008  data: 0.1144  max mem: 13008
Epoch: [2]  [ 410/3542]  eta: 0:44:36  lr: 0.000039  min_lr: 0.000039  loss: 2.9648 (3.0058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6851  data: 0.0991  max mem: 13008
Epoch: [2]  [ 420/3542]  eta: 0:44:14  lr: 0.000039  min_lr: 0.000039  loss: 2.9766 (3.0054)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6700  data: 0.0841  max mem: 13008
Epoch: [2]  [ 430/3542]  eta: 0:43:56  lr: 0.000039  min_lr: 0.000039  loss: 2.9766 (3.0053)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6948  data: 0.1095  max mem: 13008
Epoch: [2]  [ 440/3542]  eta: 0:43:39  lr: 0.000039  min_lr: 0.000039  loss: 2.9961 (3.0052)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7243  data: 0.1399  max mem: 13008
Epoch: [2]  [ 450/3542]  eta: 0:43:18  lr: 0.000039  min_lr: 0.000039  loss: 2.9902 (3.0046)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.1079  max mem: 13008
Epoch: [2]  [ 460/3542]  eta: 0:42:57  lr: 0.000039  min_lr: 0.000039  loss: 2.9805 (3.0049)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6513  data: 0.0657  max mem: 13008
Epoch: [2]  [ 470/3542]  eta: 0:42:37  lr: 0.000038  min_lr: 0.000038  loss: 2.9355 (3.0032)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0714  max mem: 13008
Epoch: [2]  [ 480/3542]  eta: 0:42:19  lr: 0.000038  min_lr: 0.000038  loss: 2.8574 (3.0027)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.0887  max mem: 13008
Epoch: [2]  [ 490/3542]  eta: 0:42:04  lr: 0.000038  min_lr: 0.000038  loss: 3.0879 (3.0055)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7034  data: 0.1167  max mem: 13008
Epoch: [2]  [ 500/3542]  eta: 0:41:45  lr: 0.000038  min_lr: 0.000038  loss: 3.0137 (3.0041)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6813  data: 0.0945  max mem: 13008
Epoch: [2]  [ 510/3542]  eta: 0:41:28  lr: 0.000038  min_lr: 0.000038  loss: 3.0020 (3.0055)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.0750  max mem: 13008
Epoch: [2]  [ 520/3542]  eta: 0:41:12  lr: 0.000038  min_lr: 0.000038  loss: 3.0391 (3.0058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6863  data: 0.0993  max mem: 13008
Epoch: [2]  [ 530/3542]  eta: 0:40:55  lr: 0.000038  min_lr: 0.000038  loss: 3.0312 (3.0058)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6765  data: 0.0905  max mem: 13008
Epoch: [2]  [ 540/3542]  eta: 0:40:40  lr: 0.000038  min_lr: 0.000038  loss: 3.0352 (3.0074)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0884  max mem: 13008
Epoch: [2]  [ 550/3542]  eta: 0:40:23  lr: 0.000038  min_lr: 0.000038  loss: 3.0254 (3.0080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0781  max mem: 13008
Epoch: [2]  [ 560/3542]  eta: 0:40:07  lr: 0.000038  min_lr: 0.000038  loss: 3.0020 (3.0080)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.0660  max mem: 13008
Epoch: [2]  [ 570/3542]  eta: 0:39:52  lr: 0.000038  min_lr: 0.000038  loss: 3.0117 (3.0081)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6708  data: 0.0850  max mem: 13008
Epoch: [2]  [ 580/3542]  eta: 0:39:35  lr: 0.000038  min_lr: 0.000038  loss: 3.0684 (3.0095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.0670  max mem: 13008
Epoch: [2]  [ 590/3542]  eta: 0:39:23  lr: 0.000038  min_lr: 0.000038  loss: 3.0332 (3.0094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.0916  max mem: 13008
Epoch: [2]  [ 600/3542]  eta: 0:39:13  lr: 0.000038  min_lr: 0.000038  loss: 3.0137 (3.0095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7406  data: 0.1539  max mem: 13008
Epoch: [2]  [ 610/3542]  eta: 0:39:00  lr: 0.000038  min_lr: 0.000038  loss: 2.9922 (3.0088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7269  data: 0.1404  max mem: 13008
Epoch: [2]  [ 620/3542]  eta: 0:38:45  lr: 0.000038  min_lr: 0.000038  loss: 2.9922 (3.0089)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.0880  max mem: 13008
Epoch: [2]  [ 630/3542]  eta: 0:38:34  lr: 0.000038  min_lr: 0.000038  loss: 3.0742 (3.0101)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6935  data: 0.1083  max mem: 13008
Epoch: [2]  [ 640/3542]  eta: 0:38:22  lr: 0.000038  min_lr: 0.000038  loss: 2.9824 (3.0096)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7154  data: 0.1296  max mem: 13008
Epoch: [2]  [ 650/3542]  eta: 0:38:08  lr: 0.000038  min_lr: 0.000038  loss: 2.9707 (3.0105)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.0911  max mem: 13008
Epoch: [2]  [ 660/3542]  eta: 0:37:56  lr: 0.000038  min_lr: 0.000038  loss: 2.9707 (3.0094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.0893  max mem: 13008
Epoch: [2]  [ 670/3542]  eta: 0:37:44  lr: 0.000038  min_lr: 0.000038  loss: 2.9961 (3.0095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6918  data: 0.1056  max mem: 13008
Epoch: [2]  [ 680/3542]  eta: 0:37:29  lr: 0.000038  min_lr: 0.000038  loss: 3.0137 (3.0097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6665  data: 0.0801  max mem: 13008
Epoch: [2]  [ 690/3542]  eta: 0:37:17  lr: 0.000038  min_lr: 0.000038  loss: 2.9688 (3.0098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.0652  max mem: 13008
Epoch: [2]  [ 700/3542]  eta: 0:37:06  lr: 0.000038  min_lr: 0.000038  loss: 3.0293 (3.0107)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.1041  max mem: 13008
Epoch: [2]  [ 710/3542]  eta: 0:36:54  lr: 0.000038  min_lr: 0.000038  loss: 3.0293 (3.0101)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6965  data: 0.1097  max mem: 13008
Epoch: [2]  [ 720/3542]  eta: 0:36:43  lr: 0.000038  min_lr: 0.000038  loss: 2.9805 (3.0106)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6861  data: 0.0995  max mem: 13008
Epoch: [2]  [ 730/3542]  eta: 0:36:33  lr: 0.000038  min_lr: 0.000038  loss: 2.9805 (3.0109)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7163  data: 0.1299  max mem: 13008
Epoch: [2]  [ 740/3542]  eta: 0:36:23  lr: 0.000038  min_lr: 0.000038  loss: 2.9355 (3.0102)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7225  data: 0.1367  max mem: 13008
Epoch: [2]  [ 750/3542]  eta: 0:36:12  lr: 0.000038  min_lr: 0.000038  loss: 2.9961 (3.0106)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6999  data: 0.1139  max mem: 13008
Epoch: [2]  [ 760/3542]  eta: 0:36:02  lr: 0.000038  min_lr: 0.000038  loss: 2.9883 (3.0095)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7106  data: 0.1248  max mem: 13008
Epoch: [2]  [ 770/3542]  eta: 0:35:50  lr: 0.000038  min_lr: 0.000038  loss: 2.9609 (3.0094)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6984  data: 0.1127  max mem: 13008
Epoch: [2]  [ 780/3542]  eta: 0:35:41  lr: 0.000038  min_lr: 0.000038  loss: 2.9707 (3.0090)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6970  data: 0.1105  max mem: 13008
Epoch: [2]  [ 790/3542]  eta: 0:35:30  lr: 0.000038  min_lr: 0.000038  loss: 3.0000 (3.0093)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7080  data: 0.1212  max mem: 13008
Epoch: [2]  [ 800/3542]  eta: 0:35:19  lr: 0.000038  min_lr: 0.000038  loss: 3.0039 (3.0097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6813  data: 0.0942  max mem: 13008
Epoch: [2]  [ 810/3542]  eta: 0:35:07  lr: 0.000038  min_lr: 0.000038  loss: 2.9531 (3.0092)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6667  data: 0.0793  max mem: 13008
Epoch: [2]  [ 820/3542]  eta: 0:34:59  lr: 0.000038  min_lr: 0.000038  loss: 2.9414 (3.0093)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6993  data: 0.1121  max mem: 13008
Epoch: [2]  [ 830/3542]  eta: 0:34:47  lr: 0.000038  min_lr: 0.000038  loss: 2.9551 (3.0101)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7013  data: 0.1148  max mem: 13008
Epoch: [2]  [ 840/3542]  eta: 0:34:37  lr: 0.000038  min_lr: 0.000038  loss: 2.9551 (3.0096)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6734  data: 0.0868  max mem: 13008
Epoch: [2]  [ 850/3542]  eta: 0:34:29  lr: 0.000038  min_lr: 0.000038  loss: 3.0527 (3.0098)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7152  data: 0.1279  max mem: 13008
Epoch: [2]  [ 860/3542]  eta: 0:34:18  lr: 0.000038  min_lr: 0.000038  loss: 3.0293 (3.0102)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7176  data: 0.1307  max mem: 13008
Epoch: [2]  [ 870/3542]  eta: 0:34:07  lr: 0.000038  min_lr: 0.000038  loss: 3.0176 (3.0104)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0887  max mem: 13008
Epoch: [2]  [ 880/3542]  eta: 0:33:58  lr: 0.000038  min_lr: 0.000038  loss: 3.0273 (3.0104)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.0949  max mem: 13008
Epoch: [2]  [ 890/3542]  eta: 0:33:50  lr: 0.000038  min_lr: 0.000038  loss: 2.9590 (3.0103)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7227  data: 0.1357  max mem: 13008
Epoch: [2]  [ 900/3542]  eta: 0:33:37  lr: 0.000038  min_lr: 0.000038  loss: 2.9590 (3.0102)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6774  data: 0.0911  max mem: 13008
Epoch: [2]  [ 910/3542]  eta: 0:33:28  lr: 0.000038  min_lr: 0.000038  loss: 2.9941 (3.0101)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.0716  max mem: 13008
[2023-05-16 07:32:39,095] [INFO] [logging.py:60:log_dist] [Rank 0] step=8000, skipped=8, lr=[3.814896847006727e-05, 3.814896847006727e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 07:32:39,097] [INFO] [timer.py:157:stop] 0/8000, SamplesPerSec=55.59871099015332
Epoch: [2]  [ 920/3542]  eta: 0:33:20  lr: 0.000038  min_lr: 0.000038  loss: 2.9180 (3.0092)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7361  data: 0.1497  max mem: 13008
Epoch: [2]  [ 930/3542]  eta: 0:33:11  lr: 0.000038  min_lr: 0.000038  loss: 2.9180 (3.0088)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7224  data: 0.1370  max mem: 13008
Epoch: [2]  [ 940/3542]  eta: 0:33:03  lr: 0.000038  min_lr: 0.000038  loss: 2.9609 (3.0097)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7172  data: 0.1311  max mem: 13008
Epoch: [2]  [ 950/3542]  eta: 0:32:53  lr: 0.000038  min_lr: 0.000038  loss: 3.0742 (3.0096)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7168  data: 0.1300  max mem: 13008
[2023-05-16 07:33:10,087] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 07:33:10,087] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [2]  [ 960/3542]  eta: 0:32:43  lr: 0.000038  min_lr: 0.000038  loss: 2.9336 (3.0096)  loss_scale: 16384.0000 (16401.0489)  weight_decay: 0.0500 (0.0500)  time: 0.6742  data: 0.0881  max mem: 13008
[2023-05-16 07:33:17,066] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 8053
[2023-05-16 07:33:17,066] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-05-16 07:33:17,067] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [ 970/3542]  eta: 0:32:35  lr: 0.000038  min_lr: 0.000038  loss: 2.9512 (3.0096)  loss_scale: 16384.0000 (16535.8599)  weight_decay: 0.0500 (0.0500)  time: 0.7127  data: 0.1269  max mem: 13008
Epoch: [2]  [ 980/3542]  eta: 0:32:25  lr: 0.000038  min_lr: 0.000038  loss: 2.9980 (3.0095)  loss_scale: 16384.0000 (16534.3119)  weight_decay: 0.0500 (0.0500)  time: 0.7156  data: 0.1297  max mem: 13008
Epoch: [2]  [ 990/3542]  eta: 0:32:15  lr: 0.000038  min_lr: 0.000038  loss: 2.9160 (3.0091)  loss_scale: 16384.0000 (16532.7952)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0771  max mem: 13008
Epoch: [2]  [1000/3542]  eta: 0:32:06  lr: 0.000038  min_lr: 0.000038  loss: 2.9961 (3.0093)  loss_scale: 16384.0000 (16531.3087)  weight_decay: 0.0500 (0.0500)  time: 0.6882  data: 0.1014  max mem: 13008
Epoch: [2]  [1010/3542]  eta: 0:31:57  lr: 0.000038  min_lr: 0.000038  loss: 2.9961 (3.0087)  loss_scale: 16384.0000 (16529.8516)  weight_decay: 0.0500 (0.0500)  time: 0.7043  data: 0.1169  max mem: 13008
Epoch: [2]  [1020/3542]  eta: 0:31:48  lr: 0.000038  min_lr: 0.000038  loss: 2.9434 (3.0085)  loss_scale: 16384.0000 (16528.4231)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.0963  max mem: 13008
Epoch: [2]  [1030/3542]  eta: 0:31:39  lr: 0.000038  min_lr: 0.000038  loss: 3.0156 (3.0087)  loss_scale: 16384.0000 (16527.0223)  weight_decay: 0.0500 (0.0500)  time: 0.6938  data: 0.1065  max mem: 13008
Epoch: [2]  [1040/3542]  eta: 0:31:30  lr: 0.000038  min_lr: 0.000038  loss: 2.9570 (3.0078)  loss_scale: 16384.0000 (16525.6484)  weight_decay: 0.0500 (0.0500)  time: 0.7109  data: 0.1235  max mem: 13008
Epoch: [2]  [1050/3542]  eta: 0:31:22  lr: 0.000038  min_lr: 0.000038  loss: 2.9180 (3.0073)  loss_scale: 16384.0000 (16524.3007)  weight_decay: 0.0500 (0.0500)  time: 0.7130  data: 0.1258  max mem: 13008
Epoch: [2]  [1060/3542]  eta: 0:31:13  lr: 0.000038  min_lr: 0.000038  loss: 3.0430 (3.0079)  loss_scale: 16384.0000 (16522.9783)  weight_decay: 0.0500 (0.0500)  time: 0.7219  data: 0.1352  max mem: 13008
Epoch: [2]  [1070/3542]  eta: 0:31:05  lr: 0.000038  min_lr: 0.000038  loss: 3.0566 (3.0084)  loss_scale: 16384.0000 (16521.6807)  weight_decay: 0.0500 (0.0500)  time: 0.7242  data: 0.1374  max mem: 13008
Epoch: [2]  [1080/3542]  eta: 0:30:55  lr: 0.000038  min_lr: 0.000038  loss: 3.0176 (3.0080)  loss_scale: 16384.0000 (16520.4070)  weight_decay: 0.0500 (0.0500)  time: 0.6823  data: 0.0951  max mem: 13008
Epoch: [2]  [1090/3542]  eta: 0:30:46  lr: 0.000038  min_lr: 0.000038  loss: 2.9922 (3.0079)  loss_scale: 16384.0000 (16519.1567)  weight_decay: 0.0500 (0.0500)  time: 0.6786  data: 0.0914  max mem: 13008
Epoch: [2]  [1100/3542]  eta: 0:30:38  lr: 0.000038  min_lr: 0.000038  loss: 2.9766 (3.0078)  loss_scale: 16384.0000 (16517.9292)  weight_decay: 0.0500 (0.0500)  time: 0.7055  data: 0.1197  max mem: 13008
Epoch: [2]  [1110/3542]  eta: 0:30:28  lr: 0.000038  min_lr: 0.000038  loss: 2.9531 (3.0074)  loss_scale: 16384.0000 (16516.7237)  weight_decay: 0.0500 (0.0500)  time: 0.6852  data: 0.0995  max mem: 13008
Epoch: [2]  [1120/3542]  eta: 0:30:20  lr: 0.000038  min_lr: 0.000038  loss: 3.0039 (3.0076)  loss_scale: 16384.0000 (16515.5397)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.1019  max mem: 13008
Epoch: [2]  [1130/3542]  eta: 0:30:12  lr: 0.000038  min_lr: 0.000038  loss: 3.0254 (3.0079)  loss_scale: 16384.0000 (16514.3767)  weight_decay: 0.0500 (0.0500)  time: 0.7175  data: 0.1303  max mem: 13008
Epoch: [2]  [1140/3542]  eta: 0:30:02  lr: 0.000038  min_lr: 0.000038  loss: 3.0039 (3.0076)  loss_scale: 16384.0000 (16513.2340)  weight_decay: 0.0500 (0.0500)  time: 0.6835  data: 0.0977  max mem: 13008
Epoch: [2]  [1150/3542]  eta: 0:29:55  lr: 0.000038  min_lr: 0.000038  loss: 2.9609 (3.0078)  loss_scale: 16384.0000 (16512.1112)  weight_decay: 0.0500 (0.0500)  time: 0.7030  data: 0.1177  max mem: 13008
Epoch: [2]  [1160/3542]  eta: 0:29:46  lr: 0.000038  min_lr: 0.000038  loss: 2.9961 (3.0084)  loss_scale: 16384.0000 (16511.0078)  weight_decay: 0.0500 (0.0500)  time: 0.7219  data: 0.1365  max mem: 13008
Epoch: [2]  [1170/3542]  eta: 0:29:39  lr: 0.000038  min_lr: 0.000038  loss: 2.9941 (3.0081)  loss_scale: 16384.0000 (16509.9231)  weight_decay: 0.0500 (0.0500)  time: 0.7293  data: 0.1422  max mem: 13008
Epoch: [2]  [1180/3542]  eta: 0:29:31  lr: 0.000038  min_lr: 0.000038  loss: 2.9102 (3.0075)  loss_scale: 16384.0000 (16508.8569)  weight_decay: 0.0500 (0.0500)  time: 0.7440  data: 0.1563  max mem: 13008
Epoch: [2]  [1190/3542]  eta: 0:29:22  lr: 0.000038  min_lr: 0.000038  loss: 2.8789 (3.0066)  loss_scale: 16384.0000 (16507.8086)  weight_decay: 0.0500 (0.0500)  time: 0.6968  data: 0.1100  max mem: 13008
Epoch: [2]  [1200/3542]  eta: 0:29:15  lr: 0.000038  min_lr: 0.000038  loss: 2.9746 (3.0072)  loss_scale: 16384.0000 (16506.7777)  weight_decay: 0.0500 (0.0500)  time: 0.7181  data: 0.1307  max mem: 13008
Epoch: [2]  [1210/3542]  eta: 0:29:05  lr: 0.000038  min_lr: 0.000038  loss: 2.9746 (3.0061)  loss_scale: 16384.0000 (16505.7638)  weight_decay: 0.0500 (0.0500)  time: 0.7102  data: 0.1232  max mem: 13008
Epoch: [2]  [1220/3542]  eta: 0:28:58  lr: 0.000038  min_lr: 0.000038  loss: 2.8965 (3.0055)  loss_scale: 16384.0000 (16504.7666)  weight_decay: 0.0500 (0.0500)  time: 0.6972  data: 0.1111  max mem: 13008
Epoch: [2]  [1230/3542]  eta: 0:28:48  lr: 0.000038  min_lr: 0.000038  loss: 2.9648 (3.0047)  loss_scale: 16384.0000 (16503.7855)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.0999  max mem: 13008
Epoch: [2]  [1240/3542]  eta: 0:28:40  lr: 0.000038  min_lr: 0.000038  loss: 2.9805 (3.0050)  loss_scale: 16384.0000 (16502.8203)  weight_decay: 0.0500 (0.0500)  time: 0.6722  data: 0.0863  max mem: 13008
Epoch: [2]  [1250/3542]  eta: 0:28:32  lr: 0.000038  min_lr: 0.000038  loss: 2.9941 (3.0049)  loss_scale: 16384.0000 (16501.8705)  weight_decay: 0.0500 (0.0500)  time: 0.7110  data: 0.1249  max mem: 13008
Epoch: [2]  [1260/3542]  eta: 0:28:24  lr: 0.000038  min_lr: 0.000038  loss: 2.9941 (3.0053)  loss_scale: 16384.0000 (16500.9358)  weight_decay: 0.0500 (0.0500)  time: 0.7164  data: 0.1299  max mem: 13008
Epoch: [2]  [1270/3542]  eta: 0:28:15  lr: 0.000038  min_lr: 0.000038  loss: 2.9688 (3.0050)  loss_scale: 16384.0000 (16500.0157)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.1141  max mem: 13008
Epoch: [2]  [1280/3542]  eta: 0:28:07  lr: 0.000038  min_lr: 0.000038  loss: 2.9688 (3.0049)  loss_scale: 16384.0000 (16499.1101)  weight_decay: 0.0500 (0.0500)  time: 0.7063  data: 0.1198  max mem: 13008
Epoch: [2]  [1290/3542]  eta: 0:27:59  lr: 0.000038  min_lr: 0.000038  loss: 3.0137 (3.0049)  loss_scale: 16384.0000 (16498.2184)  weight_decay: 0.0500 (0.0500)  time: 0.7022  data: 0.1162  max mem: 13008
Epoch: [2]  [1300/3542]  eta: 0:27:51  lr: 0.000038  min_lr: 0.000038  loss: 2.9395 (3.0044)  loss_scale: 16384.0000 (16497.3405)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.1100  max mem: 13008
Epoch: [2]  [1310/3542]  eta: 0:27:43  lr: 0.000038  min_lr: 0.000038  loss: 2.9746 (3.0046)  loss_scale: 16384.0000 (16496.4760)  weight_decay: 0.0500 (0.0500)  time: 0.7109  data: 0.1249  max mem: 13008
Epoch: [2]  [1320/3542]  eta: 0:27:33  lr: 0.000038  min_lr: 0.000038  loss: 3.0566 (3.0044)  loss_scale: 16384.0000 (16495.6245)  weight_decay: 0.0500 (0.0500)  time: 0.6629  data: 0.0772  max mem: 13008
Epoch: [2]  [1330/3542]  eta: 0:27:25  lr: 0.000038  min_lr: 0.000038  loss: 2.9922 (3.0043)  loss_scale: 16384.0000 (16494.7859)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.0806  max mem: 13008
Epoch: [2]  [1340/3542]  eta: 0:27:17  lr: 0.000038  min_lr: 0.000038  loss: 3.0293 (3.0046)  loss_scale: 16384.0000 (16493.9597)  weight_decay: 0.0500 (0.0500)  time: 0.7215  data: 0.1331  max mem: 13008
Epoch: [2]  [1350/3542]  eta: 0:27:09  lr: 0.000038  min_lr: 0.000038  loss: 3.0293 (3.0045)  loss_scale: 16384.0000 (16493.1458)  weight_decay: 0.0500 (0.0500)  time: 0.7172  data: 0.1298  max mem: 13008
Epoch: [2]  [1360/3542]  eta: 0:27:01  lr: 0.000038  min_lr: 0.000038  loss: 2.9727 (3.0044)  loss_scale: 16384.0000 (16492.3439)  weight_decay: 0.0500 (0.0500)  time: 0.7123  data: 0.1257  max mem: 13008
Epoch: [2]  [1370/3542]  eta: 0:26:54  lr: 0.000038  min_lr: 0.000038  loss: 2.9219 (3.0040)  loss_scale: 16384.0000 (16491.5536)  weight_decay: 0.0500 (0.0500)  time: 0.7194  data: 0.1335  max mem: 13008
Epoch: [2]  [1380/3542]  eta: 0:26:46  lr: 0.000038  min_lr: 0.000038  loss: 2.8984 (3.0029)  loss_scale: 16384.0000 (16490.7748)  weight_decay: 0.0500 (0.0500)  time: 0.7114  data: 0.1257  max mem: 13008
Epoch: [2]  [1390/3542]  eta: 0:26:37  lr: 0.000038  min_lr: 0.000038  loss: 2.9316 (3.0029)  loss_scale: 16384.0000 (16490.0072)  weight_decay: 0.0500 (0.0500)  time: 0.6850  data: 0.0990  max mem: 13008
Epoch: [2]  [1400/3542]  eta: 0:26:29  lr: 0.000038  min_lr: 0.000038  loss: 3.0371 (3.0032)  loss_scale: 16384.0000 (16489.2505)  weight_decay: 0.0500 (0.0500)  time: 0.6798  data: 0.0926  max mem: 13008
Epoch: [2]  [1410/3542]  eta: 0:26:20  lr: 0.000038  min_lr: 0.000038  loss: 2.9902 (3.0030)  loss_scale: 16384.0000 (16488.5046)  weight_decay: 0.0500 (0.0500)  time: 0.6674  data: 0.0794  max mem: 13008
Epoch: [2]  [1420/3542]  eta: 0:26:11  lr: 0.000038  min_lr: 0.000038  loss: 2.9609 (3.0032)  loss_scale: 16384.0000 (16487.7692)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.0678  max mem: 13008
Epoch: [2]  [1430/3542]  eta: 0:26:04  lr: 0.000038  min_lr: 0.000038  loss: 2.9609 (3.0027)  loss_scale: 16384.0000 (16487.0440)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.1069  max mem: 13008
Epoch: [2]  [1440/3542]  eta: 0:25:56  lr: 0.000038  min_lr: 0.000038  loss: 2.9434 (3.0026)  loss_scale: 16384.0000 (16486.3289)  weight_decay: 0.0500 (0.0500)  time: 0.7354  data: 0.1502  max mem: 13008
Epoch: [2]  [1450/3542]  eta: 0:25:49  lr: 0.000038  min_lr: 0.000038  loss: 2.9648 (3.0029)  loss_scale: 16384.0000 (16485.6237)  weight_decay: 0.0500 (0.0500)  time: 0.7362  data: 0.1516  max mem: 13008
Epoch: [2]  [1460/3542]  eta: 0:25:41  lr: 0.000038  min_lr: 0.000038  loss: 3.0078 (3.0023)  loss_scale: 16384.0000 (16484.9281)  weight_decay: 0.0500 (0.0500)  time: 0.7137  data: 0.1282  max mem: 13008
Epoch: [2]  [1470/3542]  eta: 0:25:33  lr: 0.000038  min_lr: 0.000038  loss: 2.9004 (3.0018)  loss_scale: 16384.0000 (16484.2420)  weight_decay: 0.0500 (0.0500)  time: 0.7047  data: 0.1187  max mem: 13008
Epoch: [2]  [1480/3542]  eta: 0:25:25  lr: 0.000038  min_lr: 0.000038  loss: 2.9258 (3.0012)  loss_scale: 16384.0000 (16483.5652)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.1030  max mem: 13008
Epoch: [2]  [1490/3542]  eta: 0:25:17  lr: 0.000038  min_lr: 0.000038  loss: 2.9746 (3.0015)  loss_scale: 16384.0000 (16482.8974)  weight_decay: 0.0500 (0.0500)  time: 0.7119  data: 0.1258  max mem: 13008
Epoch: [2]  [1500/3542]  eta: 0:25:10  lr: 0.000038  min_lr: 0.000038  loss: 3.0410 (3.0017)  loss_scale: 16384.0000 (16482.2385)  weight_decay: 0.0500 (0.0500)  time: 0.7342  data: 0.1479  max mem: 13008
Epoch: [2]  [1510/3542]  eta: 0:25:02  lr: 0.000038  min_lr: 0.000038  loss: 3.0469 (3.0015)  loss_scale: 16384.0000 (16481.5884)  weight_decay: 0.0500 (0.0500)  time: 0.7346  data: 0.1477  max mem: 13008
Epoch: [2]  [1520/3542]  eta: 0:24:55  lr: 0.000038  min_lr: 0.000038  loss: 3.0410 (3.0018)  loss_scale: 16384.0000 (16480.9467)  weight_decay: 0.0500 (0.0500)  time: 0.7407  data: 0.1541  max mem: 13008
Epoch: [2]  [1530/3542]  eta: 0:24:47  lr: 0.000038  min_lr: 0.000038  loss: 3.0000 (3.0015)  loss_scale: 16384.0000 (16480.3135)  weight_decay: 0.0500 (0.0500)  time: 0.7073  data: 0.1209  max mem: 13008
Epoch: [2]  [1540/3542]  eta: 0:24:39  lr: 0.000038  min_lr: 0.000038  loss: 2.9707 (3.0015)  loss_scale: 16384.0000 (16479.6885)  weight_decay: 0.0500 (0.0500)  time: 0.6900  data: 0.1033  max mem: 13008
Epoch: [2]  [1550/3542]  eta: 0:24:31  lr: 0.000038  min_lr: 0.000038  loss: 2.9629 (3.0013)  loss_scale: 16384.0000 (16479.0716)  weight_decay: 0.0500 (0.0500)  time: 0.6827  data: 0.0957  max mem: 13008
Epoch: [2]  [1560/3542]  eta: 0:24:23  lr: 0.000038  min_lr: 0.000038  loss: 2.9570 (3.0014)  loss_scale: 16384.0000 (16478.4625)  weight_decay: 0.0500 (0.0500)  time: 0.6859  data: 0.0993  max mem: 13008
Epoch: [2]  [1570/3542]  eta: 0:24:14  lr: 0.000038  min_lr: 0.000038  loss: 2.9863 (3.0014)  loss_scale: 16384.0000 (16477.8612)  weight_decay: 0.0500 (0.0500)  time: 0.6747  data: 0.0886  max mem: 13008
Epoch: [2]  [1580/3542]  eta: 0:24:06  lr: 0.000038  min_lr: 0.000038  loss: 2.9863 (3.0015)  loss_scale: 16384.0000 (16477.2676)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.0789  max mem: 13008
Epoch: [2]  [1590/3542]  eta: 0:23:58  lr: 0.000038  min_lr: 0.000038  loss: 2.9238 (3.0014)  loss_scale: 16384.0000 (16476.6813)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.0916  max mem: 13008
Epoch: [2]  [1600/3542]  eta: 0:23:51  lr: 0.000038  min_lr: 0.000038  loss: 2.9844 (3.0011)  loss_scale: 16384.0000 (16476.1024)  weight_decay: 0.0500 (0.0500)  time: 0.7049  data: 0.1190  max mem: 13008
Epoch: [2]  [1610/3542]  eta: 0:23:43  lr: 0.000038  min_lr: 0.000038  loss: 2.9355 (3.0009)  loss_scale: 16384.0000 (16475.5307)  weight_decay: 0.0500 (0.0500)  time: 0.7314  data: 0.1451  max mem: 13008
Epoch: [2]  [1620/3542]  eta: 0:23:36  lr: 0.000038  min_lr: 0.000038  loss: 2.9824 (3.0016)  loss_scale: 16384.0000 (16474.9661)  weight_decay: 0.0500 (0.0500)  time: 0.7302  data: 0.1440  max mem: 13008
Epoch: [2]  [1630/3542]  eta: 0:23:28  lr: 0.000038  min_lr: 0.000038  loss: 3.0059 (3.0016)  loss_scale: 16384.0000 (16474.4083)  weight_decay: 0.0500 (0.0500)  time: 0.7088  data: 0.1226  max mem: 13008
Epoch: [2]  [1640/3542]  eta: 0:23:19  lr: 0.000038  min_lr: 0.000038  loss: 2.9531 (3.0014)  loss_scale: 16384.0000 (16473.8574)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.0712  max mem: 13008
Epoch: [2]  [1650/3542]  eta: 0:23:11  lr: 0.000038  min_lr: 0.000038  loss: 2.9023 (3.0008)  loss_scale: 16384.0000 (16473.3131)  weight_decay: 0.0500 (0.0500)  time: 0.6576  data: 0.0718  max mem: 13008
Epoch: [2]  [1660/3542]  eta: 0:23:04  lr: 0.000037  min_lr: 0.000037  loss: 2.9316 (3.0009)  loss_scale: 16384.0000 (16472.7754)  weight_decay: 0.0500 (0.0500)  time: 0.7061  data: 0.1203  max mem: 13008
Epoch: [2]  [1670/3542]  eta: 0:22:56  lr: 0.000037  min_lr: 0.000037  loss: 2.9316 (3.0005)  loss_scale: 16384.0000 (16472.2442)  weight_decay: 0.0500 (0.0500)  time: 0.7098  data: 0.1231  max mem: 13008
Epoch: [2]  [1680/3542]  eta: 0:22:49  lr: 0.000037  min_lr: 0.000037  loss: 2.9219 (3.0008)  loss_scale: 16384.0000 (16471.7192)  weight_decay: 0.0500 (0.0500)  time: 0.7145  data: 0.1281  max mem: 13008
Epoch: [2]  [1690/3542]  eta: 0:22:41  lr: 0.000037  min_lr: 0.000037  loss: 3.0527 (3.0008)  loss_scale: 16384.0000 (16471.2005)  weight_decay: 0.0500 (0.0500)  time: 0.7202  data: 0.1349  max mem: 13008
Epoch: [2]  [1700/3542]  eta: 0:22:33  lr: 0.000037  min_lr: 0.000037  loss: 2.9727 (3.0012)  loss_scale: 16384.0000 (16470.6878)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.1228  max mem: 13008
Epoch: [2]  [1710/3542]  eta: 0:22:26  lr: 0.000037  min_lr: 0.000037  loss: 2.9961 (3.0011)  loss_scale: 16384.0000 (16470.1812)  weight_decay: 0.0500 (0.0500)  time: 0.7318  data: 0.1467  max mem: 13008
Epoch: [2]  [1720/3542]  eta: 0:22:19  lr: 0.000037  min_lr: 0.000037  loss: 2.9961 (3.0010)  loss_scale: 16384.0000 (16469.6804)  weight_decay: 0.0500 (0.0500)  time: 0.7390  data: 0.1535  max mem: 13008
Epoch: [2]  [1730/3542]  eta: 0:22:11  lr: 0.000037  min_lr: 0.000037  loss: 2.9727 (3.0008)  loss_scale: 16384.0000 (16469.1854)  weight_decay: 0.0500 (0.0500)  time: 0.7062  data: 0.1207  max mem: 13008
Epoch: [2]  [1740/3542]  eta: 0:22:03  lr: 0.000037  min_lr: 0.000037  loss: 2.9355 (3.0005)  loss_scale: 16384.0000 (16468.6962)  weight_decay: 0.0500 (0.0500)  time: 0.6968  data: 0.1110  max mem: 13008
Epoch: [2]  [1750/3542]  eta: 0:21:55  lr: 0.000037  min_lr: 0.000037  loss: 2.9297 (3.0001)  loss_scale: 16384.0000 (16468.2125)  weight_decay: 0.0500 (0.0500)  time: 0.6953  data: 0.1097  max mem: 13008
Epoch: [2]  [1760/3542]  eta: 0:21:48  lr: 0.000037  min_lr: 0.000037  loss: 2.9707 (3.0002)  loss_scale: 16384.0000 (16467.7342)  weight_decay: 0.0500 (0.0500)  time: 0.7104  data: 0.1253  max mem: 13008
Epoch: [2]  [1770/3542]  eta: 0:21:41  lr: 0.000037  min_lr: 0.000037  loss: 3.0449 (3.0005)  loss_scale: 16384.0000 (16467.2614)  weight_decay: 0.0500 (0.0500)  time: 0.7701  data: 0.1841  max mem: 13008
Epoch: [2]  [1780/3542]  eta: 0:21:33  lr: 0.000037  min_lr: 0.000037  loss: 2.9746 (3.0001)  loss_scale: 16384.0000 (16466.7939)  weight_decay: 0.0500 (0.0500)  time: 0.7456  data: 0.1600  max mem: 13008
Epoch: [2]  [1790/3542]  eta: 0:21:26  lr: 0.000037  min_lr: 0.000037  loss: 2.9746 (3.0005)  loss_scale: 16384.0000 (16466.3317)  weight_decay: 0.0500 (0.0500)  time: 0.6848  data: 0.0995  max mem: 13008
Epoch: [2]  [1800/3542]  eta: 0:21:18  lr: 0.000037  min_lr: 0.000037  loss: 2.9902 (3.0008)  loss_scale: 16384.0000 (16465.8745)  weight_decay: 0.0500 (0.0500)  time: 0.6838  data: 0.0989  max mem: 13008
Epoch: [2]  [1810/3542]  eta: 0:21:10  lr: 0.000037  min_lr: 0.000037  loss: 3.0391 (3.0014)  loss_scale: 16384.0000 (16465.4224)  weight_decay: 0.0500 (0.0500)  time: 0.7003  data: 0.1152  max mem: 13008
Epoch: [2]  [1820/3542]  eta: 0:21:03  lr: 0.000037  min_lr: 0.000037  loss: 3.0078 (3.0011)  loss_scale: 16384.0000 (16464.9753)  weight_decay: 0.0500 (0.0500)  time: 0.7114  data: 0.1254  max mem: 13008
Epoch: [2]  [1830/3542]  eta: 0:20:55  lr: 0.000037  min_lr: 0.000037  loss: 2.9805 (3.0012)  loss_scale: 16384.0000 (16464.5330)  weight_decay: 0.0500 (0.0500)  time: 0.6999  data: 0.1137  max mem: 13008
Epoch: [2]  [1840/3542]  eta: 0:20:47  lr: 0.000037  min_lr: 0.000037  loss: 3.0332 (3.0014)  loss_scale: 16384.0000 (16464.0956)  weight_decay: 0.0500 (0.0500)  time: 0.6931  data: 0.1059  max mem: 13008
Epoch: [2]  [1850/3542]  eta: 0:20:40  lr: 0.000037  min_lr: 0.000037  loss: 2.9844 (3.0014)  loss_scale: 16384.0000 (16463.6629)  weight_decay: 0.0500 (0.0500)  time: 0.7083  data: 0.1208  max mem: 13008
Epoch: [2]  [1860/3542]  eta: 0:20:32  lr: 0.000037  min_lr: 0.000037  loss: 2.9668 (3.0014)  loss_scale: 16384.0000 (16463.2348)  weight_decay: 0.0500 (0.0500)  time: 0.7253  data: 0.1388  max mem: 13008
Epoch: [2]  [1870/3542]  eta: 0:20:25  lr: 0.000037  min_lr: 0.000037  loss: 3.0000 (3.0016)  loss_scale: 16384.0000 (16462.8113)  weight_decay: 0.0500 (0.0500)  time: 0.7299  data: 0.1441  max mem: 13008
Epoch: [2]  [1880/3542]  eta: 0:20:17  lr: 0.000037  min_lr: 0.000037  loss: 3.0449 (3.0019)  loss_scale: 16384.0000 (16462.3923)  weight_decay: 0.0500 (0.0500)  time: 0.7049  data: 0.1188  max mem: 13008
Epoch: [2]  [1890/3542]  eta: 0:20:10  lr: 0.000037  min_lr: 0.000037  loss: 3.0312 (3.0018)  loss_scale: 16384.0000 (16461.9778)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.0983  max mem: 13008
Epoch: [2]  [1900/3542]  eta: 0:20:02  lr: 0.000037  min_lr: 0.000037  loss: 2.9609 (3.0018)  loss_scale: 16384.0000 (16461.5676)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.0896  max mem: 13008
Epoch: [2]  [1910/3542]  eta: 0:19:54  lr: 0.000037  min_lr: 0.000037  loss: 2.9570 (3.0014)  loss_scale: 16384.0000 (16461.1617)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.0835  max mem: 13008
[2023-05-16 07:44:22,081] [INFO] [logging.py:60:log_dist] [Rank 0] step=9000, skipped=9, lr=[3.7247434647992054e-05, 3.7247434647992054e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 07:44:22,084] [INFO] [timer.py:157:stop] 0/9000, SamplesPerSec=55.58939274298101
Epoch: [2]  [1920/3542]  eta: 0:19:46  lr: 0.000037  min_lr: 0.000037  loss: 2.9922 (3.0017)  loss_scale: 16384.0000 (16460.7600)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.0975  max mem: 13008
Epoch: [2]  [1930/3542]  eta: 0:19:39  lr: 0.000037  min_lr: 0.000037  loss: 3.0039 (3.0017)  loss_scale: 16384.0000 (16460.3625)  weight_decay: 0.0500 (0.0500)  time: 0.6890  data: 0.1037  max mem: 13008
Epoch: [2]  [1940/3542]  eta: 0:19:31  lr: 0.000037  min_lr: 0.000037  loss: 2.9961 (3.0015)  loss_scale: 16384.0000 (16459.9691)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.1048  max mem: 13008
Epoch: [2]  [1950/3542]  eta: 0:19:23  lr: 0.000037  min_lr: 0.000037  loss: 2.9844 (3.0016)  loss_scale: 16384.0000 (16459.5797)  weight_decay: 0.0500 (0.0500)  time: 0.6778  data: 0.0918  max mem: 13008
Epoch: [2]  [1960/3542]  eta: 0:19:15  lr: 0.000037  min_lr: 0.000037  loss: 2.9824 (3.0017)  loss_scale: 16384.0000 (16459.1943)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.0776  max mem: 13008
[2023-05-16 07:44:58,985] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 07:44:58,985] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [2]  [1970/3542]  eta: 0:19:08  lr: 0.000037  min_lr: 0.000037  loss: 2.9355 (3.0014)  loss_scale: 16384.0000 (16467.1253)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.0887  max mem: 13008
[2023-05-16 07:44:59,594] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 9055
[2023-05-16 07:44:59,595] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-05-16 07:44:59,595] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [2]  [1980/3542]  eta: 0:19:00  lr: 0.000037  min_lr: 0.000037  loss: 2.9453 (3.0011)  loss_scale: 16384.0000 (16466.7057)  weight_decay: 0.0500 (0.0500)  time: 0.6800  data: 0.0951  max mem: 13008
Epoch: [2]  [1990/3542]  eta: 0:18:52  lr: 0.000037  min_lr: 0.000037  loss: 2.9961 (3.0014)  loss_scale: 16384.0000 (16466.2903)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.0857  max mem: 13008
Epoch: [2]  [2000/3542]  eta: 0:18:44  lr: 0.000037  min_lr: 0.000037  loss: 3.0293 (3.0016)  loss_scale: 16384.0000 (16465.8791)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.0689  max mem: 13008
Epoch: [2]  [2010/3542]  eta: 0:18:36  lr: 0.000037  min_lr: 0.000037  loss: 2.9355 (3.0010)  loss_scale: 16384.0000 (16465.4719)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.0569  max mem: 13008
Epoch: [2]  [2020/3542]  eta: 0:18:29  lr: 0.000037  min_lr: 0.000037  loss: 2.9512 (3.0012)  loss_scale: 16384.0000 (16465.0688)  weight_decay: 0.0500 (0.0500)  time: 0.6780  data: 0.0921  max mem: 13008
Epoch: [2]  [2030/3542]  eta: 0:18:21  lr: 0.000037  min_lr: 0.000037  loss: 2.9727 (3.0015)  loss_scale: 16384.0000 (16464.6696)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.1032  max mem: 13008
Epoch: [2]  [2040/3542]  eta: 0:18:13  lr: 0.000037  min_lr: 0.000037  loss: 2.9434 (3.0010)  loss_scale: 16384.0000 (16464.2744)  weight_decay: 0.0500 (0.0500)  time: 0.6711  data: 0.0849  max mem: 13008
Epoch: [2]  [2050/3542]  eta: 0:18:06  lr: 0.000037  min_lr: 0.000037  loss: 2.9434 (3.0009)  loss_scale: 16384.0000 (16463.8830)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.0848  max mem: 13008
Epoch: [2]  [2060/3542]  eta: 0:17:58  lr: 0.000037  min_lr: 0.000037  loss: 2.9570 (3.0007)  loss_scale: 16384.0000 (16463.4954)  weight_decay: 0.0500 (0.0500)  time: 0.6766  data: 0.0907  max mem: 13008
Epoch: [2]  [2070/3542]  eta: 0:17:50  lr: 0.000037  min_lr: 0.000037  loss: 2.9492 (3.0001)  loss_scale: 16384.0000 (16463.1115)  weight_decay: 0.0500 (0.0500)  time: 0.6701  data: 0.0844  max mem: 13008
Epoch: [2]  [2080/3542]  eta: 0:17:42  lr: 0.000037  min_lr: 0.000037  loss: 2.9492 (3.0000)  loss_scale: 16384.0000 (16462.7314)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.0694  max mem: 13008
Epoch: [2]  [2090/3542]  eta: 0:17:35  lr: 0.000037  min_lr: 0.000037  loss: 2.9805 (3.0001)  loss_scale: 16384.0000 (16462.3549)  weight_decay: 0.0500 (0.0500)  time: 0.6824  data: 0.0959  max mem: 13008
Epoch: [2]  [2100/3542]  eta: 0:17:27  lr: 0.000037  min_lr: 0.000037  loss: 2.9512 (2.9996)  loss_scale: 16384.0000 (16461.9819)  weight_decay: 0.0500 (0.0500)  time: 0.6832  data: 0.0969  max mem: 13008
Epoch: [2]  [2110/3542]  eta: 0:17:20  lr: 0.000037  min_lr: 0.000037  loss: 2.8887 (2.9998)  loss_scale: 16384.0000 (16461.6125)  weight_decay: 0.0500 (0.0500)  time: 0.6997  data: 0.1133  max mem: 13008
Epoch: [2]  [2120/3542]  eta: 0:17:13  lr: 0.000037  min_lr: 0.000037  loss: 3.0039 (2.9996)  loss_scale: 16384.0000 (16461.2466)  weight_decay: 0.0500 (0.0500)  time: 0.7449  data: 0.1586  max mem: 13008
Epoch: [2]  [2130/3542]  eta: 0:17:05  lr: 0.000037  min_lr: 0.000037  loss: 3.0176 (2.9998)  loss_scale: 16384.0000 (16460.8841)  weight_decay: 0.0500 (0.0500)  time: 0.7184  data: 0.1320  max mem: 13008
Epoch: [2]  [2140/3542]  eta: 0:16:58  lr: 0.000037  min_lr: 0.000037  loss: 3.0176 (2.9999)  loss_scale: 16384.0000 (16460.5250)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.1005  max mem: 13008
Epoch: [2]  [2150/3542]  eta: 0:16:51  lr: 0.000037  min_lr: 0.000037  loss: 2.9551 (3.0001)  loss_scale: 16384.0000 (16460.1692)  weight_decay: 0.0500 (0.0500)  time: 0.7172  data: 0.1314  max mem: 13008
Epoch: [2]  [2160/3542]  eta: 0:16:43  lr: 0.000037  min_lr: 0.000037  loss: 2.9531 (3.0003)  loss_scale: 16384.0000 (16459.8168)  weight_decay: 0.0500 (0.0500)  time: 0.7175  data: 0.1315  max mem: 13008
Epoch: [2]  [2170/3542]  eta: 0:16:36  lr: 0.000037  min_lr: 0.000037  loss: 2.9531 (3.0002)  loss_scale: 16384.0000 (16459.4675)  weight_decay: 0.0500 (0.0500)  time: 0.7174  data: 0.1303  max mem: 13008
Epoch: [2]  [2180/3542]  eta: 0:16:29  lr: 0.000037  min_lr: 0.000037  loss: 3.0020 (3.0001)  loss_scale: 16384.0000 (16459.1215)  weight_decay: 0.0500 (0.0500)  time: 0.7321  data: 0.1450  max mem: 13008
Epoch: [2]  [2190/3542]  eta: 0:16:22  lr: 0.000037  min_lr: 0.000037  loss: 2.9688 (3.0000)  loss_scale: 16384.0000 (16458.7786)  weight_decay: 0.0500 (0.0500)  time: 0.7426  data: 0.1561  max mem: 13008
Epoch: [2]  [2200/3542]  eta: 0:16:14  lr: 0.000037  min_lr: 0.000037  loss: 2.9492 (2.9998)  loss_scale: 16384.0000 (16458.4389)  weight_decay: 0.0500 (0.0500)  time: 0.7108  data: 0.1247  max mem: 13008
Epoch: [2]  [2210/3542]  eta: 0:16:07  lr: 0.000037  min_lr: 0.000037  loss: 2.9492 (2.9997)  loss_scale: 16384.0000 (16458.1022)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.0816  max mem: 13008
Epoch: [2]  [2220/3542]  eta: 0:15:59  lr: 0.000037  min_lr: 0.000037  loss: 2.9023 (2.9992)  loss_scale: 16384.0000 (16457.7686)  weight_decay: 0.0500 (0.0500)  time: 0.6933  data: 0.1064  max mem: 13008
Epoch: [2]  [2230/3542]  eta: 0:15:52  lr: 0.000037  min_lr: 0.000037  loss: 2.9219 (2.9991)  loss_scale: 16384.0000 (16457.4379)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.0966  max mem: 13008
Epoch: [2]  [2240/3542]  eta: 0:15:44  lr: 0.000037  min_lr: 0.000037  loss: 2.9980 (2.9991)  loss_scale: 16384.0000 (16457.1102)  weight_decay: 0.0500 (0.0500)  time: 0.6785  data: 0.0920  max mem: 13008
Epoch: [2]  [2250/3542]  eta: 0:15:37  lr: 0.000037  min_lr: 0.000037  loss: 2.9844 (2.9988)  loss_scale: 16384.0000 (16456.7854)  weight_decay: 0.0500 (0.0500)  time: 0.6980  data: 0.1112  max mem: 13008
Epoch: [2]  [2260/3542]  eta: 0:15:29  lr: 0.000037  min_lr: 0.000037  loss: 2.8984 (2.9987)  loss_scale: 16384.0000 (16456.4635)  weight_decay: 0.0500 (0.0500)  time: 0.6902  data: 0.1035  max mem: 13008
Epoch: [2]  [2270/3542]  eta: 0:15:22  lr: 0.000037  min_lr: 0.000037  loss: 2.8984 (2.9985)  loss_scale: 16384.0000 (16456.1444)  weight_decay: 0.0500 (0.0500)  time: 0.6807  data: 0.0951  max mem: 13008
Epoch: [2]  [2280/3542]  eta: 0:15:14  lr: 0.000037  min_lr: 0.000037  loss: 3.0137 (2.9988)  loss_scale: 16384.0000 (16455.8281)  weight_decay: 0.0500 (0.0500)  time: 0.6982  data: 0.1131  max mem: 13008
Epoch: [2]  [2290/3542]  eta: 0:15:07  lr: 0.000037  min_lr: 0.000037  loss: 3.0332 (2.9988)  loss_scale: 16384.0000 (16455.5146)  weight_decay: 0.0500 (0.0500)  time: 0.6977  data: 0.1130  max mem: 13008
[2023-05-16 07:48:46,389] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 9383
[2023-05-16 07:48:46,389] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 07:48:46,389] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [2]  [2300/3542]  eta: 0:14:59  lr: 0.000037  min_lr: 0.000037  loss: 2.9199 (2.9986)  loss_scale: 16384.0000 (16448.0834)  weight_decay: 0.0500 (0.0500)  time: 0.6877  data: 0.1029  max mem: 13008
Epoch: [2]  [2310/3542]  eta: 0:14:52  lr: 0.000037  min_lr: 0.000037  loss: 2.8926 (2.9982)  loss_scale: 8192.0000 (16412.3583)  weight_decay: 0.0500 (0.0500)  time: 0.6975  data: 0.1118  max mem: 13008
Epoch: [2]  [2320/3542]  eta: 0:14:45  lr: 0.000037  min_lr: 0.000037  loss: 3.0020 (2.9984)  loss_scale: 8192.0000 (16376.9410)  weight_decay: 0.0500 (0.0500)  time: 0.6882  data: 0.1012  max mem: 13008
Epoch: [2]  [2330/3542]  eta: 0:14:37  lr: 0.000037  min_lr: 0.000037  loss: 3.0156 (2.9981)  loss_scale: 8192.0000 (16341.8275)  weight_decay: 0.0500 (0.0500)  time: 0.6891  data: 0.1024  max mem: 13008
Epoch: [2]  [2340/3542]  eta: 0:14:30  lr: 0.000037  min_lr: 0.000037  loss: 3.0352 (2.9985)  loss_scale: 8192.0000 (16307.0141)  weight_decay: 0.0500 (0.0500)  time: 0.7074  data: 0.1208  max mem: 13008
Epoch: [2]  [2350/3542]  eta: 0:14:23  lr: 0.000037  min_lr: 0.000037  loss: 3.1387 (2.9988)  loss_scale: 8192.0000 (16272.4968)  weight_decay: 0.0500 (0.0500)  time: 0.7135  data: 0.1261  max mem: 13008
Epoch: [2]  [2360/3542]  eta: 0:14:15  lr: 0.000037  min_lr: 0.000037  loss: 2.9863 (2.9986)  loss_scale: 8192.0000 (16238.2719)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.1011  max mem: 13008
Epoch: [2]  [2370/3542]  eta: 0:14:08  lr: 0.000037  min_lr: 0.000037  loss: 2.8867 (2.9981)  loss_scale: 8192.0000 (16204.3357)  weight_decay: 0.0500 (0.0500)  time: 0.6966  data: 0.1106  max mem: 13008
Epoch: [2]  [2380/3542]  eta: 0:14:01  lr: 0.000037  min_lr: 0.000037  loss: 2.8359 (2.9981)  loss_scale: 8192.0000 (16170.6846)  weight_decay: 0.0500 (0.0500)  time: 0.7154  data: 0.1300  max mem: 13008
Epoch: [2]  [2390/3542]  eta: 0:13:53  lr: 0.000037  min_lr: 0.000037  loss: 2.9727 (2.9979)  loss_scale: 8192.0000 (16137.3149)  weight_decay: 0.0500 (0.0500)  time: 0.6934  data: 0.1074  max mem: 13008
Epoch: [2]  [2400/3542]  eta: 0:13:46  lr: 0.000037  min_lr: 0.000037  loss: 2.9844 (2.9977)  loss_scale: 8192.0000 (16104.2232)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.0988  max mem: 13008
Epoch: [2]  [2410/3542]  eta: 0:13:39  lr: 0.000037  min_lr: 0.000037  loss: 2.9844 (2.9978)  loss_scale: 8192.0000 (16071.4061)  weight_decay: 0.0500 (0.0500)  time: 0.7088  data: 0.1220  max mem: 13008
Epoch: [2]  [2420/3542]  eta: 0:13:31  lr: 0.000037  min_lr: 0.000037  loss: 2.9434 (2.9973)  loss_scale: 8192.0000 (16038.8600)  weight_decay: 0.0500 (0.0500)  time: 0.7250  data: 0.1384  max mem: 13008
Epoch: [2]  [2430/3542]  eta: 0:13:24  lr: 0.000037  min_lr: 0.000037  loss: 2.8926 (2.9969)  loss_scale: 8192.0000 (16006.5817)  weight_decay: 0.0500 (0.0500)  time: 0.6926  data: 0.1061  max mem: 13008
Epoch: [2]  [2440/3542]  eta: 0:13:17  lr: 0.000037  min_lr: 0.000037  loss: 2.9551 (2.9969)  loss_scale: 8192.0000 (15974.5678)  weight_decay: 0.0500 (0.0500)  time: 0.6985  data: 0.1114  max mem: 13008
Epoch: [2]  [2450/3542]  eta: 0:13:09  lr: 0.000037  min_lr: 0.000037  loss: 2.9551 (2.9966)  loss_scale: 8192.0000 (15942.8152)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.1170  max mem: 13008
Epoch: [2]  [2460/3542]  eta: 0:13:02  lr: 0.000037  min_lr: 0.000037  loss: 2.9688 (2.9967)  loss_scale: 8192.0000 (15911.3206)  weight_decay: 0.0500 (0.0500)  time: 0.6819  data: 0.0950  max mem: 13008
Epoch: [2]  [2470/3542]  eta: 0:12:54  lr: 0.000037  min_lr: 0.000037  loss: 2.9531 (2.9967)  loss_scale: 8192.0000 (15880.0809)  weight_decay: 0.0500 (0.0500)  time: 0.6933  data: 0.1063  max mem: 13008
Epoch: [2]  [2480/3542]  eta: 0:12:47  lr: 0.000037  min_lr: 0.000037  loss: 2.9219 (2.9967)  loss_scale: 8192.0000 (15849.0931)  weight_decay: 0.0500 (0.0500)  time: 0.6803  data: 0.0940  max mem: 13008
Epoch: [2]  [2490/3542]  eta: 0:12:39  lr: 0.000037  min_lr: 0.000037  loss: 2.9609 (2.9965)  loss_scale: 8192.0000 (15818.3541)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.0581  max mem: 13008
Epoch: [2]  [2500/3542]  eta: 0:12:32  lr: 0.000037  min_lr: 0.000037  loss: 2.9609 (2.9964)  loss_scale: 8192.0000 (15787.8609)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.0964  max mem: 13008
Epoch: [2]  [2510/3542]  eta: 0:12:25  lr: 0.000037  min_lr: 0.000037  loss: 2.9238 (2.9963)  loss_scale: 8192.0000 (15757.6105)  weight_decay: 0.0500 (0.0500)  time: 0.7415  data: 0.1553  max mem: 13008
Epoch: [2]  [2520/3542]  eta: 0:12:18  lr: 0.000037  min_lr: 0.000037  loss: 2.9473 (2.9962)  loss_scale: 8192.0000 (15727.6002)  weight_decay: 0.0500 (0.0500)  time: 0.7353  data: 0.1493  max mem: 13008
Epoch: [2]  [2530/3542]  eta: 0:12:11  lr: 0.000037  min_lr: 0.000037  loss: 2.9336 (2.9961)  loss_scale: 8192.0000 (15697.8269)  weight_decay: 0.0500 (0.0500)  time: 0.7208  data: 0.1345  max mem: 13008
Epoch: [2]  [2540/3542]  eta: 0:12:03  lr: 0.000037  min_lr: 0.000037  loss: 2.9746 (2.9963)  loss_scale: 8192.0000 (15668.2881)  weight_decay: 0.0500 (0.0500)  time: 0.7070  data: 0.1207  max mem: 13008
Epoch: [2]  [2550/3542]  eta: 0:11:56  lr: 0.000037  min_lr: 0.000037  loss: 3.0137 (2.9964)  loss_scale: 8192.0000 (15638.9808)  weight_decay: 0.0500 (0.0500)  time: 0.7000  data: 0.1138  max mem: 13008
Epoch: [2]  [2560/3542]  eta: 0:11:49  lr: 0.000037  min_lr: 0.000037  loss: 3.0781 (2.9965)  loss_scale: 8192.0000 (15609.9024)  weight_decay: 0.0500 (0.0500)  time: 0.7221  data: 0.1366  max mem: 13008
Epoch: [2]  [2570/3542]  eta: 0:11:41  lr: 0.000037  min_lr: 0.000037  loss: 3.0781 (2.9968)  loss_scale: 8192.0000 (15581.0502)  weight_decay: 0.0500 (0.0500)  time: 0.7001  data: 0.1140  max mem: 13008
Epoch: [2]  [2580/3542]  eta: 0:11:34  lr: 0.000037  min_lr: 0.000037  loss: 3.0312 (2.9965)  loss_scale: 8192.0000 (15552.4215)  weight_decay: 0.0500 (0.0500)  time: 0.6757  data: 0.0893  max mem: 13008
Epoch: [2]  [2590/3542]  eta: 0:11:27  lr: 0.000037  min_lr: 0.000037  loss: 2.9102 (2.9963)  loss_scale: 8192.0000 (15524.0139)  weight_decay: 0.0500 (0.0500)  time: 0.7093  data: 0.1238  max mem: 13008
Epoch: [2]  [2600/3542]  eta: 0:11:19  lr: 0.000037  min_lr: 0.000037  loss: 2.9043 (2.9961)  loss_scale: 8192.0000 (15495.8247)  weight_decay: 0.0500 (0.0500)  time: 0.7043  data: 0.1185  max mem: 13008
Epoch: [2]  [2610/3542]  eta: 0:11:12  lr: 0.000037  min_lr: 0.000037  loss: 2.9434 (2.9964)  loss_scale: 8192.0000 (15467.8514)  weight_decay: 0.0500 (0.0500)  time: 0.6938  data: 0.1080  max mem: 13008
Epoch: [2]  [2620/3542]  eta: 0:11:05  lr: 0.000037  min_lr: 0.000037  loss: 2.9434 (2.9962)  loss_scale: 8192.0000 (15440.0916)  weight_decay: 0.0500 (0.0500)  time: 0.6900  data: 0.1042  max mem: 13008
Epoch: [2]  [2630/3542]  eta: 0:10:57  lr: 0.000037  min_lr: 0.000037  loss: 2.8652 (2.9959)  loss_scale: 8192.0000 (15412.5428)  weight_decay: 0.0500 (0.0500)  time: 0.6793  data: 0.0935  max mem: 13008
Epoch: [2]  [2640/3542]  eta: 0:10:50  lr: 0.000036  min_lr: 0.000036  loss: 2.9062 (2.9958)  loss_scale: 8192.0000 (15385.2026)  weight_decay: 0.0500 (0.0500)  time: 0.7015  data: 0.1157  max mem: 13008
Epoch: [2]  [2650/3542]  eta: 0:10:43  lr: 0.000036  min_lr: 0.000036  loss: 3.0098 (2.9962)  loss_scale: 8192.0000 (15358.0687)  weight_decay: 0.0500 (0.0500)  time: 0.6929  data: 0.1065  max mem: 13008
Epoch: [2]  [2660/3542]  eta: 0:10:36  lr: 0.000036  min_lr: 0.000036  loss: 3.0957 (2.9959)  loss_scale: 8192.0000 (15331.1387)  weight_decay: 0.0500 (0.0500)  time: 0.6931  data: 0.1079  max mem: 13008
Epoch: [2]  [2670/3542]  eta: 0:10:28  lr: 0.000036  min_lr: 0.000036  loss: 3.0078 (2.9960)  loss_scale: 8192.0000 (15304.4103)  weight_decay: 0.0500 (0.0500)  time: 0.7009  data: 0.1171  max mem: 13008
Epoch: [2]  [2680/3542]  eta: 0:10:21  lr: 0.000036  min_lr: 0.000036  loss: 2.9414 (2.9956)  loss_scale: 8192.0000 (15277.8814)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.1221  max mem: 13008
Epoch: [2]  [2690/3542]  eta: 0:10:14  lr: 0.000036  min_lr: 0.000036  loss: 2.9434 (2.9956)  loss_scale: 8192.0000 (15251.5496)  weight_decay: 0.0500 (0.0500)  time: 0.7170  data: 0.1300  max mem: 13008
Epoch: [2]  [2700/3542]  eta: 0:10:06  lr: 0.000036  min_lr: 0.000036  loss: 2.9727 (2.9952)  loss_scale: 8192.0000 (15225.4128)  weight_decay: 0.0500 (0.0500)  time: 0.6969  data: 0.1100  max mem: 13008
Epoch: [2]  [2710/3542]  eta: 0:09:59  lr: 0.000036  min_lr: 0.000036  loss: 2.9297 (2.9950)  loss_scale: 8192.0000 (15199.4688)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.1083  max mem: 13008
Epoch: [2]  [2720/3542]  eta: 0:09:52  lr: 0.000036  min_lr: 0.000036  loss: 2.9707 (2.9951)  loss_scale: 8192.0000 (15173.7155)  weight_decay: 0.0500 (0.0500)  time: 0.7197  data: 0.1335  max mem: 13008
Epoch: [2]  [2730/3542]  eta: 0:09:45  lr: 0.000036  min_lr: 0.000036  loss: 2.9453 (2.9948)  loss_scale: 8192.0000 (15148.1509)  weight_decay: 0.0500 (0.0500)  time: 0.7365  data: 0.1498  max mem: 13008
Epoch: [2]  [2740/3542]  eta: 0:09:38  lr: 0.000036  min_lr: 0.000036  loss: 2.8984 (2.9945)  loss_scale: 8192.0000 (15122.7727)  weight_decay: 0.0500 (0.0500)  time: 0.7211  data: 0.1353  max mem: 13008
Epoch: [2]  [2750/3542]  eta: 0:09:30  lr: 0.000036  min_lr: 0.000036  loss: 2.8984 (2.9944)  loss_scale: 8192.0000 (15097.5791)  weight_decay: 0.0500 (0.0500)  time: 0.7177  data: 0.1327  max mem: 13008
Epoch: [2]  [2760/3542]  eta: 0:09:23  lr: 0.000036  min_lr: 0.000036  loss: 2.9668 (2.9943)  loss_scale: 8192.0000 (15072.5679)  weight_decay: 0.0500 (0.0500)  time: 0.7277  data: 0.1425  max mem: 13008
Epoch: [2]  [2770/3542]  eta: 0:09:16  lr: 0.000036  min_lr: 0.000036  loss: 2.9707 (2.9943)  loss_scale: 8192.0000 (15047.7373)  weight_decay: 0.0500 (0.0500)  time: 0.7049  data: 0.1193  max mem: 13008
Epoch: [2]  [2780/3542]  eta: 0:09:09  lr: 0.000036  min_lr: 0.000036  loss: 3.0156 (2.9945)  loss_scale: 8192.0000 (15023.0852)  weight_decay: 0.0500 (0.0500)  time: 0.6995  data: 0.1142  max mem: 13008
Epoch: [2]  [2790/3542]  eta: 0:09:02  lr: 0.000036  min_lr: 0.000036  loss: 3.0449 (2.9945)  loss_scale: 8192.0000 (14998.6098)  weight_decay: 0.0500 (0.0500)  time: 0.7260  data: 0.1406  max mem: 13008
Epoch: [2]  [2800/3542]  eta: 0:08:54  lr: 0.000036  min_lr: 0.000036  loss: 2.9199 (2.9940)  loss_scale: 8192.0000 (14974.3092)  weight_decay: 0.0500 (0.0500)  time: 0.7106  data: 0.1245  max mem: 13008
Epoch: [2]  [2810/3542]  eta: 0:08:47  lr: 0.000036  min_lr: 0.000036  loss: 2.8535 (2.9937)  loss_scale: 8192.0000 (14950.1814)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.0789  max mem: 13008
Epoch: [2]  [2820/3542]  eta: 0:08:40  lr: 0.000036  min_lr: 0.000036  loss: 2.9531 (2.9936)  loss_scale: 8192.0000 (14926.2247)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.1045  max mem: 13008
Epoch: [2]  [2830/3542]  eta: 0:08:32  lr: 0.000036  min_lr: 0.000036  loss: 2.9766 (2.9936)  loss_scale: 8192.0000 (14902.4373)  weight_decay: 0.0500 (0.0500)  time: 0.7202  data: 0.1341  max mem: 13008
Epoch: [2]  [2840/3542]  eta: 0:08:25  lr: 0.000036  min_lr: 0.000036  loss: 2.9844 (2.9938)  loss_scale: 8192.0000 (14878.8173)  weight_decay: 0.0500 (0.0500)  time: 0.6778  data: 0.0917  max mem: 13008
Epoch: [2]  [2850/3542]  eta: 0:08:18  lr: 0.000036  min_lr: 0.000036  loss: 3.0078 (2.9940)  loss_scale: 8192.0000 (14855.3630)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0746  max mem: 13008
Epoch: [2]  [2860/3542]  eta: 0:08:10  lr: 0.000036  min_lr: 0.000036  loss: 2.9980 (2.9939)  loss_scale: 8192.0000 (14832.0727)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.0800  max mem: 13008
Epoch: [2]  [2870/3542]  eta: 0:08:03  lr: 0.000036  min_lr: 0.000036  loss: 2.9609 (2.9938)  loss_scale: 8192.0000 (14808.9446)  weight_decay: 0.0500 (0.0500)  time: 0.6720  data: 0.0849  max mem: 13008
Epoch: [2]  [2880/3542]  eta: 0:07:56  lr: 0.000036  min_lr: 0.000036  loss: 2.9609 (2.9937)  loss_scale: 8192.0000 (14785.9771)  weight_decay: 0.0500 (0.0500)  time: 0.7038  data: 0.1168  max mem: 13008
Epoch: [2]  [2890/3542]  eta: 0:07:49  lr: 0.000036  min_lr: 0.000036  loss: 2.9297 (2.9936)  loss_scale: 8192.0000 (14763.1685)  weight_decay: 0.0500 (0.0500)  time: 0.7264  data: 0.1399  max mem: 13008
Epoch: [2]  [2900/3542]  eta: 0:07:42  lr: 0.000036  min_lr: 0.000036  loss: 2.9297 (2.9934)  loss_scale: 8192.0000 (14740.5171)  weight_decay: 0.0500 (0.0500)  time: 0.7119  data: 0.1254  max mem: 13008
Epoch: [2]  [2910/3542]  eta: 0:07:34  lr: 0.000036  min_lr: 0.000036  loss: 2.9570 (2.9935)  loss_scale: 8192.0000 (14718.0213)  weight_decay: 0.0500 (0.0500)  time: 0.7002  data: 0.1132  max mem: 13008
[2023-05-16 07:55:57,315] [INFO] [logging.py:60:log_dist] [Rank 0] step=10000, skipped=11, lr=[3.6183377994491694e-05, 3.6183377994491694e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 07:55:57,318] [INFO] [timer.py:157:stop] 0/10000, SamplesPerSec=55.58262748650593
Epoch: [2]  [2920/3542]  eta: 0:07:27  lr: 0.000036  min_lr: 0.000036  loss: 2.9570 (2.9937)  loss_scale: 8192.0000 (14695.6796)  weight_decay: 0.0500 (0.0500)  time: 0.6815  data: 0.0944  max mem: 13008
Epoch: [2]  [2930/3542]  eta: 0:07:20  lr: 0.000036  min_lr: 0.000036  loss: 2.9297 (2.9936)  loss_scale: 8192.0000 (14673.4903)  weight_decay: 0.0500 (0.0500)  time: 0.6750  data: 0.0884  max mem: 13008
Epoch: [2]  [2940/3542]  eta: 0:07:12  lr: 0.000036  min_lr: 0.000036  loss: 2.9375 (2.9935)  loss_scale: 8192.0000 (14651.4519)  weight_decay: 0.0500 (0.0500)  time: 0.6996  data: 0.1136  max mem: 13008
Epoch: [2]  [2950/3542]  eta: 0:07:05  lr: 0.000036  min_lr: 0.000036  loss: 2.9941 (2.9939)  loss_scale: 8192.0000 (14629.5629)  weight_decay: 0.0500 (0.0500)  time: 0.7102  data: 0.1241  max mem: 13008
Epoch: [2]  [2960/3542]  eta: 0:06:58  lr: 0.000036  min_lr: 0.000036  loss: 3.0273 (2.9938)  loss_scale: 8192.0000 (14607.8217)  weight_decay: 0.0500 (0.0500)  time: 0.7070  data: 0.1202  max mem: 13008
Epoch: [2]  [2970/3542]  eta: 0:06:51  lr: 0.000036  min_lr: 0.000036  loss: 2.9355 (2.9938)  loss_scale: 8192.0000 (14586.2269)  weight_decay: 0.0500 (0.0500)  time: 0.6937  data: 0.1074  max mem: 13008
Epoch: [2]  [2980/3542]  eta: 0:06:43  lr: 0.000036  min_lr: 0.000036  loss: 2.9375 (2.9936)  loss_scale: 8192.0000 (14564.7769)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.0875  max mem: 13008
Epoch: [2]  [2990/3542]  eta: 0:06:36  lr: 0.000036  min_lr: 0.000036  loss: 2.9180 (2.9933)  loss_scale: 8192.0000 (14543.4704)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.0979  max mem: 13008
Epoch: [2]  [3000/3542]  eta: 0:06:29  lr: 0.000036  min_lr: 0.000036  loss: 2.9473 (2.9933)  loss_scale: 8192.0000 (14522.3059)  weight_decay: 0.0500 (0.0500)  time: 0.7256  data: 0.1397  max mem: 13008
Epoch: [2]  [3010/3542]  eta: 0:06:22  lr: 0.000036  min_lr: 0.000036  loss: 2.9883 (2.9934)  loss_scale: 8192.0000 (14501.2820)  weight_decay: 0.0500 (0.0500)  time: 0.7234  data: 0.1370  max mem: 13008
Epoch: [2]  [3020/3542]  eta: 0:06:15  lr: 0.000036  min_lr: 0.000036  loss: 2.9453 (2.9931)  loss_scale: 8192.0000 (14480.3972)  weight_decay: 0.0500 (0.0500)  time: 0.7179  data: 0.1314  max mem: 13008
Epoch: [2]  [3030/3542]  eta: 0:06:07  lr: 0.000036  min_lr: 0.000036  loss: 2.9199 (2.9929)  loss_scale: 8192.0000 (14459.6503)  weight_decay: 0.0500 (0.0500)  time: 0.7016  data: 0.1154  max mem: 13008
Epoch: [2]  [3040/3542]  eta: 0:06:00  lr: 0.000036  min_lr: 0.000036  loss: 2.9766 (2.9928)  loss_scale: 8192.0000 (14439.0398)  weight_decay: 0.0500 (0.0500)  time: 0.6843  data: 0.0976  max mem: 13008
Epoch: [2]  [3050/3542]  eta: 0:05:53  lr: 0.000036  min_lr: 0.000036  loss: 2.9199 (2.9925)  loss_scale: 8192.0000 (14418.5644)  weight_decay: 0.0500 (0.0500)  time: 0.7081  data: 0.1208  max mem: 13008
Epoch: [2]  [3060/3542]  eta: 0:05:46  lr: 0.000036  min_lr: 0.000036  loss: 2.9062 (2.9923)  loss_scale: 8192.0000 (14398.2228)  weight_decay: 0.0500 (0.0500)  time: 0.7139  data: 0.1279  max mem: 13008
Epoch: [2]  [3070/3542]  eta: 0:05:39  lr: 0.000036  min_lr: 0.000036  loss: 2.9297 (2.9923)  loss_scale: 8192.0000 (14378.0137)  weight_decay: 0.0500 (0.0500)  time: 0.7205  data: 0.1350  max mem: 13008
Epoch: [2]  [3080/3542]  eta: 0:05:31  lr: 0.000036  min_lr: 0.000036  loss: 2.9570 (2.9922)  loss_scale: 8192.0000 (14357.9357)  weight_decay: 0.0500 (0.0500)  time: 0.7058  data: 0.1194  max mem: 13008
Epoch: [2]  [3090/3542]  eta: 0:05:24  lr: 0.000036  min_lr: 0.000036  loss: 3.0312 (2.9925)  loss_scale: 8192.0000 (14337.9877)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.0788  max mem: 13008
Epoch: [2]  [3100/3542]  eta: 0:05:17  lr: 0.000036  min_lr: 0.000036  loss: 3.0176 (2.9923)  loss_scale: 8192.0000 (14318.1683)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.0494  max mem: 13008
Epoch: [2]  [3110/3542]  eta: 0:05:10  lr: 0.000036  min_lr: 0.000036  loss: 2.9727 (2.9921)  loss_scale: 8192.0000 (14298.4764)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.0860  max mem: 13008
Epoch: [2]  [3120/3542]  eta: 0:05:03  lr: 0.000036  min_lr: 0.000036  loss: 2.9941 (2.9922)  loss_scale: 8192.0000 (14278.9106)  weight_decay: 0.0500 (0.0500)  time: 0.7227  data: 0.1358  max mem: 13008
Epoch: [2]  [3130/3542]  eta: 0:04:55  lr: 0.000036  min_lr: 0.000036  loss: 2.9434 (2.9919)  loss_scale: 8192.0000 (14259.4698)  weight_decay: 0.0500 (0.0500)  time: 0.7172  data: 0.1304  max mem: 13008
Epoch: [2]  [3140/3542]  eta: 0:04:48  lr: 0.000036  min_lr: 0.000036  loss: 2.9121 (2.9918)  loss_scale: 8192.0000 (14240.1528)  weight_decay: 0.0500 (0.0500)  time: 0.7118  data: 0.1254  max mem: 13008
Epoch: [2]  [3150/3542]  eta: 0:04:41  lr: 0.000036  min_lr: 0.000036  loss: 2.9023 (2.9915)  loss_scale: 8192.0000 (14220.9584)  weight_decay: 0.0500 (0.0500)  time: 0.7017  data: 0.1155  max mem: 13008
Epoch: [2]  [3160/3542]  eta: 0:04:34  lr: 0.000036  min_lr: 0.000036  loss: 2.9141 (2.9916)  loss_scale: 8192.0000 (14201.8855)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.0830  max mem: 13008
Epoch: [2]  [3170/3542]  eta: 0:04:26  lr: 0.000036  min_lr: 0.000036  loss: 3.0312 (2.9918)  loss_scale: 8192.0000 (14182.9328)  weight_decay: 0.0500 (0.0500)  time: 0.6785  data: 0.0923  max mem: 13008
Epoch: [2]  [3180/3542]  eta: 0:04:19  lr: 0.000036  min_lr: 0.000036  loss: 3.0312 (2.9920)  loss_scale: 8192.0000 (14164.0993)  weight_decay: 0.0500 (0.0500)  time: 0.7052  data: 0.1187  max mem: 13008
Epoch: [2]  [3190/3542]  eta: 0:04:12  lr: 0.000036  min_lr: 0.000036  loss: 3.0273 (2.9921)  loss_scale: 8192.0000 (14145.3839)  weight_decay: 0.0500 (0.0500)  time: 0.6941  data: 0.1071  max mem: 13008
Epoch: [2]  [3200/3542]  eta: 0:04:05  lr: 0.000036  min_lr: 0.000036  loss: 2.9805 (2.9919)  loss_scale: 8192.0000 (14126.7854)  weight_decay: 0.0500 (0.0500)  time: 0.6779  data: 0.0908  max mem: 13008
Epoch: [2]  [3210/3542]  eta: 0:03:58  lr: 0.000036  min_lr: 0.000036  loss: 2.9492 (2.9919)  loss_scale: 8192.0000 (14108.3027)  weight_decay: 0.0500 (0.0500)  time: 0.7227  data: 0.1366  max mem: 13008
Epoch: [2]  [3220/3542]  eta: 0:03:51  lr: 0.000036  min_lr: 0.000036  loss: 2.9102 (2.9918)  loss_scale: 8192.0000 (14089.9348)  weight_decay: 0.0500 (0.0500)  time: 0.7549  data: 0.1692  max mem: 13008
Epoch: [2]  [3230/3542]  eta: 0:03:43  lr: 0.000036  min_lr: 0.000036  loss: 2.9258 (2.9917)  loss_scale: 8192.0000 (14071.6806)  weight_decay: 0.0500 (0.0500)  time: 0.7117  data: 0.1252  max mem: 13008
Epoch: [2]  [3240/3542]  eta: 0:03:36  lr: 0.000036  min_lr: 0.000036  loss: 3.0098 (2.9922)  loss_scale: 8192.0000 (14053.5390)  weight_decay: 0.0500 (0.0500)  time: 0.6941  data: 0.1078  max mem: 13008
Epoch: [2]  [3250/3542]  eta: 0:03:29  lr: 0.000036  min_lr: 0.000036  loss: 3.1113 (2.9923)  loss_scale: 8192.0000 (14035.5091)  weight_decay: 0.0500 (0.0500)  time: 0.6891  data: 0.1033  max mem: 13008
Epoch: [2]  [3260/3542]  eta: 0:03:22  lr: 0.000036  min_lr: 0.000036  loss: 2.9863 (2.9923)  loss_scale: 8192.0000 (14017.5897)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0848  max mem: 13008
Epoch: [2]  [3270/3542]  eta: 0:03:15  lr: 0.000036  min_lr: 0.000036  loss: 2.9863 (2.9924)  loss_scale: 8192.0000 (13999.7799)  weight_decay: 0.0500 (0.0500)  time: 0.6953  data: 0.1080  max mem: 13008
Epoch: [2]  [3280/3542]  eta: 0:03:07  lr: 0.000036  min_lr: 0.000036  loss: 2.9766 (2.9923)  loss_scale: 8192.0000 (13982.0786)  weight_decay: 0.0500 (0.0500)  time: 0.6959  data: 0.1091  max mem: 13008
Epoch: [2]  [3290/3542]  eta: 0:03:00  lr: 0.000036  min_lr: 0.000036  loss: 2.9492 (2.9922)  loss_scale: 8192.0000 (13964.4850)  weight_decay: 0.0500 (0.0500)  time: 0.6851  data: 0.0989  max mem: 13008
[2023-05-16 08:00:26,262] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 08:00:26,263] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [2]  [3300/3542]  eta: 0:02:53  lr: 0.000036  min_lr: 0.000036  loss: 2.9590 (2.9922)  loss_scale: 8192.0000 (13949.4796)  weight_decay: 0.0500 (0.0500)  time: 0.6946  data: 0.1087  max mem: 13008
Epoch: [2]  [3310/3542]  eta: 0:02:46  lr: 0.000036  min_lr: 0.000036  loss: 3.0215 (2.9923)  loss_scale: 16384.0000 (13956.8324)  weight_decay: 0.0500 (0.0500)  time: 0.6899  data: 0.1031  max mem: 13008
Epoch: [2]  [3320/3542]  eta: 0:02:39  lr: 0.000036  min_lr: 0.000036  loss: 2.9668 (2.9921)  loss_scale: 16384.0000 (13964.1409)  weight_decay: 0.0500 (0.0500)  time: 0.6711  data: 0.0842  max mem: 13008
Epoch: [2]  [3330/3542]  eta: 0:02:31  lr: 0.000036  min_lr: 0.000036  loss: 2.9688 (2.9922)  loss_scale: 16384.0000 (13971.4056)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.1046  max mem: 13008
Epoch: [2]  [3340/3542]  eta: 0:02:24  lr: 0.000036  min_lr: 0.000036  loss: 3.0098 (2.9921)  loss_scale: 16384.0000 (13978.6268)  weight_decay: 0.0500 (0.0500)  time: 0.7247  data: 0.1393  max mem: 13008
Epoch: [2]  [3350/3542]  eta: 0:02:17  lr: 0.000036  min_lr: 0.000036  loss: 3.0098 (2.9922)  loss_scale: 16384.0000 (13985.8048)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.1056  max mem: 13008
Epoch: [2]  [3360/3542]  eta: 0:02:10  lr: 0.000036  min_lr: 0.000036  loss: 3.0215 (2.9924)  loss_scale: 16384.0000 (13992.9402)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.0739  max mem: 13008
Epoch: [2]  [3370/3542]  eta: 0:02:03  lr: 0.000036  min_lr: 0.000036  loss: 2.9727 (2.9924)  loss_scale: 16384.0000 (14000.0332)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0872  max mem: 13008
Epoch: [2]  [3380/3542]  eta: 0:01:56  lr: 0.000036  min_lr: 0.000036  loss: 2.9590 (2.9925)  loss_scale: 16384.0000 (14007.0843)  weight_decay: 0.0500 (0.0500)  time: 0.6851  data: 0.0989  max mem: 13008
Epoch: [2]  [3390/3542]  eta: 0:01:48  lr: 0.000036  min_lr: 0.000036  loss: 2.9531 (2.9923)  loss_scale: 16384.0000 (14014.0938)  weight_decay: 0.0500 (0.0500)  time: 0.6848  data: 0.0984  max mem: 13008
Epoch: [2]  [3400/3542]  eta: 0:01:41  lr: 0.000036  min_lr: 0.000036  loss: 2.9727 (2.9923)  loss_scale: 16384.0000 (14021.0620)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.0888  max mem: 13008
Epoch: [2]  [3410/3542]  eta: 0:01:34  lr: 0.000036  min_lr: 0.000036  loss: 2.9961 (2.9925)  loss_scale: 16384.0000 (14027.9894)  weight_decay: 0.0500 (0.0500)  time: 0.6684  data: 0.0822  max mem: 13008
Epoch: [2]  [3420/3542]  eta: 0:01:27  lr: 0.000036  min_lr: 0.000036  loss: 2.9746 (2.9925)  loss_scale: 16384.0000 (14034.8764)  weight_decay: 0.0500 (0.0500)  time: 0.6878  data: 0.1005  max mem: 13008
Epoch: [2]  [3430/3542]  eta: 0:01:20  lr: 0.000036  min_lr: 0.000036  loss: 2.9316 (2.9923)  loss_scale: 16384.0000 (14041.7231)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.0903  max mem: 13008
Epoch: [2]  [3440/3542]  eta: 0:01:12  lr: 0.000036  min_lr: 0.000036  loss: 2.9766 (2.9924)  loss_scale: 16384.0000 (14048.5301)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.0692  max mem: 13008
Epoch: [2]  [3450/3542]  eta: 0:01:05  lr: 0.000036  min_lr: 0.000036  loss: 3.0098 (2.9926)  loss_scale: 16384.0000 (14055.2976)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.0778  max mem: 13008
Epoch: [2]  [3460/3542]  eta: 0:00:58  lr: 0.000036  min_lr: 0.000036  loss: 3.0234 (2.9925)  loss_scale: 16384.0000 (14062.0260)  weight_decay: 0.0500 (0.0500)  time: 0.6877  data: 0.1026  max mem: 13008
Epoch: [2]  [3470/3542]  eta: 0:00:51  lr: 0.000036  min_lr: 0.000036  loss: 3.0234 (2.9926)  loss_scale: 16384.0000 (14068.7156)  weight_decay: 0.0500 (0.0500)  time: 0.6877  data: 0.1024  max mem: 13008
Epoch: [2]  [3480/3542]  eta: 0:00:44  lr: 0.000036  min_lr: 0.000036  loss: 2.9785 (2.9925)  loss_scale: 16384.0000 (14075.3668)  weight_decay: 0.0500 (0.0500)  time: 0.6731  data: 0.0861  max mem: 13008
Epoch: [2]  [3490/3542]  eta: 0:00:37  lr: 0.000036  min_lr: 0.000036  loss: 2.8594 (2.9920)  loss_scale: 16384.0000 (14081.9799)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.0731  max mem: 13008
Epoch: [2]  [3500/3542]  eta: 0:00:30  lr: 0.000035  min_lr: 0.000035  loss: 2.8652 (2.9920)  loss_scale: 16384.0000 (14088.5553)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.0757  max mem: 13008
Epoch: [2]  [3510/3542]  eta: 0:00:22  lr: 0.000035  min_lr: 0.000035  loss: 2.9844 (2.9919)  loss_scale: 16384.0000 (14095.0931)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.0776  max mem: 13008
Epoch: [2]  [3520/3542]  eta: 0:00:15  lr: 0.000035  min_lr: 0.000035  loss: 3.0098 (2.9920)  loss_scale: 16384.0000 (14101.5939)  weight_decay: 0.0500 (0.0500)  time: 0.6519  data: 0.0654  max mem: 13008
Epoch: [2]  [3530/3542]  eta: 0:00:08  lr: 0.000035  min_lr: 0.000035  loss: 2.8867 (2.9917)  loss_scale: 16384.0000 (14108.0578)  weight_decay: 0.0500 (0.0500)  time: 0.6858  data: 0.0994  max mem: 13008
Epoch: [2]  [3540/3542]  eta: 0:00:01  lr: 0.000035  min_lr: 0.000035  loss: 2.8672 (2.9914)  loss_scale: 16384.0000 (14114.4852)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.0679  max mem: 13008
Epoch: [2]  [3541/3542]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000035  loss: 2.8867 (2.9914)  loss_scale: 16384.0000 (14115.1259)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.0679  max mem: 13008
Epoch: [2] Total time: 0:42:10 (0.7143 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000035  loss: 2.8867 (2.9914)  loss_scale: 16384.0000 (14115.1259)  weight_decay: 0.0500 (0.0500)
[2023-05-16 08:03:09,280] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-2/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:33:32    time: 12.9015  data: 9.4832  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:10:50    time: 4.4573  data: 0.8623  max mem: 13008
Test:  [ 20/156]  eta: 0:09:14    time: 3.6337  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 30/156]  eta: 0:08:16    time: 3.6527  data: 0.0002  max mem: 13008
Test:  [ 40/156]  eta: 0:07:27    time: 3.6213  data: 0.0002  max mem: 13008
Test:  [ 50/156]  eta: 0:06:46    time: 3.6637  data: 0.0002  max mem: 13008
Test:  [ 60/156]  eta: 0:06:02    time: 3.6200  data: 0.0002  max mem: 13008
Test:  [ 70/156]  eta: 0:05:23    time: 3.5789  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:44    time: 3.6340  data: 0.0003  max mem: 13008
Test:  [ 90/156]  eta: 0:04:06    time: 3.6393  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:28    time: 3.6633  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:51    time: 3.6904  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:13    time: 3.6944  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:36    time: 3.6738  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:00:59    time: 3.6538  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:22    time: 3.5849  data: 0.0002  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.5908  data: 0.0002  max mem: 13008
Test: Total time: 0:09:37 (3.7029 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e2.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e2.json and result file is ./output_freeze/coco_captioning_result_val_e2.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e2.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1190388.71 tokens per second.
PTBTokenizer tokenized 60172 tokens at 423094.28 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 49324, 'reflen': 48375, 'guess': [49324, 44332, 39340, 34351], 'correct': [32299, 16411, 7149, 2783]}
ratio: 1.0196175710594104
Bleu_1: 0.655
Bleu_2: 0.492
Bleu_3: 0.353
Bleu_4: 0.244
computing METEOR score...
METEOR: 0.231
computing Rouge score...
ROUGE_L: 0.502
computing CIDEr score...
CIDEr: 0.793
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [21.34 seconds]
SPICE evaluation took: 33.07 s
SPICE: 0.169
Performance of the network on the 5000 val images: 0.8%
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 08:13:45,521] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-best/mp_rank_00_model_states.pt
Max performance: 0.79%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [3]  [   0/3542]  eta: 18:31:12  lr: 0.000035  min_lr: 0.000035  loss: 2.9824 (2.9824)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 18.8233  data: 18.1962  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [3]  [  10/3542]  eta: 3:08:21  lr: 0.000035  min_lr: 0.000035  loss: 2.9688 (2.9425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 3.1998  data: 2.6121  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [3]  [  20/3542]  eta: 2:18:35  lr: 0.000035  min_lr: 0.000035  loss: 2.9199 (2.9413)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5378  data: 0.9547  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [3]  [  30/3542]  eta: 1:55:49  lr: 0.000035  min_lr: 0.000035  loss: 2.9512 (2.9764)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3071  data: 0.7248  max mem: 13008
Epoch: [3]  [  40/3542]  eta: 1:47:46  lr: 0.000035  min_lr: 0.000035  loss: 2.9727 (2.9772)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3062  data: 0.7220  max mem: 13008
Epoch: [3]  [  50/3542]  eta: 1:41:23  lr: 0.000035  min_lr: 0.000035  loss: 2.9336 (2.9676)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3753  data: 0.7906  max mem: 13008
Epoch: [3]  [  60/3542]  eta: 1:35:12  lr: 0.000035  min_lr: 0.000035  loss: 2.8730 (2.9532)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2186  data: 0.6365  max mem: 13008
Epoch: [3]  [  70/3542]  eta: 1:29:09  lr: 0.000035  min_lr: 0.000035  loss: 2.8438 (2.9417)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0272  data: 0.4453  max mem: 13008
Epoch: [3]  [  80/3542]  eta: 1:24:02  lr: 0.000035  min_lr: 0.000035  loss: 2.8965 (2.9429)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8952  data: 0.3120  max mem: 13008
Epoch: [3]  [  90/3542]  eta: 1:19:33  lr: 0.000035  min_lr: 0.000035  loss: 2.9199 (2.9420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8223  data: 0.2392  max mem: 13008
Epoch: [3]  [ 100/3542]  eta: 1:16:06  lr: 0.000035  min_lr: 0.000035  loss: 2.8906 (2.9389)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8005  data: 0.2173  max mem: 13008
Epoch: [3]  [ 110/3542]  eta: 1:13:11  lr: 0.000035  min_lr: 0.000035  loss: 2.8555 (2.9310)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8093  data: 0.2251  max mem: 13008
Epoch: [3]  [ 120/3542]  eta: 1:10:26  lr: 0.000035  min_lr: 0.000035  loss: 2.9199 (2.9359)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7736  data: 0.1886  max mem: 13008
Epoch: [3]  [ 130/3542]  eta: 1:08:09  lr: 0.000035  min_lr: 0.000035  loss: 2.9355 (2.9349)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7497  data: 0.1643  max mem: 13008
Epoch: [3]  [ 140/3542]  eta: 1:05:54  lr: 0.000035  min_lr: 0.000035  loss: 2.8633 (2.9353)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7217  data: 0.1365  max mem: 13008
Epoch: [3]  [ 150/3542]  eta: 1:03:56  lr: 0.000035  min_lr: 0.000035  loss: 2.9473 (2.9381)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6883  data: 0.1037  max mem: 13008
Epoch: [3]  [ 160/3542]  eta: 1:02:25  lr: 0.000035  min_lr: 0.000035  loss: 2.9238 (2.9348)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7216  data: 0.1374  max mem: 13008
Epoch: [3]  [ 170/3542]  eta: 1:01:14  lr: 0.000035  min_lr: 0.000035  loss: 2.9102 (2.9382)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7770  data: 0.1924  max mem: 13008
Epoch: [3]  [ 180/3542]  eta: 0:59:58  lr: 0.000035  min_lr: 0.000035  loss: 3.0352 (2.9425)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7704  data: 0.1852  max mem: 13008
Epoch: [3]  [ 190/3542]  eta: 0:58:45  lr: 0.000035  min_lr: 0.000035  loss: 2.9473 (2.9423)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7280  data: 0.1428  max mem: 13008
Epoch: [3]  [ 200/3542]  eta: 0:57:37  lr: 0.000035  min_lr: 0.000035  loss: 2.9121 (2.9402)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7115  data: 0.1264  max mem: 13008
Epoch: [3]  [ 210/3542]  eta: 0:56:42  lr: 0.000035  min_lr: 0.000035  loss: 2.9121 (2.9429)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.1434  max mem: 13008
Epoch: [3]  [ 220/3542]  eta: 0:55:36  lr: 0.000035  min_lr: 0.000035  loss: 2.9336 (2.9451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7010  data: 0.1159  max mem: 13008
Epoch: [3]  [ 230/3542]  eta: 0:54:32  lr: 0.000035  min_lr: 0.000035  loss: 2.8965 (2.9451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.0547  max mem: 13008
Epoch: [3]  [ 240/3542]  eta: 0:53:44  lr: 0.000035  min_lr: 0.000035  loss: 2.9473 (2.9456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.0824  max mem: 13008
Epoch: [3]  [ 250/3542]  eta: 0:53:01  lr: 0.000035  min_lr: 0.000035  loss: 2.9336 (2.9449)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7174  data: 0.1327  max mem: 13008
Epoch: [3]  [ 260/3542]  eta: 0:52:23  lr: 0.000035  min_lr: 0.000035  loss: 2.8516 (2.9434)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7322  data: 0.1464  max mem: 13008
Epoch: [3]  [ 270/3542]  eta: 0:51:45  lr: 0.000035  min_lr: 0.000035  loss: 2.9102 (2.9441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7301  data: 0.1443  max mem: 13008
Epoch: [3]  [ 280/3542]  eta: 0:51:02  lr: 0.000035  min_lr: 0.000035  loss: 2.9121 (2.9431)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6913  data: 0.1062  max mem: 13008
Epoch: [3]  [ 290/3542]  eta: 0:50:23  lr: 0.000035  min_lr: 0.000035  loss: 2.9453 (2.9427)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6663  data: 0.0807  max mem: 13008
Epoch: [3]  [ 300/3542]  eta: 0:49:42  lr: 0.000035  min_lr: 0.000035  loss: 2.9453 (2.9420)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0706  max mem: 13008
Epoch: [3]  [ 310/3542]  eta: 0:49:08  lr: 0.000035  min_lr: 0.000035  loss: 2.9141 (2.9410)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0720  max mem: 13008
Epoch: [3]  [ 320/3542]  eta: 0:48:34  lr: 0.000035  min_lr: 0.000035  loss: 2.8789 (2.9394)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6723  data: 0.0873  max mem: 13008
Epoch: [3]  [ 330/3542]  eta: 0:48:00  lr: 0.000035  min_lr: 0.000035  loss: 2.9023 (2.9398)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6569  data: 0.0715  max mem: 13008
Epoch: [3]  [ 340/3542]  eta: 0:47:29  lr: 0.000035  min_lr: 0.000035  loss: 2.9355 (2.9391)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.0681  max mem: 13008
Epoch: [3]  [ 350/3542]  eta: 0:47:00  lr: 0.000035  min_lr: 0.000035  loss: 2.8906 (2.9400)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6669  data: 0.0811  max mem: 13008
Epoch: [3]  [ 360/3542]  eta: 0:46:32  lr: 0.000035  min_lr: 0.000035  loss: 2.9531 (2.9426)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6698  data: 0.0839  max mem: 13008
Epoch: [3]  [ 370/3542]  eta: 0:46:05  lr: 0.000035  min_lr: 0.000035  loss: 2.9277 (2.9399)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6666  data: 0.0804  max mem: 13008
[2023-05-16 08:19:12,897] [INFO] [logging.py:60:log_dist] [Rank 0] step=11000, skipped=11, lr=[3.496712447911069e-05, 3.496712447911069e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 08:19:12,900] [INFO] [timer.py:157:stop] 0/11000, SamplesPerSec=55.58071799628299
Epoch: [3]  [ 380/3542]  eta: 0:45:48  lr: 0.000035  min_lr: 0.000035  loss: 2.8770 (2.9397)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7156  data: 0.1295  max mem: 13008
Epoch: [3]  [ 390/3542]  eta: 0:45:28  lr: 0.000035  min_lr: 0.000035  loss: 2.9609 (2.9421)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7487  data: 0.1623  max mem: 13008
Epoch: [3]  [ 400/3542]  eta: 0:45:07  lr: 0.000035  min_lr: 0.000035  loss: 2.9961 (2.9430)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7201  data: 0.1335  max mem: 13008
Epoch: [3]  [ 410/3542]  eta: 0:44:43  lr: 0.000035  min_lr: 0.000035  loss: 2.9531 (2.9454)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6810  data: 0.0953  max mem: 13008
Epoch: [3]  [ 420/3542]  eta: 0:44:20  lr: 0.000035  min_lr: 0.000035  loss: 2.9727 (2.9458)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.0761  max mem: 13008
Epoch: [3]  [ 430/3542]  eta: 0:43:57  lr: 0.000035  min_lr: 0.000035  loss: 2.8984 (2.9442)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.0732  max mem: 13008
Epoch: [3]  [ 440/3542]  eta: 0:43:35  lr: 0.000035  min_lr: 0.000035  loss: 2.9199 (2.9443)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6484  data: 0.0636  max mem: 13008
Epoch: [3]  [ 450/3542]  eta: 0:43:15  lr: 0.000035  min_lr: 0.000035  loss: 2.9219 (2.9441)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6685  data: 0.0826  max mem: 13008
Epoch: [3]  [ 460/3542]  eta: 0:42:57  lr: 0.000035  min_lr: 0.000035  loss: 2.8906 (2.9438)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.1041  max mem: 13008
Epoch: [3]  [ 470/3542]  eta: 0:42:41  lr: 0.000035  min_lr: 0.000035  loss: 2.8906 (2.9451)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.1234  max mem: 13008
Epoch: [3]  [ 480/3542]  eta: 0:42:26  lr: 0.000035  min_lr: 0.000035  loss: 2.9277 (2.9455)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7227  data: 0.1372  max mem: 13008
Epoch: [3]  [ 490/3542]  eta: 0:42:08  lr: 0.000035  min_lr: 0.000035  loss: 2.9492 (2.9458)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7021  data: 0.1158  max mem: 13008
Epoch: [3]  [ 500/3542]  eta: 0:41:48  lr: 0.000035  min_lr: 0.000035  loss: 2.8789 (2.9446)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.0717  max mem: 13008
Epoch: [3]  [ 510/3542]  eta: 0:41:32  lr: 0.000035  min_lr: 0.000035  loss: 2.8828 (2.9456)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.0729  max mem: 13008
Epoch: [3]  [ 520/3542]  eta: 0:41:16  lr: 0.000035  min_lr: 0.000035  loss: 3.0371 (2.9478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.1025  max mem: 13008
Epoch: [3]  [ 530/3542]  eta: 0:41:00  lr: 0.000035  min_lr: 0.000035  loss: 3.0234 (2.9473)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6903  data: 0.1041  max mem: 13008
Epoch: [3]  [ 540/3542]  eta: 0:40:44  lr: 0.000035  min_lr: 0.000035  loss: 2.9180 (2.9468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.0908  max mem: 13008
Epoch: [3]  [ 550/3542]  eta: 0:40:29  lr: 0.000035  min_lr: 0.000035  loss: 2.8887 (2.9460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0874  max mem: 13008
Epoch: [3]  [ 560/3542]  eta: 0:40:13  lr: 0.000035  min_lr: 0.000035  loss: 2.8945 (2.9453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6799  data: 0.0938  max mem: 13008
Epoch: [3]  [ 570/3542]  eta: 0:40:01  lr: 0.000035  min_lr: 0.000035  loss: 2.9414 (2.9452)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6989  data: 0.1135  max mem: 13008
Epoch: [3]  [ 580/3542]  eta: 0:39:45  lr: 0.000035  min_lr: 0.000035  loss: 2.9219 (2.9453)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6859  data: 0.0994  max mem: 13008
Epoch: [3]  [ 590/3542]  eta: 0:39:32  lr: 0.000035  min_lr: 0.000035  loss: 2.9648 (2.9457)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6874  data: 0.1011  max mem: 13008
Epoch: [3]  [ 600/3542]  eta: 0:39:18  lr: 0.000035  min_lr: 0.000035  loss: 2.9727 (2.9457)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7029  data: 0.1167  max mem: 13008
Epoch: [3]  [ 610/3542]  eta: 0:39:06  lr: 0.000035  min_lr: 0.000035  loss: 2.9277 (2.9462)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7001  data: 0.1143  max mem: 13008
Epoch: [3]  [ 620/3542]  eta: 0:38:53  lr: 0.000035  min_lr: 0.000035  loss: 2.9414 (2.9463)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7019  data: 0.1167  max mem: 13008
Epoch: [3]  [ 630/3542]  eta: 0:38:40  lr: 0.000035  min_lr: 0.000035  loss: 2.9785 (2.9476)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6938  data: 0.1074  max mem: 13008
Epoch: [3]  [ 640/3542]  eta: 0:38:26  lr: 0.000035  min_lr: 0.000035  loss: 2.9824 (2.9484)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.0929  max mem: 13008
Epoch: [3]  [ 650/3542]  eta: 0:38:12  lr: 0.000035  min_lr: 0.000035  loss: 2.9922 (2.9494)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6576  data: 0.0717  max mem: 13008
Epoch: [3]  [ 660/3542]  eta: 0:38:02  lr: 0.000035  min_lr: 0.000035  loss: 2.9141 (2.9478)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.1105  max mem: 13008
Epoch: [3]  [ 670/3542]  eta: 0:37:48  lr: 0.000035  min_lr: 0.000035  loss: 2.8438 (2.9461)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6952  data: 0.1096  max mem: 13008
Epoch: [3]  [ 680/3542]  eta: 0:37:36  lr: 0.000035  min_lr: 0.000035  loss: 2.8652 (2.9468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0856  max mem: 13008
Epoch: [3]  [ 690/3542]  eta: 0:37:25  lr: 0.000035  min_lr: 0.000035  loss: 2.9297 (2.9462)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7003  data: 0.1150  max mem: 13008
Epoch: [3]  [ 700/3542]  eta: 0:37:14  lr: 0.000035  min_lr: 0.000035  loss: 2.9297 (2.9460)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7100  data: 0.1237  max mem: 13008
Epoch: [3]  [ 710/3542]  eta: 0:37:01  lr: 0.000035  min_lr: 0.000035  loss: 2.8750 (2.9454)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.0986  max mem: 13008
Epoch: [3]  [ 720/3542]  eta: 0:36:50  lr: 0.000035  min_lr: 0.000035  loss: 2.9434 (2.9468)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6864  data: 0.1000  max mem: 13008
Epoch: [3]  [ 730/3542]  eta: 0:36:37  lr: 0.000034  min_lr: 0.000034  loss: 3.0176 (2.9470)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.0962  max mem: 13008
Epoch: [3]  [ 740/3542]  eta: 0:36:25  lr: 0.000034  min_lr: 0.000034  loss: 2.9531 (2.9466)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0751  max mem: 13008
Epoch: [3]  [ 750/3542]  eta: 0:36:16  lr: 0.000034  min_lr: 0.000034  loss: 2.9258 (2.9470)  loss_scale: 16384.0000 (16384.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7079  data: 0.1228  max mem: 13008
[2023-05-16 08:23:37,520] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 08:23:37,520] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
Epoch: [3]  [ 760/3542]  eta: 0:36:05  lr: 0.000034  min_lr: 0.000034  loss: 2.9102 (2.9460)  loss_scale: 16384.0000 (16448.5887)  weight_decay: 0.0500 (0.0500)  time: 0.7127  data: 0.1271  max mem: 13008
[2023-05-16 08:23:42,086] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 11390
[2023-05-16 08:23:42,086] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-05-16 08:23:42,086] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [3]  [ 770/3542]  eta: 0:35:53  lr: 0.000034  min_lr: 0.000034  loss: 2.8750 (2.9463)  loss_scale: 16384.0000 (16511.5019)  weight_decay: 0.0500 (0.0500)  time: 0.6737  data: 0.0878  max mem: 13008
Epoch: [3]  [ 780/3542]  eta: 0:35:44  lr: 0.000034  min_lr: 0.000034  loss: 2.9102 (2.9463)  loss_scale: 16384.0000 (16509.8694)  weight_decay: 0.0500 (0.0500)  time: 0.7072  data: 0.1209  max mem: 13008
Epoch: [3]  [ 790/3542]  eta: 0:35:34  lr: 0.000034  min_lr: 0.000034  loss: 2.9473 (2.9469)  loss_scale: 16384.0000 (16508.2781)  weight_decay: 0.0500 (0.0500)  time: 0.7317  data: 0.1456  max mem: 13008
Epoch: [3]  [ 800/3542]  eta: 0:35:23  lr: 0.000034  min_lr: 0.000034  loss: 2.9551 (2.9474)  loss_scale: 16384.0000 (16506.7266)  weight_decay: 0.0500 (0.0500)  time: 0.7018  data: 0.1164  max mem: 13008
Epoch: [3]  [ 810/3542]  eta: 0:35:12  lr: 0.000034  min_lr: 0.000034  loss: 2.9492 (2.9467)  loss_scale: 16384.0000 (16505.2133)  weight_decay: 0.0500 (0.0500)  time: 0.6774  data: 0.0923  max mem: 13008
Epoch: [3]  [ 820/3542]  eta: 0:35:00  lr: 0.000034  min_lr: 0.000034  loss: 2.9434 (2.9470)  loss_scale: 16384.0000 (16503.7369)  weight_decay: 0.0500 (0.0500)  time: 0.6517  data: 0.0658  max mem: 13008
Epoch: [3]  [ 830/3542]  eta: 0:34:51  lr: 0.000034  min_lr: 0.000034  loss: 2.9668 (2.9466)  loss_scale: 16384.0000 (16502.2960)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.1005  max mem: 13008
Epoch: [3]  [ 840/3542]  eta: 0:34:40  lr: 0.000034  min_lr: 0.000034  loss: 2.9062 (2.9461)  loss_scale: 16384.0000 (16500.8894)  weight_decay: 0.0500 (0.0500)  time: 0.7025  data: 0.1169  max mem: 13008
Epoch: [3]  [ 850/3542]  eta: 0:34:29  lr: 0.000034  min_lr: 0.000034  loss: 2.8965 (2.9466)  loss_scale: 16384.0000 (16499.5159)  weight_decay: 0.0500 (0.0500)  time: 0.6737  data: 0.0869  max mem: 13008
Epoch: [3]  [ 860/3542]  eta: 0:34:18  lr: 0.000034  min_lr: 0.000034  loss: 2.8965 (2.9467)  loss_scale: 16384.0000 (16498.1742)  weight_decay: 0.0500 (0.0500)  time: 0.6674  data: 0.0811  max mem: 13008
Epoch: [3]  [ 870/3542]  eta: 0:34:08  lr: 0.000034  min_lr: 0.000034  loss: 2.9355 (2.9464)  loss_scale: 16384.0000 (16496.8634)  weight_decay: 0.0500 (0.0500)  time: 0.6705  data: 0.0853  max mem: 13008
Epoch: [3]  [ 880/3542]  eta: 0:33:58  lr: 0.000034  min_lr: 0.000034  loss: 2.9512 (2.9469)  loss_scale: 16384.0000 (16495.5823)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.0970  max mem: 13008
Epoch: [3]  [ 890/3542]  eta: 0:33:48  lr: 0.000034  min_lr: 0.000034  loss: 2.9336 (2.9463)  loss_scale: 16384.0000 (16494.3300)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.1013  max mem: 13008
Epoch: [3]  [ 900/3542]  eta: 0:33:39  lr: 0.000034  min_lr: 0.000034  loss: 2.8789 (2.9460)  loss_scale: 16384.0000 (16493.1054)  weight_decay: 0.0500 (0.0500)  time: 0.6998  data: 0.1135  max mem: 13008
Epoch: [3]  [ 910/3542]  eta: 0:33:29  lr: 0.000034  min_lr: 0.000034  loss: 2.9023 (2.9455)  loss_scale: 16384.0000 (16491.9078)  weight_decay: 0.0500 (0.0500)  time: 0.7031  data: 0.1172  max mem: 13008
Epoch: [3]  [ 920/3542]  eta: 0:33:20  lr: 0.000034  min_lr: 0.000034  loss: 2.8164 (2.9433)  loss_scale: 16384.0000 (16490.7362)  weight_decay: 0.0500 (0.0500)  time: 0.6995  data: 0.1136  max mem: 13008
Epoch: [3]  [ 930/3542]  eta: 0:33:10  lr: 0.000034  min_lr: 0.000034  loss: 2.7500 (2.9428)  loss_scale: 16384.0000 (16489.5897)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.1104  max mem: 13008
Epoch: [3]  [ 940/3542]  eta: 0:33:00  lr: 0.000034  min_lr: 0.000034  loss: 2.8535 (2.9420)  loss_scale: 16384.0000 (16488.4676)  weight_decay: 0.0500 (0.0500)  time: 0.6848  data: 0.0996  max mem: 13008
Epoch: [3]  [ 950/3542]  eta: 0:32:52  lr: 0.000034  min_lr: 0.000034  loss: 2.9141 (2.9420)  loss_scale: 16384.0000 (16487.3691)  weight_decay: 0.0500 (0.0500)  time: 0.7056  data: 0.1204  max mem: 13008
Epoch: [3]  [ 960/3542]  eta: 0:32:42  lr: 0.000034  min_lr: 0.000034  loss: 2.9141 (2.9412)  loss_scale: 16384.0000 (16486.2934)  weight_decay: 0.0500 (0.0500)  time: 0.7114  data: 0.1253  max mem: 13008
Epoch: [3]  [ 970/3542]  eta: 0:32:32  lr: 0.000034  min_lr: 0.000034  loss: 2.8867 (2.9412)  loss_scale: 16384.0000 (16485.2400)  weight_decay: 0.0500 (0.0500)  time: 0.6770  data: 0.0900  max mem: 13008
Epoch: [3]  [ 980/3542]  eta: 0:32:24  lr: 0.000034  min_lr: 0.000034  loss: 2.8906 (2.9416)  loss_scale: 16384.0000 (16484.2080)  weight_decay: 0.0500 (0.0500)  time: 0.7032  data: 0.1162  max mem: 13008
Epoch: [3]  [ 990/3542]  eta: 0:32:13  lr: 0.000034  min_lr: 0.000034  loss: 2.9238 (2.9412)  loss_scale: 16384.0000 (16483.1968)  weight_decay: 0.0500 (0.0500)  time: 0.6792  data: 0.0918  max mem: 13008
Epoch: [3]  [1000/3542]  eta: 0:32:05  lr: 0.000034  min_lr: 0.000034  loss: 2.9160 (2.9409)  loss_scale: 16384.0000 (16482.2058)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.1000  max mem: 13008
Epoch: [3]  [1010/3542]  eta: 0:31:56  lr: 0.000034  min_lr: 0.000034  loss: 2.8750 (2.9406)  loss_scale: 16384.0000 (16481.2344)  weight_decay: 0.0500 (0.0500)  time: 0.7148  data: 0.1294  max mem: 13008
Epoch: [3]  [1020/3542]  eta: 0:31:47  lr: 0.000034  min_lr: 0.000034  loss: 2.8750 (2.9402)  loss_scale: 16384.0000 (16480.2821)  weight_decay: 0.0500 (0.0500)  time: 0.6905  data: 0.1042  max mem: 13008
Epoch: [3]  [1030/3542]  eta: 0:31:37  lr: 0.000034  min_lr: 0.000034  loss: 2.9160 (2.9399)  loss_scale: 16384.0000 (16479.3482)  weight_decay: 0.0500 (0.0500)  time: 0.6824  data: 0.0954  max mem: 13008
Epoch: [3]  [1040/3542]  eta: 0:31:28  lr: 0.000034  min_lr: 0.000034  loss: 2.9316 (2.9406)  loss_scale: 16384.0000 (16478.4323)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.0960  max mem: 13008
Epoch: [3]  [1050/3542]  eta: 0:31:19  lr: 0.000034  min_lr: 0.000034  loss: 2.9316 (2.9405)  loss_scale: 16384.0000 (16477.5338)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.1034  max mem: 13008
Epoch: [3]  [1060/3542]  eta: 0:31:10  lr: 0.000034  min_lr: 0.000034  loss: 2.9023 (2.9396)  loss_scale: 16384.0000 (16476.6522)  weight_decay: 0.0500 (0.0500)  time: 0.6844  data: 0.0977  max mem: 13008
Epoch: [3]  [1070/3542]  eta: 0:31:00  lr: 0.000034  min_lr: 0.000034  loss: 2.9180 (2.9400)  loss_scale: 16384.0000 (16475.7871)  weight_decay: 0.0500 (0.0500)  time: 0.6832  data: 0.0968  max mem: 13008
Epoch: [3]  [1080/3542]  eta: 0:30:51  lr: 0.000034  min_lr: 0.000034  loss: 2.9219 (2.9392)  loss_scale: 16384.0000 (16474.9380)  weight_decay: 0.0500 (0.0500)  time: 0.6868  data: 0.0998  max mem: 13008
Epoch: [3]  [1090/3542]  eta: 0:30:42  lr: 0.000034  min_lr: 0.000034  loss: 2.8496 (2.9387)  loss_scale: 16384.0000 (16474.1045)  weight_decay: 0.0500 (0.0500)  time: 0.6863  data: 0.0991  max mem: 13008
Epoch: [3]  [1100/3542]  eta: 0:30:33  lr: 0.000034  min_lr: 0.000034  loss: 2.9238 (2.9391)  loss_scale: 16384.0000 (16473.2861)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.0981  max mem: 13008
Epoch: [3]  [1110/3542]  eta: 0:30:24  lr: 0.000034  min_lr: 0.000034  loss: 2.9727 (2.9393)  loss_scale: 16384.0000 (16472.4824)  weight_decay: 0.0500 (0.0500)  time: 0.6791  data: 0.0928  max mem: 13008
Epoch: [3]  [1120/3542]  eta: 0:30:16  lr: 0.000034  min_lr: 0.000034  loss: 2.8945 (2.9395)  loss_scale: 16384.0000 (16471.6931)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.1003  max mem: 13008
Epoch: [3]  [1130/3542]  eta: 0:30:07  lr: 0.000034  min_lr: 0.000034  loss: 2.9297 (2.9396)  loss_scale: 16384.0000 (16470.9178)  weight_decay: 0.0500 (0.0500)  time: 0.7011  data: 0.1155  max mem: 13008
Epoch: [3]  [1140/3542]  eta: 0:29:59  lr: 0.000034  min_lr: 0.000034  loss: 2.9668 (2.9403)  loss_scale: 16384.0000 (16470.1560)  weight_decay: 0.0500 (0.0500)  time: 0.7042  data: 0.1188  max mem: 13008
Epoch: [3]  [1150/3542]  eta: 0:29:49  lr: 0.000034  min_lr: 0.000034  loss: 2.9062 (2.9402)  loss_scale: 16384.0000 (16469.4075)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.1055  max mem: 13008
Epoch: [3]  [1160/3542]  eta: 0:29:40  lr: 0.000034  min_lr: 0.000034  loss: 2.9062 (2.9408)  loss_scale: 16384.0000 (16468.6718)  weight_decay: 0.0500 (0.0500)  time: 0.6646  data: 0.0786  max mem: 13008
Epoch: [3]  [1170/3542]  eta: 0:29:32  lr: 0.000034  min_lr: 0.000034  loss: 2.9375 (2.9401)  loss_scale: 16384.0000 (16467.9488)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.0998  max mem: 13008
Epoch: [3]  [1180/3542]  eta: 0:29:23  lr: 0.000034  min_lr: 0.000034  loss: 2.8926 (2.9397)  loss_scale: 16384.0000 (16467.2379)  weight_decay: 0.0500 (0.0500)  time: 0.6950  data: 0.1094  max mem: 13008
Epoch: [3]  [1190/3542]  eta: 0:29:15  lr: 0.000034  min_lr: 0.000034  loss: 2.9258 (2.9401)  loss_scale: 16384.0000 (16466.5390)  weight_decay: 0.0500 (0.0500)  time: 0.7068  data: 0.1204  max mem: 13008
Epoch: [3]  [1200/3542]  eta: 0:29:08  lr: 0.000034  min_lr: 0.000034  loss: 2.9121 (2.9399)  loss_scale: 16384.0000 (16465.8518)  weight_decay: 0.0500 (0.0500)  time: 0.7350  data: 0.1481  max mem: 13008
Epoch: [3]  [1210/3542]  eta: 0:29:00  lr: 0.000034  min_lr: 0.000034  loss: 2.9102 (2.9397)  loss_scale: 16384.0000 (16465.1759)  weight_decay: 0.0500 (0.0500)  time: 0.7218  data: 0.1350  max mem: 13008
Epoch: [3]  [1220/3542]  eta: 0:28:51  lr: 0.000034  min_lr: 0.000034  loss: 2.9727 (2.9395)  loss_scale: 16384.0000 (16464.5111)  weight_decay: 0.0500 (0.0500)  time: 0.7021  data: 0.1164  max mem: 13008
Epoch: [3]  [1230/3542]  eta: 0:28:43  lr: 0.000034  min_lr: 0.000034  loss: 2.9258 (2.9395)  loss_scale: 16384.0000 (16463.8570)  weight_decay: 0.0500 (0.0500)  time: 0.7147  data: 0.1289  max mem: 13008
Epoch: [3]  [1240/3542]  eta: 0:28:34  lr: 0.000034  min_lr: 0.000034  loss: 2.9570 (2.9397)  loss_scale: 16384.0000 (16463.2135)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.1006  max mem: 13008
Epoch: [3]  [1250/3542]  eta: 0:28:26  lr: 0.000034  min_lr: 0.000034  loss: 2.9570 (2.9395)  loss_scale: 16384.0000 (16462.5803)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.0906  max mem: 13008
Epoch: [3]  [1260/3542]  eta: 0:28:17  lr: 0.000034  min_lr: 0.000034  loss: 2.9531 (2.9399)  loss_scale: 16384.0000 (16461.9572)  weight_decay: 0.0500 (0.0500)  time: 0.6973  data: 0.1113  max mem: 13008
Epoch: [3]  [1270/3542]  eta: 0:28:10  lr: 0.000034  min_lr: 0.000034  loss: 3.0176 (2.9408)  loss_scale: 16384.0000 (16461.3438)  weight_decay: 0.0500 (0.0500)  time: 0.7074  data: 0.1209  max mem: 13008
Epoch: [3]  [1280/3542]  eta: 0:28:01  lr: 0.000034  min_lr: 0.000034  loss: 2.9473 (2.9407)  loss_scale: 16384.0000 (16460.7400)  weight_decay: 0.0500 (0.0500)  time: 0.6985  data: 0.1120  max mem: 13008
Epoch: [3]  [1290/3542]  eta: 0:27:53  lr: 0.000034  min_lr: 0.000034  loss: 2.8633 (2.9398)  loss_scale: 16384.0000 (16460.1456)  weight_decay: 0.0500 (0.0500)  time: 0.6838  data: 0.0982  max mem: 13008
Epoch: [3]  [1300/3542]  eta: 0:27:45  lr: 0.000034  min_lr: 0.000034  loss: 2.8613 (2.9396)  loss_scale: 16384.0000 (16459.5603)  weight_decay: 0.0500 (0.0500)  time: 0.7245  data: 0.1382  max mem: 13008
Epoch: [3]  [1310/3542]  eta: 0:27:37  lr: 0.000034  min_lr: 0.000034  loss: 2.9141 (2.9395)  loss_scale: 16384.0000 (16458.9840)  weight_decay: 0.0500 (0.0500)  time: 0.7040  data: 0.1169  max mem: 13008
Epoch: [3]  [1320/3542]  eta: 0:27:29  lr: 0.000034  min_lr: 0.000034  loss: 2.8691 (2.9393)  loss_scale: 16384.0000 (16458.4164)  weight_decay: 0.0500 (0.0500)  time: 0.7010  data: 0.1139  max mem: 13008
Epoch: [3]  [1330/3542]  eta: 0:27:21  lr: 0.000034  min_lr: 0.000034  loss: 2.8848 (2.9395)  loss_scale: 16384.0000 (16457.8573)  weight_decay: 0.0500 (0.0500)  time: 0.7270  data: 0.1403  max mem: 13008
Epoch: [3]  [1340/3542]  eta: 0:27:14  lr: 0.000034  min_lr: 0.000034  loss: 2.9590 (2.9397)  loss_scale: 16384.0000 (16457.3065)  weight_decay: 0.0500 (0.0500)  time: 0.7440  data: 0.1582  max mem: 13008
Epoch: [3]  [1350/3542]  eta: 0:27:09  lr: 0.000034  min_lr: 0.000034  loss: 2.9434 (2.9401)  loss_scale: 16384.0000 (16456.7639)  weight_decay: 0.0500 (0.0500)  time: 0.8340  data: 0.2465  max mem: 13008
Epoch: [3]  [1360/3542]  eta: 0:27:01  lr: 0.000034  min_lr: 0.000034  loss: 2.9434 (2.9397)  loss_scale: 16384.0000 (16456.2292)  weight_decay: 0.0500 (0.0500)  time: 0.7841  data: 0.1964  max mem: 13008
Epoch: [3]  [1370/3542]  eta: 0:26:53  lr: 0.000034  min_lr: 0.000034  loss: 2.8438 (2.9391)  loss_scale: 16384.0000 (16455.7024)  weight_decay: 0.0500 (0.0500)  time: 0.6871  data: 0.1017  max mem: 13008
[2023-05-16 08:30:47,456] [INFO] [logging.py:60:log_dist] [Rank 0] step=12000, skipped=12, lr=[3.361047704177419e-05, 3.361047704177419e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 08:30:47,459] [INFO] [timer.py:157:stop] 0/12000, SamplesPerSec=55.576358244885206
Epoch: [3]  [1380/3542]  eta: 0:26:45  lr: 0.000034  min_lr: 0.000034  loss: 2.8262 (2.9351)  loss_scale: 16384.0000 (16455.1832)  weight_decay: 0.0500 (0.0500)  time: 0.6968  data: 0.0923  max mem: 13008
Epoch: [3]  [1390/3542]  eta: 0:26:36  lr: 0.000034  min_lr: 0.000034  loss: 2.0039 (2.9279)  loss_scale: 16384.0000 (16454.6715)  weight_decay: 0.0500 (0.0500)  time: 0.6757  data: 0.0708  max mem: 13008
Epoch: [3]  [1400/3542]  eta: 0:26:28  lr: 0.000034  min_lr: 0.000034  loss: 1.9707 (2.9211)  loss_scale: 16384.0000 (16454.1670)  weight_decay: 0.0500 (0.0500)  time: 0.6754  data: 0.0886  max mem: 13008
Epoch: [3]  [1410/3542]  eta: 0:26:20  lr: 0.000034  min_lr: 0.000034  loss: 1.9775 (2.9148)  loss_scale: 16384.0000 (16453.6697)  weight_decay: 0.0500 (0.0500)  time: 0.7016  data: 0.1145  max mem: 13008
Epoch: [3]  [1420/3542]  eta: 0:26:11  lr: 0.000034  min_lr: 0.000034  loss: 2.0039 (2.9085)  loss_scale: 16384.0000 (16453.1795)  weight_decay: 0.0500 (0.0500)  time: 0.6843  data: 0.0980  max mem: 13008
Epoch: [3]  [1430/3542]  eta: 0:26:03  lr: 0.000034  min_lr: 0.000034  loss: 1.9521 (2.9023)  loss_scale: 16384.0000 (16452.6960)  weight_decay: 0.0500 (0.0500)  time: 0.6757  data: 0.0902  max mem: 13008
Epoch: [3]  [1440/3542]  eta: 0:25:55  lr: 0.000034  min_lr: 0.000034  loss: 1.9521 (2.8957)  loss_scale: 16384.0000 (16452.2193)  weight_decay: 0.0500 (0.0500)  time: 0.6927  data: 0.1072  max mem: 13008
Epoch: [3]  [1450/3542]  eta: 0:25:47  lr: 0.000034  min_lr: 0.000034  loss: 1.9307 (2.8891)  loss_scale: 16384.0000 (16451.7491)  weight_decay: 0.0500 (0.0500)  time: 0.6874  data: 0.1012  max mem: 13008
Epoch: [3]  [1460/3542]  eta: 0:25:38  lr: 0.000033  min_lr: 0.000033  loss: 1.9443 (2.8832)  loss_scale: 16384.0000 (16451.2854)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.0940  max mem: 13008
Epoch: [3]  [1470/3542]  eta: 0:25:30  lr: 0.000033  min_lr: 0.000033  loss: 2.0039 (2.8774)  loss_scale: 16384.0000 (16450.8280)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0805  max mem: 13008
Epoch: [3]  [1480/3542]  eta: 0:25:22  lr: 0.000033  min_lr: 0.000033  loss: 1.9844 (2.8711)  loss_scale: 16384.0000 (16450.3768)  weight_decay: 0.0500 (0.0500)  time: 0.6704  data: 0.0848  max mem: 13008
Epoch: [3]  [1490/3542]  eta: 0:25:14  lr: 0.000033  min_lr: 0.000033  loss: 1.9121 (2.8645)  loss_scale: 16384.0000 (16449.9316)  weight_decay: 0.0500 (0.0500)  time: 0.7071  data: 0.1216  max mem: 13008
Epoch: [3]  [1500/3542]  eta: 0:25:06  lr: 0.000033  min_lr: 0.000033  loss: 1.9170 (2.8583)  loss_scale: 16384.0000 (16449.4923)  weight_decay: 0.0500 (0.0500)  time: 0.6908  data: 0.1055  max mem: 13008
Epoch: [3]  [1510/3542]  eta: 0:24:57  lr: 0.000033  min_lr: 0.000033  loss: 1.9404 (2.8525)  loss_scale: 16384.0000 (16449.0589)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.0727  max mem: 13008
Epoch: [3]  [1520/3542]  eta: 0:24:50  lr: 0.000033  min_lr: 0.000033  loss: 1.9404 (2.8465)  loss_scale: 16384.0000 (16448.6312)  weight_decay: 0.0500 (0.0500)  time: 0.6878  data: 0.1015  max mem: 13008
Epoch: [3]  [1530/3542]  eta: 0:24:41  lr: 0.000033  min_lr: 0.000033  loss: 1.9150 (2.8404)  loss_scale: 16384.0000 (16448.2090)  weight_decay: 0.0500 (0.0500)  time: 0.6987  data: 0.1120  max mem: 13008
Epoch: [3]  [1540/3542]  eta: 0:24:34  lr: 0.000033  min_lr: 0.000033  loss: 1.9492 (2.8350)  loss_scale: 16384.0000 (16447.7923)  weight_decay: 0.0500 (0.0500)  time: 0.7033  data: 0.1157  max mem: 13008
Epoch: [3]  [1550/3542]  eta: 0:24:26  lr: 0.000033  min_lr: 0.000033  loss: 1.9531 (2.8295)  loss_scale: 16384.0000 (16447.3810)  weight_decay: 0.0500 (0.0500)  time: 0.6992  data: 0.1113  max mem: 13008
Epoch: [3]  [1560/3542]  eta: 0:24:18  lr: 0.000033  min_lr: 0.000033  loss: 1.9219 (2.8236)  loss_scale: 16384.0000 (16446.9750)  weight_decay: 0.0500 (0.0500)  time: 0.6754  data: 0.0893  max mem: 13008
Epoch: [3]  [1570/3542]  eta: 0:24:10  lr: 0.000033  min_lr: 0.000033  loss: 1.8955 (2.8179)  loss_scale: 16384.0000 (16446.5742)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.1052  max mem: 13008
Epoch: [3]  [1580/3542]  eta: 0:24:02  lr: 0.000033  min_lr: 0.000033  loss: 1.9707 (2.8129)  loss_scale: 16384.0000 (16446.1784)  weight_decay: 0.0500 (0.0500)  time: 0.7037  data: 0.1170  max mem: 13008
Epoch: [3]  [1590/3542]  eta: 0:23:55  lr: 0.000033  min_lr: 0.000033  loss: 2.0156 (2.8078)  loss_scale: 16384.0000 (16445.7876)  weight_decay: 0.0500 (0.0500)  time: 0.7319  data: 0.1453  max mem: 13008
Epoch: [3]  [1600/3542]  eta: 0:23:47  lr: 0.000033  min_lr: 0.000033  loss: 1.9668 (2.8029)  loss_scale: 16384.0000 (16445.4016)  weight_decay: 0.0500 (0.0500)  time: 0.7135  data: 0.1280  max mem: 13008
Epoch: [3]  [1610/3542]  eta: 0:23:39  lr: 0.000033  min_lr: 0.000033  loss: 1.9600 (2.7977)  loss_scale: 16384.0000 (16445.0205)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.0724  max mem: 13008
Epoch: [3]  [1620/3542]  eta: 0:23:31  lr: 0.000033  min_lr: 0.000033  loss: 1.9502 (2.7924)  loss_scale: 16384.0000 (16444.6440)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.1038  max mem: 13008
Epoch: [3]  [1630/3542]  eta: 0:23:23  lr: 0.000033  min_lr: 0.000033  loss: 1.9824 (2.7873)  loss_scale: 16384.0000 (16444.2722)  weight_decay: 0.0500 (0.0500)  time: 0.7112  data: 0.1259  max mem: 13008
Epoch: [3]  [1640/3542]  eta: 0:23:16  lr: 0.000033  min_lr: 0.000033  loss: 1.9688 (2.7823)  loss_scale: 16384.0000 (16443.9049)  weight_decay: 0.0500 (0.0500)  time: 0.6966  data: 0.1115  max mem: 13008
Epoch: [3]  [1650/3542]  eta: 0:23:08  lr: 0.000033  min_lr: 0.000033  loss: 1.9346 (2.7772)  loss_scale: 16384.0000 (16443.5421)  weight_decay: 0.0500 (0.0500)  time: 0.7252  data: 0.1389  max mem: 13008
Epoch: [3]  [1660/3542]  eta: 0:23:00  lr: 0.000033  min_lr: 0.000033  loss: 1.9287 (2.7723)  loss_scale: 16384.0000 (16443.1836)  weight_decay: 0.0500 (0.0500)  time: 0.7136  data: 0.1259  max mem: 13008
Epoch: [3]  [1670/3542]  eta: 0:22:52  lr: 0.000033  min_lr: 0.000033  loss: 1.9131 (2.7670)  loss_scale: 16384.0000 (16442.8294)  weight_decay: 0.0500 (0.0500)  time: 0.6768  data: 0.0888  max mem: 13008
Epoch: [3]  [1680/3542]  eta: 0:22:45  lr: 0.000033  min_lr: 0.000033  loss: 1.8770 (2.7618)  loss_scale: 16384.0000 (16442.4795)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.1045  max mem: 13008
Epoch: [3]  [1690/3542]  eta: 0:22:37  lr: 0.000033  min_lr: 0.000033  loss: 1.8789 (2.7568)  loss_scale: 16384.0000 (16442.1336)  weight_decay: 0.0500 (0.0500)  time: 0.7158  data: 0.1294  max mem: 13008
Epoch: [3]  [1700/3542]  eta: 0:22:30  lr: 0.000033  min_lr: 0.000033  loss: 1.9209 (2.7521)  loss_scale: 16384.0000 (16441.7919)  weight_decay: 0.0500 (0.0500)  time: 0.7109  data: 0.1251  max mem: 13008
Epoch: [3]  [1710/3542]  eta: 0:22:22  lr: 0.000033  min_lr: 0.000033  loss: 1.9258 (2.7473)  loss_scale: 16384.0000 (16441.4541)  weight_decay: 0.0500 (0.0500)  time: 0.6913  data: 0.1048  max mem: 13008
Epoch: [3]  [1720/3542]  eta: 0:22:14  lr: 0.000033  min_lr: 0.000033  loss: 1.9404 (2.7424)  loss_scale: 16384.0000 (16441.1203)  weight_decay: 0.0500 (0.0500)  time: 0.6877  data: 0.1006  max mem: 13008
Epoch: [3]  [1730/3542]  eta: 0:22:06  lr: 0.000033  min_lr: 0.000033  loss: 1.9600 (2.7382)  loss_scale: 16384.0000 (16440.7903)  weight_decay: 0.0500 (0.0500)  time: 0.6927  data: 0.1055  max mem: 13008
Epoch: [3]  [1740/3542]  eta: 0:21:59  lr: 0.000033  min_lr: 0.000033  loss: 1.9580 (2.7334)  loss_scale: 16384.0000 (16440.4641)  weight_decay: 0.0500 (0.0500)  time: 0.7317  data: 0.1445  max mem: 13008
Epoch: [3]  [1750/3542]  eta: 0:21:51  lr: 0.000033  min_lr: 0.000033  loss: 1.8789 (2.7289)  loss_scale: 16384.0000 (16440.1416)  weight_decay: 0.0500 (0.0500)  time: 0.7182  data: 0.1313  max mem: 13008
Epoch: [3]  [1760/3542]  eta: 0:21:44  lr: 0.000033  min_lr: 0.000033  loss: 1.8828 (2.7246)  loss_scale: 16384.0000 (16439.8228)  weight_decay: 0.0500 (0.0500)  time: 0.7098  data: 0.1226  max mem: 13008
[2023-05-16 08:35:19,796] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 08:35:19,796] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-05-16 08:35:20,971] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 12393
[2023-05-16 08:35:20,971] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-05-16 08:35:20,971] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [3]  [1770/3542]  eta: 0:21:37  lr: 0.000033  min_lr: 0.000033  loss: 1.9824 (2.7207)  loss_scale: 16384.0000 (16458.0102)  weight_decay: 0.0500 (0.0500)  time: 0.7184  data: 0.1314  max mem: 13008
Epoch: [3]  [1780/3542]  eta: 0:21:29  lr: 0.000033  min_lr: 0.000033  loss: 2.0332 (2.7168)  loss_scale: 16384.0000 (16457.5946)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.1021  max mem: 13008
Epoch: [3]  [1790/3542]  eta: 0:21:22  lr: 0.000033  min_lr: 0.000033  loss: 1.9668 (2.7128)  loss_scale: 16384.0000 (16457.1837)  weight_decay: 0.0500 (0.0500)  time: 0.7353  data: 0.1479  max mem: 13008
Epoch: [3]  [1800/3542]  eta: 0:21:14  lr: 0.000033  min_lr: 0.000033  loss: 1.9463 (2.7083)  loss_scale: 16384.0000 (16456.7773)  weight_decay: 0.0500 (0.0500)  time: 0.7317  data: 0.1462  max mem: 13008
Epoch: [3]  [1810/3542]  eta: 0:21:07  lr: 0.000033  min_lr: 0.000033  loss: 1.8867 (2.7040)  loss_scale: 16384.0000 (16456.3755)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.1060  max mem: 13008
Epoch: [3]  [1820/3542]  eta: 0:20:59  lr: 0.000033  min_lr: 0.000033  loss: 1.9238 (2.6998)  loss_scale: 16384.0000 (16455.9780)  weight_decay: 0.0500 (0.0500)  time: 0.7013  data: 0.1155  max mem: 13008
Epoch: [3]  [1830/3542]  eta: 0:20:51  lr: 0.000033  min_lr: 0.000033  loss: 1.9111 (2.6952)  loss_scale: 16384.0000 (16455.5849)  weight_decay: 0.0500 (0.0500)  time: 0.6891  data: 0.1026  max mem: 13008
Epoch: [3]  [1840/3542]  eta: 0:20:44  lr: 0.000033  min_lr: 0.000033  loss: 2.0020 (2.6917)  loss_scale: 16384.0000 (16455.1961)  weight_decay: 0.0500 (0.0500)  time: 0.7042  data: 0.1182  max mem: 13008
Epoch: [3]  [1850/3542]  eta: 0:20:36  lr: 0.000033  min_lr: 0.000033  loss: 2.0352 (2.6873)  loss_scale: 16384.0000 (16454.8115)  weight_decay: 0.0500 (0.0500)  time: 0.7157  data: 0.1297  max mem: 13008
Epoch: [3]  [1860/3542]  eta: 0:20:29  lr: 0.000033  min_lr: 0.000033  loss: 1.8770 (2.6831)  loss_scale: 16384.0000 (16454.4310)  weight_decay: 0.0500 (0.0500)  time: 0.6985  data: 0.1127  max mem: 13008
Epoch: [3]  [1870/3542]  eta: 0:20:21  lr: 0.000033  min_lr: 0.000033  loss: 1.8896 (2.6790)  loss_scale: 16384.0000 (16454.0545)  weight_decay: 0.0500 (0.0500)  time: 0.7001  data: 0.1144  max mem: 13008
Epoch: [3]  [1880/3542]  eta: 0:20:14  lr: 0.000033  min_lr: 0.000033  loss: 1.9365 (2.6753)  loss_scale: 16384.0000 (16453.6821)  weight_decay: 0.0500 (0.0500)  time: 0.7206  data: 0.1346  max mem: 13008
Epoch: [3]  [1890/3542]  eta: 0:20:06  lr: 0.000033  min_lr: 0.000033  loss: 2.0391 (2.6715)  loss_scale: 16384.0000 (16453.3136)  weight_decay: 0.0500 (0.0500)  time: 0.7194  data: 0.1336  max mem: 13008
Epoch: [3]  [1900/3542]  eta: 0:19:59  lr: 0.000033  min_lr: 0.000033  loss: 1.9502 (2.6677)  loss_scale: 16384.0000 (16452.9490)  weight_decay: 0.0500 (0.0500)  time: 0.6931  data: 0.1077  max mem: 13008
Epoch: [3]  [1910/3542]  eta: 0:19:50  lr: 0.000033  min_lr: 0.000033  loss: 1.9209 (2.6640)  loss_scale: 16384.0000 (16452.5882)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0805  max mem: 13008
[2023-05-16 08:37:03,850] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 12539
[2023-05-16 08:37:03,850] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 08:37:03,850] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [3]  [1920/3542]  eta: 0:19:43  lr: 0.000033  min_lr: 0.000033  loss: 1.9678 (2.6603)  loss_scale: 16384.0000 (16418.1156)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.0986  max mem: 13008
Epoch: [3]  [1930/3542]  eta: 0:19:36  lr: 0.000033  min_lr: 0.000033  loss: 1.9258 (2.6563)  loss_scale: 8192.0000 (16375.5153)  weight_decay: 0.0500 (0.0500)  time: 0.7125  data: 0.1260  max mem: 13008
Epoch: [3]  [1940/3542]  eta: 0:19:28  lr: 0.000033  min_lr: 0.000033  loss: 1.8682 (2.6521)  loss_scale: 8192.0000 (16333.3539)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.0968  max mem: 13008
Epoch: [3]  [1950/3542]  eta: 0:19:20  lr: 0.000033  min_lr: 0.000033  loss: 1.8701 (2.6484)  loss_scale: 8192.0000 (16291.6248)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.0868  max mem: 13008
Epoch: [3]  [1960/3542]  eta: 0:19:13  lr: 0.000033  min_lr: 0.000033  loss: 1.9229 (2.6449)  loss_scale: 8192.0000 (16250.3213)  weight_decay: 0.0500 (0.0500)  time: 0.6909  data: 0.1048  max mem: 13008
Epoch: [3]  [1970/3542]  eta: 0:19:05  lr: 0.000033  min_lr: 0.000033  loss: 1.9346 (2.6414)  loss_scale: 8192.0000 (16209.4368)  weight_decay: 0.0500 (0.0500)  time: 0.6893  data: 0.1029  max mem: 13008
Epoch: [3]  [1980/3542]  eta: 0:18:57  lr: 0.000033  min_lr: 0.000033  loss: 1.8682 (2.6373)  loss_scale: 8192.0000 (16168.9652)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0792  max mem: 13008
Epoch: [3]  [1990/3542]  eta: 0:18:49  lr: 0.000033  min_lr: 0.000033  loss: 1.8398 (2.6338)  loss_scale: 8192.0000 (16128.9001)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.0891  max mem: 13008
Epoch: [3]  [2000/3542]  eta: 0:18:42  lr: 0.000033  min_lr: 0.000033  loss: 1.8984 (2.6304)  loss_scale: 8192.0000 (16089.2354)  weight_decay: 0.0500 (0.0500)  time: 0.7052  data: 0.1192  max mem: 13008
Epoch: [3]  [2010/3542]  eta: 0:18:35  lr: 0.000033  min_lr: 0.000033  loss: 1.9453 (2.6273)  loss_scale: 8192.0000 (16049.9652)  weight_decay: 0.0500 (0.0500)  time: 0.7066  data: 0.1204  max mem: 13008
Epoch: [3]  [2020/3542]  eta: 0:18:27  lr: 0.000033  min_lr: 0.000033  loss: 2.0293 (2.6239)  loss_scale: 8192.0000 (16011.0836)  weight_decay: 0.0500 (0.0500)  time: 0.6838  data: 0.0982  max mem: 13008
Epoch: [3]  [2030/3542]  eta: 0:18:19  lr: 0.000033  min_lr: 0.000033  loss: 1.9590 (2.6206)  loss_scale: 8192.0000 (15972.5849)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.0842  max mem: 13008
Epoch: [3]  [2040/3542]  eta: 0:18:12  lr: 0.000033  min_lr: 0.000033  loss: 1.9502 (2.6172)  loss_scale: 8192.0000 (15934.4635)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.0900  max mem: 13008
Epoch: [3]  [2050/3542]  eta: 0:18:04  lr: 0.000033  min_lr: 0.000033  loss: 1.9502 (2.6138)  loss_scale: 8192.0000 (15896.7138)  weight_decay: 0.0500 (0.0500)  time: 0.7157  data: 0.1285  max mem: 13008
Epoch: [3]  [2060/3542]  eta: 0:17:57  lr: 0.000033  min_lr: 0.000033  loss: 1.9326 (2.6105)  loss_scale: 8192.0000 (15859.3304)  weight_decay: 0.0500 (0.0500)  time: 0.7126  data: 0.1258  max mem: 13008
Epoch: [3]  [2070/3542]  eta: 0:17:50  lr: 0.000033  min_lr: 0.000033  loss: 1.9297 (2.6076)  loss_scale: 8192.0000 (15822.3081)  weight_decay: 0.0500 (0.0500)  time: 0.7044  data: 0.1184  max mem: 13008
Epoch: [3]  [2080/3542]  eta: 0:17:41  lr: 0.000033  min_lr: 0.000033  loss: 1.8975 (2.6041)  loss_scale: 8192.0000 (15785.6415)  weight_decay: 0.0500 (0.0500)  time: 0.6703  data: 0.0847  max mem: 13008
Epoch: [3]  [2090/3542]  eta: 0:17:34  lr: 0.000033  min_lr: 0.000033  loss: 1.8428 (2.6003)  loss_scale: 8192.0000 (15749.3257)  weight_decay: 0.0500 (0.0500)  time: 0.6756  data: 0.0894  max mem: 13008
Epoch: [3]  [2100/3542]  eta: 0:17:27  lr: 0.000033  min_lr: 0.000033  loss: 1.8799 (2.5972)  loss_scale: 8192.0000 (15713.3555)  weight_decay: 0.0500 (0.0500)  time: 0.7448  data: 0.1580  max mem: 13008
Epoch: [3]  [2110/3542]  eta: 0:17:19  lr: 0.000033  min_lr: 0.000033  loss: 1.8799 (2.5937)  loss_scale: 8192.0000 (15677.7262)  weight_decay: 0.0500 (0.0500)  time: 0.6910  data: 0.1041  max mem: 13008
Epoch: [3]  [2120/3542]  eta: 0:17:12  lr: 0.000033  min_lr: 0.000033  loss: 1.8916 (2.5909)  loss_scale: 8192.0000 (15642.4328)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0714  max mem: 13008
Epoch: [3]  [2130/3542]  eta: 0:17:04  lr: 0.000032  min_lr: 0.000032  loss: 1.9629 (2.5877)  loss_scale: 8192.0000 (15607.4707)  weight_decay: 0.0500 (0.0500)  time: 0.6969  data: 0.1110  max mem: 13008
Epoch: [3]  [2140/3542]  eta: 0:16:57  lr: 0.000032  min_lr: 0.000032  loss: 1.8574 (2.5844)  loss_scale: 8192.0000 (15572.8351)  weight_decay: 0.0500 (0.0500)  time: 0.7262  data: 0.1396  max mem: 13008
Epoch: [3]  [2150/3542]  eta: 0:16:50  lr: 0.000032  min_lr: 0.000032  loss: 1.9268 (2.5817)  loss_scale: 8192.0000 (15538.5216)  weight_decay: 0.0500 (0.0500)  time: 0.7288  data: 0.1422  max mem: 13008
Epoch: [3]  [2160/3542]  eta: 0:16:42  lr: 0.000032  min_lr: 0.000032  loss: 1.9639 (2.5789)  loss_scale: 8192.0000 (15504.5257)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.1084  max mem: 13008
Epoch: [3]  [2170/3542]  eta: 0:16:35  lr: 0.000032  min_lr: 0.000032  loss: 1.9033 (2.5757)  loss_scale: 8192.0000 (15470.8429)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.0970  max mem: 13008
Epoch: [3]  [2180/3542]  eta: 0:16:27  lr: 0.000032  min_lr: 0.000032  loss: 1.9014 (2.5729)  loss_scale: 8192.0000 (15437.4691)  weight_decay: 0.0500 (0.0500)  time: 0.6884  data: 0.1018  max mem: 13008
Epoch: [3]  [2190/3542]  eta: 0:16:20  lr: 0.000032  min_lr: 0.000032  loss: 1.9570 (2.5700)  loss_scale: 8192.0000 (15404.3998)  weight_decay: 0.0500 (0.0500)  time: 0.6975  data: 0.1117  max mem: 13008
Epoch: [3]  [2200/3542]  eta: 0:16:13  lr: 0.000032  min_lr: 0.000032  loss: 1.8916 (2.5669)  loss_scale: 8192.0000 (15371.6311)  weight_decay: 0.0500 (0.0500)  time: 0.7058  data: 0.1206  max mem: 13008
Epoch: [3]  [2210/3542]  eta: 0:16:05  lr: 0.000032  min_lr: 0.000032  loss: 1.8730 (2.5643)  loss_scale: 8192.0000 (15339.1588)  weight_decay: 0.0500 (0.0500)  time: 0.6861  data: 0.1012  max mem: 13008
Epoch: [3]  [2220/3542]  eta: 0:15:58  lr: 0.000032  min_lr: 0.000032  loss: 2.0020 (2.5617)  loss_scale: 8192.0000 (15306.9788)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.1002  max mem: 13008
Epoch: [3]  [2230/3542]  eta: 0:15:50  lr: 0.000032  min_lr: 0.000032  loss: 2.0215 (2.5592)  loss_scale: 8192.0000 (15275.0874)  weight_decay: 0.0500 (0.0500)  time: 0.6972  data: 0.1101  max mem: 13008
Epoch: [3]  [2240/3542]  eta: 0:15:43  lr: 0.000032  min_lr: 0.000032  loss: 1.9932 (2.5565)  loss_scale: 8192.0000 (15243.4806)  weight_decay: 0.0500 (0.0500)  time: 0.6960  data: 0.1085  max mem: 13008
Epoch: [3]  [2250/3542]  eta: 0:15:36  lr: 0.000032  min_lr: 0.000032  loss: 1.9189 (2.5537)  loss_scale: 8192.0000 (15212.1546)  weight_decay: 0.0500 (0.0500)  time: 0.7091  data: 0.1218  max mem: 13008
Epoch: [3]  [2260/3542]  eta: 0:15:28  lr: 0.000032  min_lr: 0.000032  loss: 1.9375 (2.5509)  loss_scale: 8192.0000 (15181.1057)  weight_decay: 0.0500 (0.0500)  time: 0.6961  data: 0.1087  max mem: 13008
Epoch: [3]  [2270/3542]  eta: 0:15:20  lr: 0.000032  min_lr: 0.000032  loss: 1.9482 (2.5484)  loss_scale: 8192.0000 (15150.3303)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0857  max mem: 13008
Epoch: [3]  [2280/3542]  eta: 0:15:13  lr: 0.000032  min_lr: 0.000032  loss: 1.9482 (2.5458)  loss_scale: 8192.0000 (15119.8246)  weight_decay: 0.0500 (0.0500)  time: 0.6915  data: 0.1049  max mem: 13008
Epoch: [3]  [2290/3542]  eta: 0:15:06  lr: 0.000032  min_lr: 0.000032  loss: 1.9092 (2.5432)  loss_scale: 8192.0000 (15089.5853)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.0996  max mem: 13008
Epoch: [3]  [2300/3542]  eta: 0:14:58  lr: 0.000032  min_lr: 0.000032  loss: 1.8984 (2.5403)  loss_scale: 8192.0000 (15059.6089)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.0959  max mem: 13008
Epoch: [3]  [2310/3542]  eta: 0:14:51  lr: 0.000032  min_lr: 0.000032  loss: 1.9258 (2.5379)  loss_scale: 8192.0000 (15029.8918)  weight_decay: 0.0500 (0.0500)  time: 0.7042  data: 0.1177  max mem: 13008
Epoch: [3]  [2320/3542]  eta: 0:14:44  lr: 0.000032  min_lr: 0.000032  loss: 1.8633 (2.5347)  loss_scale: 8192.0000 (15000.4308)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.1210  max mem: 13008
Epoch: [3]  [2330/3542]  eta: 0:14:36  lr: 0.000032  min_lr: 0.000032  loss: 1.8281 (2.5320)  loss_scale: 8192.0000 (14971.2227)  weight_decay: 0.0500 (0.0500)  time: 0.6856  data: 0.0989  max mem: 13008
Epoch: [3]  [2340/3542]  eta: 0:14:29  lr: 0.000032  min_lr: 0.000032  loss: 1.9209 (2.5295)  loss_scale: 8192.0000 (14942.2640)  weight_decay: 0.0500 (0.0500)  time: 0.7056  data: 0.1192  max mem: 13008
Epoch: [3]  [2350/3542]  eta: 0:14:22  lr: 0.000032  min_lr: 0.000032  loss: 1.9531 (2.5271)  loss_scale: 8192.0000 (14913.5517)  weight_decay: 0.0500 (0.0500)  time: 0.7226  data: 0.1369  max mem: 13008
Epoch: [3]  [2360/3542]  eta: 0:14:14  lr: 0.000032  min_lr: 0.000032  loss: 1.9404 (2.5246)  loss_scale: 8192.0000 (14885.0826)  weight_decay: 0.0500 (0.0500)  time: 0.6975  data: 0.1120  max mem: 13008
Epoch: [3]  [2370/3542]  eta: 0:14:07  lr: 0.000032  min_lr: 0.000032  loss: 1.9697 (2.5224)  loss_scale: 8192.0000 (14856.8536)  weight_decay: 0.0500 (0.0500)  time: 0.7054  data: 0.1195  max mem: 13008
[2023-05-16 08:42:23,718] [INFO] [logging.py:60:log_dist] [Rank 0] step=13000, skipped=14, lr=[3.212660105302376e-05, 3.212660105302376e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 08:42:23,721] [INFO] [timer.py:157:stop] 0/13000, SamplesPerSec=55.56862371559532
Epoch: [3]  [2380/3542]  eta: 0:13:59  lr: 0.000032  min_lr: 0.000032  loss: 1.9756 (2.5202)  loss_scale: 8192.0000 (14828.8618)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.0949  max mem: 13008
Epoch: [3]  [2390/3542]  eta: 0:13:52  lr: 0.000032  min_lr: 0.000032  loss: 1.9531 (2.5179)  loss_scale: 8192.0000 (14801.1041)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.0935  max mem: 13008
Epoch: [3]  [2400/3542]  eta: 0:13:45  lr: 0.000032  min_lr: 0.000032  loss: 1.9824 (2.5155)  loss_scale: 8192.0000 (14773.5777)  weight_decay: 0.0500 (0.0500)  time: 0.6988  data: 0.1126  max mem: 13008
Epoch: [3]  [2410/3542]  eta: 0:13:37  lr: 0.000032  min_lr: 0.000032  loss: 1.8887 (2.5127)  loss_scale: 8192.0000 (14746.2796)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.0897  max mem: 13008
Epoch: [3]  [2420/3542]  eta: 0:13:30  lr: 0.000032  min_lr: 0.000032  loss: 1.8887 (2.5104)  loss_scale: 8192.0000 (14719.2069)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.0806  max mem: 13008
Epoch: [3]  [2430/3542]  eta: 0:13:22  lr: 0.000032  min_lr: 0.000032  loss: 1.9414 (2.5083)  loss_scale: 8192.0000 (14692.3571)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.0774  max mem: 13008
Epoch: [3]  [2440/3542]  eta: 0:13:15  lr: 0.000032  min_lr: 0.000032  loss: 1.8838 (2.5056)  loss_scale: 8192.0000 (14665.7272)  weight_decay: 0.0500 (0.0500)  time: 0.6758  data: 0.0882  max mem: 13008
Epoch: [3]  [2450/3542]  eta: 0:13:08  lr: 0.000032  min_lr: 0.000032  loss: 1.8604 (2.5032)  loss_scale: 8192.0000 (14639.3146)  weight_decay: 0.0500 (0.0500)  time: 0.6946  data: 0.1070  max mem: 13008
Epoch: [3]  [2460/3542]  eta: 0:13:00  lr: 0.000032  min_lr: 0.000032  loss: 1.8936 (2.5008)  loss_scale: 8192.0000 (14613.1166)  weight_decay: 0.0500 (0.0500)  time: 0.6931  data: 0.1062  max mem: 13008
Epoch: [3]  [2470/3542]  eta: 0:12:53  lr: 0.000032  min_lr: 0.000032  loss: 1.8916 (2.4986)  loss_scale: 8192.0000 (14587.1307)  weight_decay: 0.0500 (0.0500)  time: 0.6879  data: 0.1015  max mem: 13008
Epoch: [3]  [2480/3542]  eta: 0:12:45  lr: 0.000032  min_lr: 0.000032  loss: 1.8916 (2.4964)  loss_scale: 8192.0000 (14561.3543)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.0965  max mem: 13008
Epoch: [3]  [2490/3542]  eta: 0:12:38  lr: 0.000032  min_lr: 0.000032  loss: 1.9688 (2.4942)  loss_scale: 8192.0000 (14535.7848)  weight_decay: 0.0500 (0.0500)  time: 0.6818  data: 0.0963  max mem: 13008
Epoch: [3]  [2500/3542]  eta: 0:12:31  lr: 0.000032  min_lr: 0.000032  loss: 1.9600 (2.4919)  loss_scale: 8192.0000 (14510.4198)  weight_decay: 0.0500 (0.0500)  time: 0.6878  data: 0.1016  max mem: 13008
Epoch: [3]  [2510/3542]  eta: 0:12:23  lr: 0.000032  min_lr: 0.000032  loss: 1.8350 (2.4891)  loss_scale: 8192.0000 (14485.2569)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.1077  max mem: 13008
Epoch: [3]  [2520/3542]  eta: 0:12:16  lr: 0.000032  min_lr: 0.000032  loss: 1.9131 (2.4872)  loss_scale: 8192.0000 (14460.2935)  weight_decay: 0.0500 (0.0500)  time: 0.6878  data: 0.1015  max mem: 13008
Epoch: [3]  [2530/3542]  eta: 0:12:08  lr: 0.000032  min_lr: 0.000032  loss: 2.0098 (2.4853)  loss_scale: 8192.0000 (14435.5275)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.0598  max mem: 13008
Epoch: [3]  [2540/3542]  eta: 0:12:01  lr: 0.000032  min_lr: 0.000032  loss: 1.9629 (2.4831)  loss_scale: 8192.0000 (14410.9563)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.0726  max mem: 13008
Epoch: [3]  [2550/3542]  eta: 0:11:54  lr: 0.000032  min_lr: 0.000032  loss: 1.8340 (2.4807)  loss_scale: 8192.0000 (14386.5778)  weight_decay: 0.0500 (0.0500)  time: 0.6868  data: 0.0998  max mem: 13008
Epoch: [3]  [2560/3542]  eta: 0:11:46  lr: 0.000032  min_lr: 0.000032  loss: 1.8584 (2.4784)  loss_scale: 8192.0000 (14362.3897)  weight_decay: 0.0500 (0.0500)  time: 0.6805  data: 0.0935  max mem: 13008
Epoch: [3]  [2570/3542]  eta: 0:11:39  lr: 0.000032  min_lr: 0.000032  loss: 1.9277 (2.4765)  loss_scale: 8192.0000 (14338.3897)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.0873  max mem: 13008
Epoch: [3]  [2580/3542]  eta: 0:11:31  lr: 0.000032  min_lr: 0.000032  loss: 1.9316 (2.4742)  loss_scale: 8192.0000 (14314.5757)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.0612  max mem: 13008
Epoch: [3]  [2590/3542]  eta: 0:11:24  lr: 0.000032  min_lr: 0.000032  loss: 1.8867 (2.4724)  loss_scale: 8192.0000 (14290.9456)  weight_decay: 0.0500 (0.0500)  time: 0.6646  data: 0.0794  max mem: 13008
Epoch: [3]  [2600/3542]  eta: 0:11:17  lr: 0.000032  min_lr: 0.000032  loss: 1.9688 (2.4703)  loss_scale: 8192.0000 (14267.4971)  weight_decay: 0.0500 (0.0500)  time: 0.6756  data: 0.0902  max mem: 13008
Epoch: [3]  [2610/3542]  eta: 0:11:09  lr: 0.000032  min_lr: 0.000032  loss: 1.9326 (2.4683)  loss_scale: 8192.0000 (14244.2283)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0758  max mem: 13008
Epoch: [3]  [2620/3542]  eta: 0:11:02  lr: 0.000032  min_lr: 0.000032  loss: 1.9062 (2.4664)  loss_scale: 8192.0000 (14221.1370)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.0710  max mem: 13008
Epoch: [3]  [2630/3542]  eta: 0:10:55  lr: 0.000032  min_lr: 0.000032  loss: 1.9170 (2.4643)  loss_scale: 8192.0000 (14198.2212)  weight_decay: 0.0500 (0.0500)  time: 0.6572  data: 0.0709  max mem: 13008
Epoch: [3]  [2640/3542]  eta: 0:10:47  lr: 0.000032  min_lr: 0.000032  loss: 1.9590 (2.4625)  loss_scale: 8192.0000 (14175.4790)  weight_decay: 0.0500 (0.0500)  time: 0.6747  data: 0.0882  max mem: 13008
Epoch: [3]  [2650/3542]  eta: 0:10:40  lr: 0.000032  min_lr: 0.000032  loss: 1.9277 (2.4602)  loss_scale: 8192.0000 (14152.9083)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.0769  max mem: 13008
Epoch: [3]  [2660/3542]  eta: 0:10:32  lr: 0.000032  min_lr: 0.000032  loss: 1.9053 (2.4585)  loss_scale: 8192.0000 (14130.5073)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.0643  max mem: 13008
Epoch: [3]  [2670/3542]  eta: 0:10:25  lr: 0.000032  min_lr: 0.000032  loss: 1.9268 (2.4565)  loss_scale: 8192.0000 (14108.2741)  weight_decay: 0.0500 (0.0500)  time: 0.6660  data: 0.0807  max mem: 13008
Epoch: [3]  [2680/3542]  eta: 0:10:18  lr: 0.000032  min_lr: 0.000032  loss: 1.9414 (2.4545)  loss_scale: 8192.0000 (14086.2066)  weight_decay: 0.0500 (0.0500)  time: 0.6852  data: 0.1003  max mem: 13008
Epoch: [3]  [2690/3542]  eta: 0:10:10  lr: 0.000032  min_lr: 0.000032  loss: 1.9150 (2.4524)  loss_scale: 8192.0000 (14064.3032)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.0776  max mem: 13008
Epoch: [3]  [2700/3542]  eta: 0:10:03  lr: 0.000032  min_lr: 0.000032  loss: 1.8604 (2.4505)  loss_scale: 8192.0000 (14042.5620)  weight_decay: 0.0500 (0.0500)  time: 0.6651  data: 0.0789  max mem: 13008
Epoch: [3]  [2710/3542]  eta: 0:09:56  lr: 0.000032  min_lr: 0.000032  loss: 1.9404 (2.4484)  loss_scale: 8192.0000 (14020.9812)  weight_decay: 0.0500 (0.0500)  time: 0.6960  data: 0.1097  max mem: 13008
Epoch: [3]  [2720/3542]  eta: 0:09:49  lr: 0.000032  min_lr: 0.000032  loss: 1.9453 (2.4467)  loss_scale: 8192.0000 (13999.5590)  weight_decay: 0.0500 (0.0500)  time: 0.6902  data: 0.1038  max mem: 13008
Epoch: [3]  [2730/3542]  eta: 0:09:42  lr: 0.000032  min_lr: 0.000032  loss: 1.9316 (2.4447)  loss_scale: 8192.0000 (13978.2937)  weight_decay: 0.0500 (0.0500)  time: 0.6953  data: 0.1089  max mem: 13008
Epoch: [3]  [2740/3542]  eta: 0:09:34  lr: 0.000032  min_lr: 0.000032  loss: 1.9619 (2.4431)  loss_scale: 8192.0000 (13957.1835)  weight_decay: 0.0500 (0.0500)  time: 0.6866  data: 0.1003  max mem: 13008
Epoch: [3]  [2750/3542]  eta: 0:09:27  lr: 0.000032  min_lr: 0.000032  loss: 1.9463 (2.4412)  loss_scale: 8192.0000 (13936.2268)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0754  max mem: 13008
Epoch: [3]  [2760/3542]  eta: 0:09:20  lr: 0.000032  min_lr: 0.000032  loss: 1.8467 (2.4392)  loss_scale: 8192.0000 (13915.4219)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.0998  max mem: 13008
Epoch: [3]  [2770/3542]  eta: 0:09:12  lr: 0.000032  min_lr: 0.000032  loss: 1.8828 (2.4373)  loss_scale: 8192.0000 (13894.7672)  weight_decay: 0.0500 (0.0500)  time: 0.6925  data: 0.1067  max mem: 13008
Epoch: [3]  [2780/3542]  eta: 0:09:05  lr: 0.000031  min_lr: 0.000031  loss: 1.8955 (2.4356)  loss_scale: 8192.0000 (13874.2611)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0741  max mem: 13008
Epoch: [3]  [2790/3542]  eta: 0:08:58  lr: 0.000031  min_lr: 0.000031  loss: 1.9473 (2.4338)  loss_scale: 8192.0000 (13853.9018)  weight_decay: 0.0500 (0.0500)  time: 0.6732  data: 0.0877  max mem: 13008
Epoch: [3]  [2800/3542]  eta: 0:08:51  lr: 0.000031  min_lr: 0.000031  loss: 1.9326 (2.4319)  loss_scale: 8192.0000 (13833.6880)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.1004  max mem: 13008
Epoch: [3]  [2810/3542]  eta: 0:08:43  lr: 0.000031  min_lr: 0.000031  loss: 1.8867 (2.4302)  loss_scale: 8192.0000 (13813.6179)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.1050  max mem: 13008
Epoch: [3]  [2820/3542]  eta: 0:08:36  lr: 0.000031  min_lr: 0.000031  loss: 1.9023 (2.4285)  loss_scale: 8192.0000 (13793.6902)  weight_decay: 0.0500 (0.0500)  time: 0.6922  data: 0.1052  max mem: 13008
Epoch: [3]  [2830/3542]  eta: 0:08:29  lr: 0.000031  min_lr: 0.000031  loss: 1.8994 (2.4267)  loss_scale: 8192.0000 (13773.9032)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.0866  max mem: 13008
Epoch: [3]  [2840/3542]  eta: 0:08:22  lr: 0.000031  min_lr: 0.000031  loss: 1.8994 (2.4249)  loss_scale: 8192.0000 (13754.2555)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.0820  max mem: 13008
Epoch: [3]  [2850/3542]  eta: 0:08:14  lr: 0.000031  min_lr: 0.000031  loss: 1.8994 (2.4232)  loss_scale: 8192.0000 (13734.7457)  weight_decay: 0.0500 (0.0500)  time: 0.6911  data: 0.1050  max mem: 13008
Epoch: [3]  [2860/3542]  eta: 0:08:07  lr: 0.000031  min_lr: 0.000031  loss: 1.8818 (2.4214)  loss_scale: 8192.0000 (13715.3722)  weight_decay: 0.0500 (0.0500)  time: 0.7159  data: 0.1300  max mem: 13008
Epoch: [3]  [2870/3542]  eta: 0:08:00  lr: 0.000031  min_lr: 0.000031  loss: 1.9102 (2.4198)  loss_scale: 8192.0000 (13696.1338)  weight_decay: 0.0500 (0.0500)  time: 0.6956  data: 0.1091  max mem: 13008
Epoch: [3]  [2880/3542]  eta: 0:07:53  lr: 0.000031  min_lr: 0.000031  loss: 1.9287 (2.4182)  loss_scale: 8192.0000 (13677.0288)  weight_decay: 0.0500 (0.0500)  time: 0.6578  data: 0.0723  max mem: 13008
Epoch: [3]  [2890/3542]  eta: 0:07:45  lr: 0.000031  min_lr: 0.000031  loss: 1.9043 (2.4165)  loss_scale: 8192.0000 (13658.0560)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0746  max mem: 13008
Epoch: [3]  [2900/3542]  eta: 0:07:38  lr: 0.000031  min_lr: 0.000031  loss: 1.8779 (2.4149)  loss_scale: 8192.0000 (13639.2141)  weight_decay: 0.0500 (0.0500)  time: 0.6732  data: 0.0868  max mem: 13008
Epoch: [3]  [2910/3542]  eta: 0:07:31  lr: 0.000031  min_lr: 0.000031  loss: 1.9707 (2.4134)  loss_scale: 8192.0000 (13620.5015)  weight_decay: 0.0500 (0.0500)  time: 0.6674  data: 0.0810  max mem: 13008
[2023-05-16 08:48:30,400] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 08:48:30,400] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [3]  [2920/3542]  eta: 0:07:24  lr: 0.000031  min_lr: 0.000031  loss: 1.9102 (2.4115)  loss_scale: 8192.0000 (13621.5488)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0787  max mem: 13008
Epoch: [3]  [2930/3542]  eta: 0:07:17  lr: 0.000031  min_lr: 0.000031  loss: 1.7939 (2.4097)  loss_scale: 16384.0000 (13630.9737)  weight_decay: 0.0500 (0.0500)  time: 0.6971  data: 0.1098  max mem: 13008
Epoch: [3]  [2940/3542]  eta: 0:07:09  lr: 0.000031  min_lr: 0.000031  loss: 1.8760 (2.4080)  loss_scale: 16384.0000 (13640.3346)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.0930  max mem: 13008
Epoch: [3]  [2950/3542]  eta: 0:07:02  lr: 0.000031  min_lr: 0.000031  loss: 1.9248 (2.4066)  loss_scale: 16384.0000 (13649.6320)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.0582  max mem: 13008
Epoch: [3]  [2960/3542]  eta: 0:06:55  lr: 0.000031  min_lr: 0.000031  loss: 2.0527 (2.4053)  loss_scale: 16384.0000 (13658.8666)  weight_decay: 0.0500 (0.0500)  time: 0.7029  data: 0.1149  max mem: 13008
[2023-05-16 08:49:02,797] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 13588
[2023-05-16 08:49:02,797] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 08:49:02,797] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [3]  [2970/3542]  eta: 0:06:48  lr: 0.000031  min_lr: 0.000031  loss: 1.9570 (2.4035)  loss_scale: 16384.0000 (13643.2232)  weight_decay: 0.0500 (0.0500)  time: 0.7227  data: 0.1358  max mem: 13008
Epoch: [3]  [2980/3542]  eta: 0:06:41  lr: 0.000031  min_lr: 0.000031  loss: 1.8691 (2.4019)  loss_scale: 8192.0000 (13624.9366)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.1070  max mem: 13008
Epoch: [3]  [2990/3542]  eta: 0:06:33  lr: 0.000031  min_lr: 0.000031  loss: 1.9746 (2.4005)  loss_scale: 8192.0000 (13606.7723)  weight_decay: 0.0500 (0.0500)  time: 0.6831  data: 0.0963  max mem: 13008
Epoch: [3]  [3000/3542]  eta: 0:06:26  lr: 0.000031  min_lr: 0.000031  loss: 1.9668 (2.3989)  loss_scale: 8192.0000 (13588.7291)  weight_decay: 0.0500 (0.0500)  time: 0.6779  data: 0.0917  max mem: 13008
Epoch: [3]  [3010/3542]  eta: 0:06:19  lr: 0.000031  min_lr: 0.000031  loss: 1.9668 (2.3975)  loss_scale: 8192.0000 (13570.8057)  weight_decay: 0.0500 (0.0500)  time: 0.6987  data: 0.1125  max mem: 13008
Epoch: [3]  [3020/3542]  eta: 0:06:12  lr: 0.000031  min_lr: 0.000031  loss: 1.9678 (2.3959)  loss_scale: 8192.0000 (13553.0010)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.1032  max mem: 13008
Epoch: [3]  [3030/3542]  eta: 0:06:05  lr: 0.000031  min_lr: 0.000031  loss: 1.9414 (2.3946)  loss_scale: 8192.0000 (13535.3138)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.0733  max mem: 13008
Epoch: [3]  [3040/3542]  eta: 0:05:57  lr: 0.000031  min_lr: 0.000031  loss: 1.9561 (2.3933)  loss_scale: 8192.0000 (13517.7428)  weight_decay: 0.0500 (0.0500)  time: 0.6712  data: 0.0850  max mem: 13008
Epoch: [3]  [3050/3542]  eta: 0:05:50  lr: 0.000031  min_lr: 0.000031  loss: 1.9229 (2.3917)  loss_scale: 8192.0000 (13500.2871)  weight_decay: 0.0500 (0.0500)  time: 0.6852  data: 0.0990  max mem: 13008
Epoch: [3]  [3060/3542]  eta: 0:05:43  lr: 0.000031  min_lr: 0.000031  loss: 1.9141 (2.3901)  loss_scale: 8192.0000 (13482.9454)  weight_decay: 0.0500 (0.0500)  time: 0.6939  data: 0.1070  max mem: 13008
Epoch: [3]  [3070/3542]  eta: 0:05:36  lr: 0.000031  min_lr: 0.000031  loss: 1.8750 (2.3884)  loss_scale: 8192.0000 (13465.7167)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.0986  max mem: 13008
Epoch: [3]  [3080/3542]  eta: 0:05:29  lr: 0.000031  min_lr: 0.000031  loss: 1.8682 (2.3867)  loss_scale: 8192.0000 (13448.5998)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.0583  max mem: 13008
Epoch: [3]  [3090/3542]  eta: 0:05:22  lr: 0.000031  min_lr: 0.000031  loss: 1.8965 (2.3852)  loss_scale: 8192.0000 (13431.5937)  weight_decay: 0.0500 (0.0500)  time: 0.6691  data: 0.0831  max mem: 13008
Epoch: [3]  [3100/3542]  eta: 0:05:14  lr: 0.000031  min_lr: 0.000031  loss: 1.8936 (2.3835)  loss_scale: 8192.0000 (13414.6972)  weight_decay: 0.0500 (0.0500)  time: 0.6956  data: 0.1093  max mem: 13008
Epoch: [3]  [3110/3542]  eta: 0:05:07  lr: 0.000031  min_lr: 0.000031  loss: 1.8740 (2.3821)  loss_scale: 8192.0000 (13397.9094)  weight_decay: 0.0500 (0.0500)  time: 0.6741  data: 0.0874  max mem: 13008
Epoch: [3]  [3120/3542]  eta: 0:05:00  lr: 0.000031  min_lr: 0.000031  loss: 1.9502 (2.3808)  loss_scale: 8192.0000 (13381.2291)  weight_decay: 0.0500 (0.0500)  time: 0.6815  data: 0.0942  max mem: 13008
Epoch: [3]  [3130/3542]  eta: 0:04:53  lr: 0.000031  min_lr: 0.000031  loss: 1.8848 (2.3792)  loss_scale: 8192.0000 (13364.6554)  weight_decay: 0.0500 (0.0500)  time: 0.6869  data: 0.1001  max mem: 13008
Epoch: [3]  [3140/3542]  eta: 0:04:46  lr: 0.000031  min_lr: 0.000031  loss: 1.8799 (2.3776)  loss_scale: 8192.0000 (13348.1872)  weight_decay: 0.0500 (0.0500)  time: 0.6811  data: 0.0949  max mem: 13008
Epoch: [3]  [3150/3542]  eta: 0:04:39  lr: 0.000031  min_lr: 0.000031  loss: 1.8096 (2.3759)  loss_scale: 8192.0000 (13331.8235)  weight_decay: 0.0500 (0.0500)  time: 0.6724  data: 0.0858  max mem: 13008
Epoch: [3]  [3160/3542]  eta: 0:04:31  lr: 0.000031  min_lr: 0.000031  loss: 1.8418 (2.3747)  loss_scale: 8192.0000 (13315.5634)  weight_decay: 0.0500 (0.0500)  time: 0.6816  data: 0.0948  max mem: 13008
Epoch: [3]  [3170/3542]  eta: 0:04:24  lr: 0.000031  min_lr: 0.000031  loss: 1.9893 (2.3735)  loss_scale: 8192.0000 (13299.4059)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.0917  max mem: 13008
Epoch: [3]  [3180/3542]  eta: 0:04:17  lr: 0.000031  min_lr: 0.000031  loss: 1.9707 (2.3723)  loss_scale: 8192.0000 (13283.3499)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0744  max mem: 13008
Epoch: [3]  [3190/3542]  eta: 0:04:10  lr: 0.000031  min_lr: 0.000031  loss: 1.9111 (2.3709)  loss_scale: 8192.0000 (13267.3945)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.0767  max mem: 13008
Epoch: [3]  [3200/3542]  eta: 0:04:03  lr: 0.000031  min_lr: 0.000031  loss: 1.9121 (2.3697)  loss_scale: 8192.0000 (13251.5389)  weight_decay: 0.0500 (0.0500)  time: 0.6763  data: 0.0901  max mem: 13008
Epoch: [3]  [3210/3542]  eta: 0:03:56  lr: 0.000031  min_lr: 0.000031  loss: 1.9277 (2.3682)  loss_scale: 8192.0000 (13235.7820)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.1056  max mem: 13008
Epoch: [3]  [3220/3542]  eta: 0:03:48  lr: 0.000031  min_lr: 0.000031  loss: 1.9277 (2.3669)  loss_scale: 8192.0000 (13220.1229)  weight_decay: 0.0500 (0.0500)  time: 0.6872  data: 0.1010  max mem: 13008
Epoch: [3]  [3230/3542]  eta: 0:03:41  lr: 0.000031  min_lr: 0.000031  loss: 1.8984 (2.3654)  loss_scale: 8192.0000 (13204.5608)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.1048  max mem: 13008
Epoch: [3]  [3240/3542]  eta: 0:03:34  lr: 0.000031  min_lr: 0.000031  loss: 1.8691 (2.3640)  loss_scale: 8192.0000 (13189.0947)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.0961  max mem: 13008
Epoch: [3]  [3250/3542]  eta: 0:03:27  lr: 0.000031  min_lr: 0.000031  loss: 1.8428 (2.3624)  loss_scale: 8192.0000 (13173.7238)  weight_decay: 0.0500 (0.0500)  time: 0.6722  data: 0.0850  max mem: 13008
Epoch: [3]  [3260/3542]  eta: 0:03:20  lr: 0.000031  min_lr: 0.000031  loss: 1.8271 (2.3609)  loss_scale: 8192.0000 (13158.4471)  weight_decay: 0.0500 (0.0500)  time: 0.6806  data: 0.0936  max mem: 13008
Epoch: [3]  [3270/3542]  eta: 0:03:13  lr: 0.000031  min_lr: 0.000031  loss: 1.9258 (2.3598)  loss_scale: 8192.0000 (13143.2638)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.1039  max mem: 13008
Epoch: [3]  [3280/3542]  eta: 0:03:06  lr: 0.000031  min_lr: 0.000031  loss: 1.8779 (2.3582)  loss_scale: 8192.0000 (13128.1731)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.0811  max mem: 13008
Epoch: [3]  [3290/3542]  eta: 0:02:58  lr: 0.000031  min_lr: 0.000031  loss: 1.8701 (2.3568)  loss_scale: 8192.0000 (13113.1741)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.0461  max mem: 13008
Epoch: [3]  [3300/3542]  eta: 0:02:51  lr: 0.000031  min_lr: 0.000031  loss: 1.8555 (2.3553)  loss_scale: 8192.0000 (13098.2660)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.0541  max mem: 13008
Epoch: [3]  [3310/3542]  eta: 0:02:44  lr: 0.000031  min_lr: 0.000031  loss: 1.8555 (2.3540)  loss_scale: 8192.0000 (13083.4479)  weight_decay: 0.0500 (0.0500)  time: 0.6824  data: 0.0956  max mem: 13008
Epoch: [3]  [3320/3542]  eta: 0:02:37  lr: 0.000031  min_lr: 0.000031  loss: 1.9307 (2.3530)  loss_scale: 8192.0000 (13068.7191)  weight_decay: 0.0500 (0.0500)  time: 0.6988  data: 0.1122  max mem: 13008
Epoch: [3]  [3330/3542]  eta: 0:02:30  lr: 0.000031  min_lr: 0.000031  loss: 1.9014 (2.3515)  loss_scale: 8192.0000 (13054.0787)  weight_decay: 0.0500 (0.0500)  time: 0.6850  data: 0.0990  max mem: 13008
Epoch: [3]  [3340/3542]  eta: 0:02:23  lr: 0.000031  min_lr: 0.000031  loss: 1.9014 (2.3504)  loss_scale: 8192.0000 (13039.5259)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.1066  max mem: 13008
Epoch: [3]  [3350/3542]  eta: 0:02:16  lr: 0.000031  min_lr: 0.000031  loss: 1.9434 (2.3491)  loss_scale: 8192.0000 (13025.0600)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.0902  max mem: 13008
Epoch: [3]  [3360/3542]  eta: 0:02:09  lr: 0.000031  min_lr: 0.000031  loss: 1.8828 (2.3476)  loss_scale: 8192.0000 (13010.6802)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.0697  max mem: 13008
Epoch: [3]  [3370/3542]  eta: 0:02:02  lr: 0.000031  min_lr: 0.000031  loss: 1.9561 (2.3464)  loss_scale: 8192.0000 (12996.3856)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.0822  max mem: 13008
[2023-05-16 08:53:41,314] [INFO] [logging.py:60:log_dist] [Rank 0] step=14000, skipped=15, lr=[3.05298965527607e-05, 3.05298965527607e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 08:53:41,317] [INFO] [timer.py:157:stop] 0/14000, SamplesPerSec=55.563465217612894
Epoch: [3]  [3380/3542]  eta: 0:01:54  lr: 0.000031  min_lr: 0.000031  loss: 1.8887 (2.3450)  loss_scale: 8192.0000 (12982.1757)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.0962  max mem: 13008
Epoch: [3]  [3390/3542]  eta: 0:01:47  lr: 0.000031  min_lr: 0.000031  loss: 1.8613 (2.3435)  loss_scale: 8192.0000 (12968.0495)  weight_decay: 0.0500 (0.0500)  time: 0.6665  data: 0.0802  max mem: 13008
Epoch: [3]  [3400/3542]  eta: 0:01:40  lr: 0.000030  min_lr: 0.000030  loss: 1.7969 (2.3421)  loss_scale: 8192.0000 (12954.0065)  weight_decay: 0.0500 (0.0500)  time: 0.6461  data: 0.0597  max mem: 13008
Epoch: [3]  [3410/3542]  eta: 0:01:33  lr: 0.000030  min_lr: 0.000030  loss: 1.8662 (2.3408)  loss_scale: 8192.0000 (12940.0457)  weight_decay: 0.0500 (0.0500)  time: 0.7074  data: 0.0763  max mem: 13008
Epoch: [3]  [3420/3542]  eta: 0:01:26  lr: 0.000030  min_lr: 0.000030  loss: 1.9893 (2.3399)  loss_scale: 8192.0000 (12926.1666)  weight_decay: 0.0500 (0.0500)  time: 0.6941  data: 0.0636  max mem: 13008
Epoch: [3]  [3430/3542]  eta: 0:01:19  lr: 0.000030  min_lr: 0.000030  loss: 1.9766 (2.3386)  loss_scale: 8192.0000 (12912.3684)  weight_decay: 0.0500 (0.0500)  time: 0.6470  data: 0.0614  max mem: 13008
Epoch: [3]  [3440/3542]  eta: 0:01:12  lr: 0.000030  min_lr: 0.000030  loss: 1.8584 (2.3371)  loss_scale: 8192.0000 (12898.6504)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.0996  max mem: 13008
Epoch: [3]  [3450/3542]  eta: 0:01:05  lr: 0.000030  min_lr: 0.000030  loss: 1.8652 (2.3360)  loss_scale: 8192.0000 (12885.0119)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.1025  max mem: 13008
Epoch: [3]  [3460/3542]  eta: 0:00:58  lr: 0.000030  min_lr: 0.000030  loss: 1.8857 (2.3350)  loss_scale: 8192.0000 (12871.4522)  weight_decay: 0.0500 (0.0500)  time: 0.6874  data: 0.1003  max mem: 13008
Epoch: [3]  [3470/3542]  eta: 0:00:51  lr: 0.000030  min_lr: 0.000030  loss: 2.0000 (2.3340)  loss_scale: 8192.0000 (12857.9706)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.0931  max mem: 13008
Epoch: [3]  [3480/3542]  eta: 0:00:43  lr: 0.000030  min_lr: 0.000030  loss: 1.9043 (2.3326)  loss_scale: 8192.0000 (12844.5665)  weight_decay: 0.0500 (0.0500)  time: 0.6952  data: 0.1084  max mem: 13008
Epoch: [3]  [3490/3542]  eta: 0:00:36  lr: 0.000030  min_lr: 0.000030  loss: 1.8418 (2.3312)  loss_scale: 8192.0000 (12831.2392)  weight_decay: 0.0500 (0.0500)  time: 0.6931  data: 0.1068  max mem: 13008
Epoch: [3]  [3500/3542]  eta: 0:00:29  lr: 0.000030  min_lr: 0.000030  loss: 1.9131 (2.3301)  loss_scale: 8192.0000 (12817.9880)  weight_decay: 0.0500 (0.0500)  time: 0.6745  data: 0.0870  max mem: 13008
Epoch: [3]  [3510/3542]  eta: 0:00:22  lr: 0.000030  min_lr: 0.000030  loss: 1.9355 (2.3289)  loss_scale: 8192.0000 (12804.8123)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.0626  max mem: 13008
Epoch: [3]  [3520/3542]  eta: 0:00:15  lr: 0.000030  min_lr: 0.000030  loss: 1.9482 (2.3278)  loss_scale: 8192.0000 (12791.7114)  weight_decay: 0.0500 (0.0500)  time: 0.6522  data: 0.0665  max mem: 13008
Epoch: [3]  [3530/3542]  eta: 0:00:08  lr: 0.000030  min_lr: 0.000030  loss: 1.9482 (2.3268)  loss_scale: 8192.0000 (12778.6848)  weight_decay: 0.0500 (0.0500)  time: 0.6554  data: 0.0693  max mem: 13008
Epoch: [3]  [3540/3542]  eta: 0:00:01  lr: 0.000030  min_lr: 0.000030  loss: 1.9541 (2.3260)  loss_scale: 8192.0000 (12765.7317)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0184  max mem: 13008
Epoch: [3]  [3541/3542]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000030  loss: 1.9541 (2.3258)  loss_scale: 8192.0000 (12764.4404)  weight_decay: 0.0500 (0.0500)  time: 0.6042  data: 0.0184  max mem: 13008
Epoch: [3] Total time: 0:41:47 (0.7079 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000030  loss: 1.9541 (2.3258)  loss_scale: 8192.0000 (12764.4404)  weight_decay: 0.0500 (0.0500)
[2023-05-16 08:55:34,442] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-3/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:35:23    time: 13.6115  data: 9.7845  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:11:11    time: 4.5999  data: 0.8897  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 20/156]  eta: 0:09:28    time: 3.7062  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 30/156]  eta: 0:08:29    time: 3.7391  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 40/156]  eta: 0:07:38    time: 3.7190  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 50/156]  eta: 0:06:52    time: 3.6535  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 60/156]  eta: 0:06:07    time: 3.5665  data: 0.0002  max mem: 13008
Test:  [ 70/156]  eta: 0:05:28    time: 3.6273  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:48    time: 3.6971  data: 0.0002  max mem: 13008
Test:  [ 90/156]  eta: 0:04:10    time: 3.6960  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:31    time: 3.7308  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:53    time: 3.7431  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:15    time: 3.7422  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:37    time: 3.6954  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:01:00    time: 3.6972  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:22    time: 3.7079  data: 0.0002  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.7293  data: 0.0002  max mem: 13008
Test: Total time: 0:09:47 (3.7640 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e3.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e3.json and result file is ./output_freeze/coco_captioning_result_val_e3.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e3.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1140740.22 tokens per second.
PTBTokenizer tokenized 62820 tokens at 438043.65 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 51156, 'reflen': 49074, 'guess': [51156, 46164, 41172, 36180], 'correct': [33045, 17039, 7572, 3033]}
ratio: 1.0424257244161665
Bleu_1: 0.646
Bleu_2: 0.488
Bleu_3: 0.353
Bleu_4: 0.246
computing METEOR score...
METEOR: 0.236
computing Rouge score...
ROUGE_L: 0.505
computing CIDEr score...
CIDEr: 0.798
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [22.63 seconds]
SPICE evaluation took: 34.30 s
SPICE: 0.173
Performance of the network on the 5000 val images: 0.8%
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 09:06:21,316] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-best/mp_rank_00_model_states.pt
Max performance: 0.80%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [4]  [   0/3542]  eta: 15:31:52  lr: 0.000030  min_lr: 0.000030  loss: 1.7881 (1.7881)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 15.7856  data: 15.1722  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [4]  [  10/3542]  eta: 2:41:17  lr: 0.000030  min_lr: 0.000030  loss: 1.8389 (1.8952)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.7400  data: 2.1512  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [4]  [  20/3542]  eta: 1:59:00  lr: 0.000030  min_lr: 0.000030  loss: 1.8350 (1.8784)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3396  data: 0.7561  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [4]  [  30/3542]  eta: 1:44:41  lr: 0.000030  min_lr: 0.000030  loss: 1.8516 (1.8806)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2655  data: 0.6845  max mem: 13008
Epoch: [4]  [  40/3542]  eta: 1:38:42  lr: 0.000030  min_lr: 0.000030  loss: 1.8701 (1.8876)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3380  data: 0.7569  max mem: 13008
Epoch: [4]  [  50/3542]  eta: 1:32:21  lr: 0.000030  min_lr: 0.000030  loss: 1.8262 (1.8860)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2743  data: 0.6935  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [4]  [  60/3542]  eta: 1:25:32  lr: 0.000030  min_lr: 0.000030  loss: 1.8613 (1.9020)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0290  data: 0.4477  max mem: 13008
Epoch: [4]  [  70/3542]  eta: 1:21:25  lr: 0.000030  min_lr: 0.000030  loss: 1.9355 (1.9062)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9480  data: 0.3659  max mem: 13008
Epoch: [4]  [  80/3542]  eta: 1:16:38  lr: 0.000030  min_lr: 0.000030  loss: 1.8887 (1.9027)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8840  data: 0.3011  max mem: 13008
Epoch: [4]  [  90/3542]  eta: 1:13:13  lr: 0.000030  min_lr: 0.000030  loss: 1.8857 (1.9056)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.2116  max mem: 13008
Epoch: [4]  [ 100/3542]  eta: 1:10:18  lr: 0.000030  min_lr: 0.000030  loss: 1.8916 (1.9047)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8092  data: 0.2251  max mem: 13008
Epoch: [4]  [ 110/3542]  eta: 1:07:30  lr: 0.000030  min_lr: 0.000030  loss: 1.9004 (1.9046)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7603  data: 0.1767  max mem: 13008
Epoch: [4]  [ 120/3542]  eta: 1:05:20  lr: 0.000030  min_lr: 0.000030  loss: 1.9004 (1.9047)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7429  data: 0.1586  max mem: 13008
Epoch: [4]  [ 130/3542]  eta: 1:03:37  lr: 0.000030  min_lr: 0.000030  loss: 1.8906 (1.9061)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7779  data: 0.1938  max mem: 13008
Epoch: [4]  [ 140/3542]  eta: 1:01:45  lr: 0.000030  min_lr: 0.000030  loss: 1.8984 (1.9098)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7465  data: 0.1633  max mem: 13008
Epoch: [4]  [ 150/3542]  eta: 0:59:58  lr: 0.000030  min_lr: 0.000030  loss: 1.8926 (1.9110)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6809  data: 0.0971  max mem: 13008
Epoch: [4]  [ 160/3542]  eta: 0:58:41  lr: 0.000030  min_lr: 0.000030  loss: 1.8887 (1.9110)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7036  data: 0.1188  max mem: 13008
Epoch: [4]  [ 170/3542]  eta: 0:57:22  lr: 0.000030  min_lr: 0.000030  loss: 1.9277 (1.9137)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7194  data: 0.1347  max mem: 13008
Epoch: [4]  [ 180/3542]  eta: 0:56:07  lr: 0.000030  min_lr: 0.000030  loss: 1.8857 (1.9121)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.0991  max mem: 13008
Epoch: [4]  [ 190/3542]  eta: 0:55:00  lr: 0.000030  min_lr: 0.000030  loss: 1.8789 (1.9103)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.0908  max mem: 13008
Epoch: [4]  [ 200/3542]  eta: 0:54:08  lr: 0.000030  min_lr: 0.000030  loss: 1.8789 (1.9074)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7029  data: 0.1189  max mem: 13008
Epoch: [4]  [ 210/3542]  eta: 0:53:04  lr: 0.000030  min_lr: 0.000030  loss: 1.8271 (1.9039)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6782  data: 0.0936  max mem: 13008
Epoch: [4]  [ 220/3542]  eta: 0:52:13  lr: 0.000030  min_lr: 0.000030  loss: 1.9346 (1.9093)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.0696  max mem: 13008
Epoch: [4]  [ 230/3542]  eta: 0:51:24  lr: 0.000030  min_lr: 0.000030  loss: 1.9404 (1.9071)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6744  data: 0.0892  max mem: 13008
Epoch: [4]  [ 240/3542]  eta: 0:50:33  lr: 0.000030  min_lr: 0.000030  loss: 1.8262 (1.9031)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.0628  max mem: 13008
Epoch: [4]  [ 250/3542]  eta: 0:49:51  lr: 0.000030  min_lr: 0.000030  loss: 1.8262 (1.9022)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6470  data: 0.0613  max mem: 13008
Epoch: [4]  [ 260/3542]  eta: 0:49:09  lr: 0.000030  min_lr: 0.000030  loss: 1.8027 (1.8990)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.0700  max mem: 13008
Epoch: [4]  [ 270/3542]  eta: 0:48:32  lr: 0.000030  min_lr: 0.000030  loss: 1.8428 (1.8996)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6571  data: 0.0719  max mem: 13008
Epoch: [4]  [ 280/3542]  eta: 0:47:56  lr: 0.000030  min_lr: 0.000030  loss: 1.8838 (1.9006)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.0787  max mem: 13008
Epoch: [4]  [ 290/3542]  eta: 0:47:16  lr: 0.000030  min_lr: 0.000030  loss: 1.9082 (1.9020)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.0430  max mem: 13008
Epoch: [4]  [ 300/3542]  eta: 0:46:44  lr: 0.000030  min_lr: 0.000030  loss: 1.9082 (1.9024)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6269  data: 0.0418  max mem: 13008
Epoch: [4]  [ 310/3542]  eta: 0:46:17  lr: 0.000030  min_lr: 0.000030  loss: 1.8750 (1.9017)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6727  data: 0.0874  max mem: 13008
Epoch: [4]  [ 320/3542]  eta: 0:45:45  lr: 0.000030  min_lr: 0.000030  loss: 1.8906 (1.9014)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.0721  max mem: 13008
Epoch: [4]  [ 330/3542]  eta: 0:45:17  lr: 0.000030  min_lr: 0.000030  loss: 1.9131 (1.9034)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6387  data: 0.0546  max mem: 13008
Epoch: [4]  [ 340/3542]  eta: 0:44:56  lr: 0.000030  min_lr: 0.000030  loss: 1.9434 (1.9028)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6814  data: 0.0967  max mem: 13008
Epoch: [4]  [ 350/3542]  eta: 0:44:33  lr: 0.000030  min_lr: 0.000030  loss: 1.9414 (1.9043)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6984  data: 0.1126  max mem: 13008
Epoch: [4]  [ 360/3542]  eta: 0:44:21  lr: 0.000030  min_lr: 0.000030  loss: 1.8633 (1.9034)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7395  data: 0.1533  max mem: 13008
Epoch: [4]  [ 370/3542]  eta: 0:44:04  lr: 0.000030  min_lr: 0.000030  loss: 1.8223 (1.9030)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7674  data: 0.1808  max mem: 13008
Epoch: [4]  [ 380/3542]  eta: 0:43:54  lr: 0.000030  min_lr: 0.000030  loss: 1.8506 (1.9038)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7740  data: 0.1876  max mem: 13008
Epoch: [4]  [ 390/3542]  eta: 0:43:53  lr: 0.000030  min_lr: 0.000030  loss: 1.8770 (1.9044)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8650  data: 0.2796  max mem: 13008
Epoch: [4]  [ 400/3542]  eta: 0:44:10  lr: 0.000030  min_lr: 0.000030  loss: 1.9551 (1.9064)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0448  data: 0.4585  max mem: 13008
Epoch: [4]  [ 410/3542]  eta: 0:44:24  lr: 0.000030  min_lr: 0.000030  loss: 1.9277 (1.9063)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1533  data: 0.5669  max mem: 13008
Epoch: [4]  [ 420/3542]  eta: 0:44:31  lr: 0.000030  min_lr: 0.000030  loss: 1.9092 (1.9071)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1010  data: 0.5160  max mem: 13008
[2023-05-16 09:12:23,953] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 09:12:23,953] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [4]  [ 430/3542]  eta: 0:44:50  lr: 0.000030  min_lr: 0.000030  loss: 1.9248 (1.9080)  loss_scale: 8192.0000 (8382.0696)  weight_decay: 0.0500 (0.0500)  time: 1.1493  data: 0.5644  max mem: 13008
Epoch: [4]  [ 440/3542]  eta: 0:45:03  lr: 0.000030  min_lr: 0.000030  loss: 1.9248 (1.9076)  loss_scale: 16384.0000 (8563.5193)  weight_decay: 0.0500 (0.0500)  time: 1.2016  data: 0.6150  max mem: 13008
Epoch: [4]  [ 450/3542]  eta: 0:45:17  lr: 0.000029  min_lr: 0.000029  loss: 1.9336 (1.9078)  loss_scale: 16384.0000 (8736.9224)  weight_decay: 0.0500 (0.0500)  time: 1.1851  data: 0.5990  max mem: 13008
Epoch: [4]  [ 460/3542]  eta: 0:45:21  lr: 0.000029  min_lr: 0.000029  loss: 1.9258 (1.9076)  loss_scale: 16384.0000 (8902.8026)  weight_decay: 0.0500 (0.0500)  time: 1.1368  data: 0.5531  max mem: 13008
Epoch: [4]  [ 470/3542]  eta: 0:45:43  lr: 0.000029  min_lr: 0.000029  loss: 1.8975 (1.9070)  loss_scale: 16384.0000 (9061.6391)  weight_decay: 0.0500 (0.0500)  time: 1.2119  data: 0.6281  max mem: 13008
Epoch: [4]  [ 480/3542]  eta: 0:46:10  lr: 0.000029  min_lr: 0.000029  loss: 1.8975 (1.9074)  loss_scale: 16384.0000 (9213.8711)  weight_decay: 0.0500 (0.0500)  time: 1.4054  data: 0.8191  max mem: 13008
Epoch: [4]  [ 490/3542]  eta: 0:46:11  lr: 0.000029  min_lr: 0.000029  loss: 1.8789 (1.9068)  loss_scale: 16384.0000 (9359.9022)  weight_decay: 0.0500 (0.0500)  time: 1.2648  data: 0.6792  max mem: 13008
Epoch: [4]  [ 500/3542]  eta: 0:46:04  lr: 0.000029  min_lr: 0.000029  loss: 1.8398 (1.9050)  loss_scale: 16384.0000 (9500.1038)  weight_decay: 0.0500 (0.0500)  time: 1.0081  data: 0.4247  max mem: 13008
Epoch: [4]  [ 510/3542]  eta: 0:46:08  lr: 0.000029  min_lr: 0.000029  loss: 1.8271 (1.9047)  loss_scale: 16384.0000 (9634.8180)  weight_decay: 0.0500 (0.0500)  time: 1.0352  data: 0.4515  max mem: 13008
Epoch: [4]  [ 520/3542]  eta: 0:46:35  lr: 0.000029  min_lr: 0.000029  loss: 1.8057 (1.9019)  loss_scale: 16384.0000 (9764.3608)  weight_decay: 0.0500 (0.0500)  time: 1.3301  data: 0.7454  max mem: 13008
Epoch: [4]  [ 530/3542]  eta: 0:46:44  lr: 0.000029  min_lr: 0.000029  loss: 1.8174 (1.9014)  loss_scale: 16384.0000 (9889.0245)  weight_decay: 0.0500 (0.0500)  time: 1.3931  data: 0.8087  max mem: 13008
Epoch: [4]  [ 540/3542]  eta: 0:46:55  lr: 0.000029  min_lr: 0.000029  loss: 1.8232 (1.9009)  loss_scale: 16384.0000 (10009.0795)  weight_decay: 0.0500 (0.0500)  time: 1.2715  data: 0.6892  max mem: 13008
Epoch: [4]  [ 550/3542]  eta: 0:46:59  lr: 0.000029  min_lr: 0.000029  loss: 1.8965 (1.9012)  loss_scale: 16384.0000 (10124.7768)  weight_decay: 0.0500 (0.0500)  time: 1.2361  data: 0.6518  max mem: 13008
Epoch: [4]  [ 560/3542]  eta: 0:47:12  lr: 0.000029  min_lr: 0.000029  loss: 1.9199 (1.8998)  loss_scale: 16384.0000 (10236.3494)  weight_decay: 0.0500 (0.0500)  time: 1.2764  data: 0.6922  max mem: 13008
Epoch: [4]  [ 570/3542]  eta: 0:47:17  lr: 0.000029  min_lr: 0.000029  loss: 1.8789 (1.8992)  loss_scale: 16384.0000 (10344.0140)  weight_decay: 0.0500 (0.0500)  time: 1.2984  data: 0.7160  max mem: 13008
Epoch: [4]  [ 580/3542]  eta: 0:47:34  lr: 0.000029  min_lr: 0.000029  loss: 1.8789 (1.8989)  loss_scale: 16384.0000 (10447.9725)  weight_decay: 0.0500 (0.0500)  time: 1.3496  data: 0.7670  max mem: 13008
Epoch: [4]  [ 590/3542]  eta: 0:47:53  lr: 0.000029  min_lr: 0.000029  loss: 1.8506 (1.8993)  loss_scale: 16384.0000 (10548.4129)  weight_decay: 0.0500 (0.0500)  time: 1.5084  data: 0.9238  max mem: 13008
Epoch: [4]  [ 600/3542]  eta: 0:47:57  lr: 0.000029  min_lr: 0.000029  loss: 1.8721 (1.8991)  loss_scale: 16384.0000 (10645.5108)  weight_decay: 0.0500 (0.0500)  time: 1.3995  data: 0.8143  max mem: 13008
Epoch: [4]  [ 610/3542]  eta: 0:48:16  lr: 0.000029  min_lr: 0.000029  loss: 1.8955 (1.8984)  loss_scale: 16384.0000 (10739.4304)  weight_decay: 0.0500 (0.0500)  time: 1.4106  data: 0.8280  max mem: 13008
Epoch: [4]  [ 620/3542]  eta: 0:48:35  lr: 0.000029  min_lr: 0.000029  loss: 1.9062 (1.8994)  loss_scale: 16384.0000 (10830.3253)  weight_decay: 0.0500 (0.0500)  time: 1.5858  data: 1.0015  max mem: 13008
Epoch: [4]  [ 630/3542]  eta: 0:48:36  lr: 0.000029  min_lr: 0.000029  loss: 1.9102 (1.9003)  loss_scale: 16384.0000 (10918.3391)  weight_decay: 0.0500 (0.0500)  time: 1.4177  data: 0.8333  max mem: 13008
Epoch: [4]  [ 640/3542]  eta: 0:48:40  lr: 0.000029  min_lr: 0.000029  loss: 1.9053 (1.9004)  loss_scale: 16384.0000 (11003.6069)  weight_decay: 0.0500 (0.0500)  time: 1.2708  data: 0.6883  max mem: 13008
Epoch: [4]  [ 650/3542]  eta: 0:48:47  lr: 0.000029  min_lr: 0.000029  loss: 1.8252 (1.8996)  loss_scale: 16384.0000 (11086.2550)  weight_decay: 0.0500 (0.0500)  time: 1.3589  data: 0.7769  max mem: 13008
Epoch: [4]  [ 660/3542]  eta: 0:48:58  lr: 0.000029  min_lr: 0.000029  loss: 1.8193 (1.8986)  loss_scale: 16384.0000 (11166.4024)  weight_decay: 0.0500 (0.0500)  time: 1.4503  data: 0.8666  max mem: 13008
Epoch: [4]  [ 670/3542]  eta: 0:49:03  lr: 0.000029  min_lr: 0.000029  loss: 1.8242 (1.8984)  loss_scale: 16384.0000 (11244.1610)  weight_decay: 0.0500 (0.0500)  time: 1.4365  data: 0.8523  max mem: 13008
Epoch: [4]  [ 680/3542]  eta: 0:49:09  lr: 0.000029  min_lr: 0.000029  loss: 1.8242 (1.8977)  loss_scale: 16384.0000 (11319.6358)  weight_decay: 0.0500 (0.0500)  time: 1.3930  data: 0.8097  max mem: 13008
Epoch: [4]  [ 690/3542]  eta: 0:49:22  lr: 0.000029  min_lr: 0.000029  loss: 1.8506 (1.8987)  loss_scale: 16384.0000 (11392.9262)  weight_decay: 0.0500 (0.0500)  time: 1.5022  data: 0.9176  max mem: 13008
Epoch: [4]  [ 700/3542]  eta: 0:49:48  lr: 0.000029  min_lr: 0.000029  loss: 1.9199 (1.8985)  loss_scale: 16384.0000 (11464.1255)  weight_decay: 0.0500 (0.0500)  time: 1.7577  data: 1.1741  max mem: 13008
Epoch: [4]  [ 710/3542]  eta: 0:49:58  lr: 0.000029  min_lr: 0.000029  loss: 1.9199 (1.8992)  loss_scale: 16384.0000 (11533.3221)  weight_decay: 0.0500 (0.0500)  time: 1.7480  data: 1.1655  max mem: 13008
Epoch: [4]  [ 720/3542]  eta: 0:49:53  lr: 0.000029  min_lr: 0.000029  loss: 1.9307 (1.8999)  loss_scale: 16384.0000 (11600.5992)  weight_decay: 0.0500 (0.0500)  time: 1.3939  data: 0.8095  max mem: 13008
Epoch: [4]  [ 730/3542]  eta: 0:49:50  lr: 0.000029  min_lr: 0.000029  loss: 1.9971 (1.9014)  loss_scale: 16384.0000 (11666.0356)  weight_decay: 0.0500 (0.0500)  time: 1.2320  data: 0.6479  max mem: 13008
Epoch: [4]  [ 740/3542]  eta: 0:49:42  lr: 0.000029  min_lr: 0.000029  loss: 1.9580 (1.9015)  loss_scale: 16384.0000 (11729.7058)  weight_decay: 0.0500 (0.0500)  time: 1.1874  data: 0.6048  max mem: 13008
Epoch: [4]  [ 750/3542]  eta: 0:49:35  lr: 0.000029  min_lr: 0.000029  loss: 1.8662 (1.9020)  loss_scale: 16384.0000 (11791.6804)  weight_decay: 0.0500 (0.0500)  time: 1.1441  data: 0.5613  max mem: 13008
Epoch: [4]  [ 760/3542]  eta: 0:49:22  lr: 0.000029  min_lr: 0.000029  loss: 1.8506 (1.9018)  loss_scale: 16384.0000 (11852.0263)  weight_decay: 0.0500 (0.0500)  time: 1.0895  data: 0.5069  max mem: 13008
Epoch: [4]  [ 770/3542]  eta: 0:49:07  lr: 0.000029  min_lr: 0.000029  loss: 1.8857 (1.9023)  loss_scale: 16384.0000 (11910.8067)  weight_decay: 0.0500 (0.0500)  time: 0.9668  data: 0.3836  max mem: 13008
Epoch: [4]  [ 780/3542]  eta: 0:48:53  lr: 0.000029  min_lr: 0.000029  loss: 1.9121 (1.9022)  loss_scale: 16384.0000 (11968.0819)  weight_decay: 0.0500 (0.0500)  time: 0.9448  data: 0.3584  max mem: 13008
Epoch: [4]  [ 790/3542]  eta: 0:48:38  lr: 0.000029  min_lr: 0.000029  loss: 1.9199 (1.9035)  loss_scale: 16384.0000 (12023.9090)  weight_decay: 0.0500 (0.0500)  time: 0.9605  data: 0.3744  max mem: 13008
Epoch: [4]  [ 800/3542]  eta: 0:48:17  lr: 0.000029  min_lr: 0.000029  loss: 1.9473 (1.9043)  loss_scale: 16384.0000 (12078.3421)  weight_decay: 0.0500 (0.0500)  time: 0.8535  data: 0.2694  max mem: 13008
Epoch: [4]  [ 810/3542]  eta: 0:47:57  lr: 0.000029  min_lr: 0.000029  loss: 1.8760 (1.9036)  loss_scale: 16384.0000 (12131.4328)  weight_decay: 0.0500 (0.0500)  time: 0.7721  data: 0.1879  max mem: 13008
Epoch: [4]  [ 820/3542]  eta: 0:47:36  lr: 0.000029  min_lr: 0.000029  loss: 1.8594 (1.9038)  loss_scale: 16384.0000 (12183.2302)  weight_decay: 0.0500 (0.0500)  time: 0.7523  data: 0.1681  max mem: 13008
Epoch: [4]  [ 830/3542]  eta: 0:47:17  lr: 0.000029  min_lr: 0.000029  loss: 1.8945 (1.9044)  loss_scale: 16384.0000 (12233.7810)  weight_decay: 0.0500 (0.0500)  time: 0.7552  data: 0.1710  max mem: 13008
[2023-05-16 09:20:53,150] [INFO] [logging.py:60:log_dist] [Rank 0] step=15000, skipped=15, lr=[2.8835858507335776e-05, 2.8835858507335776e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 09:20:53,153] [INFO] [timer.py:157:stop] 0/15000, SamplesPerSec=55.563946390222014
Epoch: [4]  [ 840/3542]  eta: 0:46:57  lr: 0.000029  min_lr: 0.000029  loss: 1.8945 (1.9041)  loss_scale: 16384.0000 (12283.1296)  weight_decay: 0.0500 (0.0500)  time: 0.7809  data: 0.1959  max mem: 13008
Epoch: [4]  [ 850/3542]  eta: 0:46:37  lr: 0.000029  min_lr: 0.000029  loss: 1.8750 (1.9041)  loss_scale: 16384.0000 (12331.3184)  weight_decay: 0.0500 (0.0500)  time: 0.7449  data: 0.1594  max mem: 13008
Epoch: [4]  [ 860/3542]  eta: 0:46:15  lr: 0.000029  min_lr: 0.000029  loss: 1.8281 (1.9037)  loss_scale: 16384.0000 (12378.3879)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.1094  max mem: 13008
Epoch: [4]  [ 870/3542]  eta: 0:45:55  lr: 0.000029  min_lr: 0.000029  loss: 1.9277 (1.9041)  loss_scale: 16384.0000 (12424.3766)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.1036  max mem: 13008
Epoch: [4]  [ 880/3542]  eta: 0:45:33  lr: 0.000029  min_lr: 0.000029  loss: 1.9492 (1.9045)  loss_scale: 16384.0000 (12469.3212)  weight_decay: 0.0500 (0.0500)  time: 0.6915  data: 0.1047  max mem: 13008
Epoch: [4]  [ 890/3542]  eta: 0:45:14  lr: 0.000029  min_lr: 0.000029  loss: 1.9307 (1.9054)  loss_scale: 16384.0000 (12513.2570)  weight_decay: 0.0500 (0.0500)  time: 0.6993  data: 0.1125  max mem: 13008
Epoch: [4]  [ 900/3542]  eta: 0:44:55  lr: 0.000029  min_lr: 0.000029  loss: 1.9395 (1.9056)  loss_scale: 16384.0000 (12556.2175)  weight_decay: 0.0500 (0.0500)  time: 0.7203  data: 0.1341  max mem: 13008
Epoch: [4]  [ 910/3542]  eta: 0:44:36  lr: 0.000029  min_lr: 0.000029  loss: 1.9326 (1.9057)  loss_scale: 16384.0000 (12598.2349)  weight_decay: 0.0500 (0.0500)  time: 0.7147  data: 0.1287  max mem: 13008
Epoch: [4]  [ 920/3542]  eta: 0:44:16  lr: 0.000029  min_lr: 0.000029  loss: 1.9170 (1.9057)  loss_scale: 16384.0000 (12639.3398)  weight_decay: 0.0500 (0.0500)  time: 0.6938  data: 0.1075  max mem: 13008
Epoch: [4]  [ 930/3542]  eta: 0:43:58  lr: 0.000029  min_lr: 0.000029  loss: 1.9248 (1.9068)  loss_scale: 16384.0000 (12679.5618)  weight_decay: 0.0500 (0.0500)  time: 0.6941  data: 0.1084  max mem: 13008
Epoch: [4]  [ 940/3542]  eta: 0:43:40  lr: 0.000029  min_lr: 0.000029  loss: 1.9580 (1.9073)  loss_scale: 16384.0000 (12718.9288)  weight_decay: 0.0500 (0.0500)  time: 0.7257  data: 0.1398  max mem: 13008
Epoch: [4]  [ 950/3542]  eta: 0:43:21  lr: 0.000029  min_lr: 0.000029  loss: 1.8838 (1.9072)  loss_scale: 16384.0000 (12757.4679)  weight_decay: 0.0500 (0.0500)  time: 0.7124  data: 0.1259  max mem: 13008
Epoch: [4]  [ 960/3542]  eta: 0:43:04  lr: 0.000029  min_lr: 0.000029  loss: 1.8818 (1.9075)  loss_scale: 16384.0000 (12795.2050)  weight_decay: 0.0500 (0.0500)  time: 0.7137  data: 0.1271  max mem: 13008
Epoch: [4]  [ 970/3542]  eta: 0:42:46  lr: 0.000029  min_lr: 0.000029  loss: 1.9082 (1.9071)  loss_scale: 16384.0000 (12832.1648)  weight_decay: 0.0500 (0.0500)  time: 0.7217  data: 0.1347  max mem: 13008
Epoch: [4]  [ 980/3542]  eta: 0:42:29  lr: 0.000029  min_lr: 0.000029  loss: 1.8574 (1.9071)  loss_scale: 16384.0000 (12868.3710)  weight_decay: 0.0500 (0.0500)  time: 0.7130  data: 0.1254  max mem: 13008
Epoch: [4]  [ 990/3542]  eta: 0:42:09  lr: 0.000029  min_lr: 0.000029  loss: 1.8975 (1.9072)  loss_scale: 16384.0000 (12903.8466)  weight_decay: 0.0500 (0.0500)  time: 0.6678  data: 0.0806  max mem: 13008
Epoch: [4]  [1000/3542]  eta: 0:41:52  lr: 0.000029  min_lr: 0.000029  loss: 1.9023 (1.9067)  loss_scale: 16384.0000 (12938.6134)  weight_decay: 0.0500 (0.0500)  time: 0.6599  data: 0.0735  max mem: 13008
Epoch: [4]  [1010/3542]  eta: 0:41:35  lr: 0.000029  min_lr: 0.000029  loss: 1.7979 (1.9058)  loss_scale: 16384.0000 (12972.6924)  weight_decay: 0.0500 (0.0500)  time: 0.7055  data: 0.1197  max mem: 13008
Epoch: [4]  [1020/3542]  eta: 0:41:18  lr: 0.000029  min_lr: 0.000029  loss: 1.8154 (1.9057)  loss_scale: 16384.0000 (13006.1038)  weight_decay: 0.0500 (0.0500)  time: 0.6981  data: 0.1118  max mem: 13008
Epoch: [4]  [1030/3542]  eta: 0:41:01  lr: 0.000028  min_lr: 0.000028  loss: 1.9072 (1.9066)  loss_scale: 16384.0000 (13038.8671)  weight_decay: 0.0500 (0.0500)  time: 0.6947  data: 0.1077  max mem: 13008
Epoch: [4]  [1040/3542]  eta: 0:40:44  lr: 0.000028  min_lr: 0.000028  loss: 1.8887 (1.9064)  loss_scale: 16384.0000 (13071.0010)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.1083  max mem: 13008
Epoch: [4]  [1050/3542]  eta: 0:40:29  lr: 0.000028  min_lr: 0.000028  loss: 1.8643 (1.9062)  loss_scale: 16384.0000 (13102.5233)  weight_decay: 0.0500 (0.0500)  time: 0.7075  data: 0.1208  max mem: 13008
Epoch: [4]  [1060/3542]  eta: 0:40:11  lr: 0.000028  min_lr: 0.000028  loss: 1.9336 (1.9069)  loss_scale: 16384.0000 (13133.4515)  weight_decay: 0.0500 (0.0500)  time: 0.6913  data: 0.1041  max mem: 13008
Epoch: [4]  [1070/3542]  eta: 0:39:55  lr: 0.000028  min_lr: 0.000028  loss: 1.9248 (1.9066)  loss_scale: 16384.0000 (13163.8021)  weight_decay: 0.0500 (0.0500)  time: 0.6734  data: 0.0867  max mem: 13008
Epoch: [4]  [1080/3542]  eta: 0:39:39  lr: 0.000028  min_lr: 0.000028  loss: 1.8740 (1.9065)  loss_scale: 16384.0000 (13193.5911)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.1031  max mem: 13008
Epoch: [4]  [1090/3542]  eta: 0:39:24  lr: 0.000028  min_lr: 0.000028  loss: 1.9102 (1.9064)  loss_scale: 16384.0000 (13222.8341)  weight_decay: 0.0500 (0.0500)  time: 0.6994  data: 0.1127  max mem: 13008
Epoch: [4]  [1100/3542]  eta: 0:39:07  lr: 0.000028  min_lr: 0.000028  loss: 1.8818 (1.9062)  loss_scale: 16384.0000 (13251.5459)  weight_decay: 0.0500 (0.0500)  time: 0.6904  data: 0.1038  max mem: 13008
Epoch: [4]  [1110/3542]  eta: 0:38:50  lr: 0.000028  min_lr: 0.000028  loss: 1.8818 (1.9063)  loss_scale: 16384.0000 (13279.7408)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.0580  max mem: 13008
Epoch: [4]  [1120/3542]  eta: 0:38:35  lr: 0.000028  min_lr: 0.000028  loss: 1.9121 (1.9061)  loss_scale: 16384.0000 (13307.4326)  weight_decay: 0.0500 (0.0500)  time: 0.6464  data: 0.0597  max mem: 13008
Epoch: [4]  [1130/3542]  eta: 0:38:20  lr: 0.000028  min_lr: 0.000028  loss: 1.9258 (1.9070)  loss_scale: 16384.0000 (13334.6348)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.1037  max mem: 13008
Epoch: [4]  [1140/3542]  eta: 0:38:05  lr: 0.000028  min_lr: 0.000028  loss: 1.9463 (1.9075)  loss_scale: 16384.0000 (13361.3602)  weight_decay: 0.0500 (0.0500)  time: 0.6971  data: 0.1094  max mem: 13008
Epoch: [4]  [1150/3542]  eta: 0:37:50  lr: 0.000028  min_lr: 0.000028  loss: 1.8984 (1.9072)  loss_scale: 16384.0000 (13387.6212)  weight_decay: 0.0500 (0.0500)  time: 0.7019  data: 0.1146  max mem: 13008
Epoch: [4]  [1160/3542]  eta: 0:37:37  lr: 0.000028  min_lr: 0.000028  loss: 1.8125 (1.9069)  loss_scale: 16384.0000 (13413.4298)  weight_decay: 0.0500 (0.0500)  time: 0.7298  data: 0.1424  max mem: 13008
Epoch: [4]  [1170/3542]  eta: 0:37:22  lr: 0.000028  min_lr: 0.000028  loss: 1.8291 (1.9069)  loss_scale: 16384.0000 (13438.7976)  weight_decay: 0.0500 (0.0500)  time: 0.7176  data: 0.1291  max mem: 13008
Epoch: [4]  [1180/3542]  eta: 0:37:08  lr: 0.000028  min_lr: 0.000028  loss: 1.8418 (1.9067)  loss_scale: 16384.0000 (13463.7358)  weight_decay: 0.0500 (0.0500)  time: 0.7100  data: 0.1221  max mem: 13008
Epoch: [4]  [1190/3542]  eta: 0:36:54  lr: 0.000028  min_lr: 0.000028  loss: 1.9111 (1.9070)  loss_scale: 16384.0000 (13488.2552)  weight_decay: 0.0500 (0.0500)  time: 0.7064  data: 0.1199  max mem: 13008
Epoch: [4]  [1200/3542]  eta: 0:36:40  lr: 0.000028  min_lr: 0.000028  loss: 1.9199 (1.9067)  loss_scale: 16384.0000 (13512.3664)  weight_decay: 0.0500 (0.0500)  time: 0.6955  data: 0.1089  max mem: 13008
Epoch: [4]  [1210/3542]  eta: 0:36:24  lr: 0.000028  min_lr: 0.000028  loss: 1.8877 (1.9066)  loss_scale: 16384.0000 (13536.0793)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.0850  max mem: 13008
Epoch: [4]  [1220/3542]  eta: 0:36:11  lr: 0.000028  min_lr: 0.000028  loss: 1.9209 (1.9069)  loss_scale: 16384.0000 (13559.4038)  weight_decay: 0.0500 (0.0500)  time: 0.6744  data: 0.0889  max mem: 13008
Epoch: [4]  [1230/3542]  eta: 0:35:57  lr: 0.000028  min_lr: 0.000028  loss: 1.8633 (1.9060)  loss_scale: 16384.0000 (13582.3493)  weight_decay: 0.0500 (0.0500)  time: 0.6988  data: 0.1132  max mem: 13008
Epoch: [4]  [1240/3542]  eta: 0:35:42  lr: 0.000028  min_lr: 0.000028  loss: 1.8428 (1.9057)  loss_scale: 16384.0000 (13604.9251)  weight_decay: 0.0500 (0.0500)  time: 0.6759  data: 0.0905  max mem: 13008
Epoch: [4]  [1250/3542]  eta: 0:35:28  lr: 0.000028  min_lr: 0.000028  loss: 1.8887 (1.9061)  loss_scale: 16384.0000 (13627.1399)  weight_decay: 0.0500 (0.0500)  time: 0.6727  data: 0.0868  max mem: 13008
Epoch: [4]  [1260/3542]  eta: 0:35:14  lr: 0.000028  min_lr: 0.000028  loss: 1.9004 (1.9061)  loss_scale: 16384.0000 (13649.0024)  weight_decay: 0.0500 (0.0500)  time: 0.6677  data: 0.0809  max mem: 13008
Epoch: [4]  [1270/3542]  eta: 0:35:01  lr: 0.000028  min_lr: 0.000028  loss: 1.9521 (1.9061)  loss_scale: 16384.0000 (13670.5208)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.1020  max mem: 13008
Epoch: [4]  [1280/3542]  eta: 0:34:48  lr: 0.000028  min_lr: 0.000028  loss: 1.9248 (1.9060)  loss_scale: 16384.0000 (13691.7034)  weight_decay: 0.0500 (0.0500)  time: 0.7087  data: 0.1219  max mem: 13008
Epoch: [4]  [1290/3542]  eta: 0:34:35  lr: 0.000028  min_lr: 0.000028  loss: 1.8652 (1.9055)  loss_scale: 16384.0000 (13712.5577)  weight_decay: 0.0500 (0.0500)  time: 0.7034  data: 0.1165  max mem: 13008
Epoch: [4]  [1300/3542]  eta: 0:34:21  lr: 0.000028  min_lr: 0.000028  loss: 1.8135 (1.9049)  loss_scale: 16384.0000 (13733.0915)  weight_decay: 0.0500 (0.0500)  time: 0.6789  data: 0.0924  max mem: 13008
Epoch: [4]  [1310/3542]  eta: 0:34:08  lr: 0.000028  min_lr: 0.000028  loss: 1.8359 (1.9051)  loss_scale: 16384.0000 (13753.3120)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.0768  max mem: 13008
Epoch: [4]  [1320/3542]  eta: 0:33:56  lr: 0.000028  min_lr: 0.000028  loss: 1.8359 (1.9046)  loss_scale: 16384.0000 (13773.2263)  weight_decay: 0.0500 (0.0500)  time: 0.7080  data: 0.1215  max mem: 13008
Epoch: [4]  [1330/3542]  eta: 0:33:44  lr: 0.000028  min_lr: 0.000028  loss: 1.8447 (1.9049)  loss_scale: 16384.0000 (13792.8415)  weight_decay: 0.0500 (0.0500)  time: 0.7473  data: 0.1606  max mem: 13008
Epoch: [4]  [1340/3542]  eta: 0:33:30  lr: 0.000028  min_lr: 0.000028  loss: 1.8604 (1.9048)  loss_scale: 16384.0000 (13812.1641)  weight_decay: 0.0500 (0.0500)  time: 0.7077  data: 0.1211  max mem: 13008
Epoch: [4]  [1350/3542]  eta: 0:33:18  lr: 0.000028  min_lr: 0.000028  loss: 1.8525 (1.9049)  loss_scale: 16384.0000 (13831.2006)  weight_decay: 0.0500 (0.0500)  time: 0.6766  data: 0.0899  max mem: 13008
Epoch: [4]  [1360/3542]  eta: 0:33:05  lr: 0.000028  min_lr: 0.000028  loss: 1.9014 (1.9050)  loss_scale: 16384.0000 (13849.9574)  weight_decay: 0.0500 (0.0500)  time: 0.6958  data: 0.1090  max mem: 13008
Epoch: [4]  [1370/3542]  eta: 0:32:52  lr: 0.000028  min_lr: 0.000028  loss: 1.9023 (1.9049)  loss_scale: 16384.0000 (13868.4406)  weight_decay: 0.0500 (0.0500)  time: 0.6842  data: 0.0973  max mem: 13008
Epoch: [4]  [1380/3542]  eta: 0:32:39  lr: 0.000028  min_lr: 0.000028  loss: 1.8994 (1.9054)  loss_scale: 16384.0000 (13886.6560)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.0738  max mem: 13008
Epoch: [4]  [1390/3542]  eta: 0:32:28  lr: 0.000028  min_lr: 0.000028  loss: 1.8740 (1.9055)  loss_scale: 16384.0000 (13904.6096)  weight_decay: 0.0500 (0.0500)  time: 0.7013  data: 0.1153  max mem: 13008
Epoch: [4]  [1400/3542]  eta: 0:32:14  lr: 0.000028  min_lr: 0.000028  loss: 1.8535 (1.9057)  loss_scale: 16384.0000 (13922.3069)  weight_decay: 0.0500 (0.0500)  time: 0.6826  data: 0.0973  max mem: 13008
Epoch: [4]  [1410/3542]  eta: 0:32:03  lr: 0.000028  min_lr: 0.000028  loss: 1.9531 (1.9061)  loss_scale: 16384.0000 (13939.7534)  weight_decay: 0.0500 (0.0500)  time: 0.6805  data: 0.0948  max mem: 13008
Epoch: [4]  [1420/3542]  eta: 0:31:51  lr: 0.000028  min_lr: 0.000028  loss: 1.8740 (1.9052)  loss_scale: 16384.0000 (13956.9543)  weight_decay: 0.0500 (0.0500)  time: 0.7342  data: 0.1476  max mem: 13008
[2023-05-16 09:27:43,903] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 09:27:43,903] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[2023-05-16 09:27:45,075] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 15591
[2023-05-16 09:27:45,075] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-05-16 09:27:45,075] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
Epoch: [4]  [1430/3542]  eta: 0:31:37  lr: 0.000028  min_lr: 0.000028  loss: 1.8330 (1.9050)  loss_scale: 16384.0000 (13996.8134)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.0653  max mem: 13008
Epoch: [4]  [1440/3542]  eta: 0:31:25  lr: 0.000028  min_lr: 0.000028  loss: 1.8525 (1.9044)  loss_scale: 16384.0000 (14013.3796)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.0512  max mem: 13008
Epoch: [4]  [1450/3542]  eta: 0:31:13  lr: 0.000028  min_lr: 0.000028  loss: 1.8525 (1.9043)  loss_scale: 16384.0000 (14029.7174)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.0922  max mem: 13008
Epoch: [4]  [1460/3542]  eta: 0:31:01  lr: 0.000028  min_lr: 0.000028  loss: 1.8672 (1.9041)  loss_scale: 16384.0000 (14045.8316)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.0771  max mem: 13008
Epoch: [4]  [1470/3542]  eta: 0:30:49  lr: 0.000028  min_lr: 0.000028  loss: 1.8330 (1.9039)  loss_scale: 16384.0000 (14061.7267)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.0885  max mem: 13008
Epoch: [4]  [1480/3542]  eta: 0:30:37  lr: 0.000028  min_lr: 0.000028  loss: 1.8613 (1.9040)  loss_scale: 16384.0000 (14077.4072)  weight_decay: 0.0500 (0.0500)  time: 0.6936  data: 0.1071  max mem: 13008
Epoch: [4]  [1490/3542]  eta: 0:30:25  lr: 0.000028  min_lr: 0.000028  loss: 1.8682 (1.9038)  loss_scale: 16384.0000 (14092.8773)  weight_decay: 0.0500 (0.0500)  time: 0.6871  data: 0.1000  max mem: 13008
Epoch: [4]  [1500/3542]  eta: 0:30:14  lr: 0.000028  min_lr: 0.000028  loss: 1.8564 (1.9035)  loss_scale: 16384.0000 (14108.1412)  weight_decay: 0.0500 (0.0500)  time: 0.6728  data: 0.0851  max mem: 13008
Epoch: [4]  [1510/3542]  eta: 0:30:02  lr: 0.000028  min_lr: 0.000028  loss: 1.8555 (1.9030)  loss_scale: 16384.0000 (14123.2032)  weight_decay: 0.0500 (0.0500)  time: 0.6642  data: 0.0776  max mem: 13008
Epoch: [4]  [1520/3542]  eta: 0:29:51  lr: 0.000028  min_lr: 0.000028  loss: 1.8721 (1.9031)  loss_scale: 16384.0000 (14138.0671)  weight_decay: 0.0500 (0.0500)  time: 0.6977  data: 0.1116  max mem: 13008
Epoch: [4]  [1530/3542]  eta: 0:29:40  lr: 0.000028  min_lr: 0.000028  loss: 1.8809 (1.9032)  loss_scale: 16384.0000 (14152.7368)  weight_decay: 0.0500 (0.0500)  time: 0.7247  data: 0.1384  max mem: 13008
Epoch: [4]  [1540/3542]  eta: 0:29:29  lr: 0.000028  min_lr: 0.000028  loss: 1.8750 (1.9031)  loss_scale: 16384.0000 (14167.2161)  weight_decay: 0.0500 (0.0500)  time: 0.7192  data: 0.1330  max mem: 13008
Epoch: [4]  [1550/3542]  eta: 0:29:17  lr: 0.000028  min_lr: 0.000028  loss: 1.8662 (1.9032)  loss_scale: 16384.0000 (14181.5087)  weight_decay: 0.0500 (0.0500)  time: 0.6865  data: 0.1002  max mem: 13008
Epoch: [4]  [1560/3542]  eta: 0:29:06  lr: 0.000028  min_lr: 0.000028  loss: 1.9229 (1.9034)  loss_scale: 16384.0000 (14195.6182)  weight_decay: 0.0500 (0.0500)  time: 0.6691  data: 0.0824  max mem: 13008
Epoch: [4]  [1570/3542]  eta: 0:28:54  lr: 0.000028  min_lr: 0.000028  loss: 1.9463 (1.9035)  loss_scale: 16384.0000 (14209.5481)  weight_decay: 0.0500 (0.0500)  time: 0.6792  data: 0.0923  max mem: 13008
Epoch: [4]  [1580/3542]  eta: 0:28:42  lr: 0.000028  min_lr: 0.000028  loss: 1.9082 (1.9037)  loss_scale: 16384.0000 (14223.3017)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.0657  max mem: 13008
Epoch: [4]  [1590/3542]  eta: 0:28:31  lr: 0.000027  min_lr: 0.000027  loss: 1.9082 (1.9033)  loss_scale: 16384.0000 (14236.8825)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.0883  max mem: 13008
Epoch: [4]  [1600/3542]  eta: 0:28:21  lr: 0.000027  min_lr: 0.000027  loss: 1.9229 (1.9037)  loss_scale: 16384.0000 (14250.2936)  weight_decay: 0.0500 (0.0500)  time: 0.7185  data: 0.1319  max mem: 13008
Epoch: [4]  [1610/3542]  eta: 0:28:10  lr: 0.000027  min_lr: 0.000027  loss: 1.9307 (1.9037)  loss_scale: 16384.0000 (14263.5382)  weight_decay: 0.0500 (0.0500)  time: 0.6940  data: 0.1073  max mem: 13008
Epoch: [4]  [1620/3542]  eta: 0:27:59  lr: 0.000027  min_lr: 0.000027  loss: 1.9307 (1.9045)  loss_scale: 16384.0000 (14276.6194)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.1075  max mem: 13008
Epoch: [4]  [1630/3542]  eta: 0:27:48  lr: 0.000027  min_lr: 0.000027  loss: 1.9932 (1.9048)  loss_scale: 16384.0000 (14289.5402)  weight_decay: 0.0500 (0.0500)  time: 0.6924  data: 0.1056  max mem: 13008
Epoch: [4]  [1640/3542]  eta: 0:27:37  lr: 0.000027  min_lr: 0.000027  loss: 1.9238 (1.9047)  loss_scale: 16384.0000 (14302.3035)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.0732  max mem: 13008
Epoch: [4]  [1650/3542]  eta: 0:27:26  lr: 0.000027  min_lr: 0.000027  loss: 1.8896 (1.9046)  loss_scale: 16384.0000 (14314.9122)  weight_decay: 0.0500 (0.0500)  time: 0.6656  data: 0.0789  max mem: 13008
Epoch: [4]  [1660/3542]  eta: 0:27:15  lr: 0.000027  min_lr: 0.000027  loss: 1.8809 (1.9047)  loss_scale: 16384.0000 (14327.3691)  weight_decay: 0.0500 (0.0500)  time: 0.6748  data: 0.0886  max mem: 13008
Epoch: [4]  [1670/3542]  eta: 0:27:04  lr: 0.000027  min_lr: 0.000027  loss: 1.9717 (1.9052)  loss_scale: 16384.0000 (14339.6768)  weight_decay: 0.0500 (0.0500)  time: 0.6948  data: 0.1086  max mem: 13008
Epoch: [4]  [1680/3542]  eta: 0:26:53  lr: 0.000027  min_lr: 0.000027  loss: 2.0039 (1.9057)  loss_scale: 16384.0000 (14351.8382)  weight_decay: 0.0500 (0.0500)  time: 0.6942  data: 0.1079  max mem: 13008
Epoch: [4]  [1690/3542]  eta: 0:26:42  lr: 0.000027  min_lr: 0.000027  loss: 1.8848 (1.9050)  loss_scale: 16384.0000 (14363.8557)  weight_decay: 0.0500 (0.0500)  time: 0.6313  data: 0.0445  max mem: 13008
Epoch: [4]  [1700/3542]  eta: 0:26:30  lr: 0.000027  min_lr: 0.000027  loss: 1.8271 (1.9048)  loss_scale: 16384.0000 (14375.7319)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0208  max mem: 13008
Epoch: [4]  [1710/3542]  eta: 0:26:20  lr: 0.000027  min_lr: 0.000027  loss: 1.8672 (1.9051)  loss_scale: 16384.0000 (14387.4693)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.0590  max mem: 13008
Epoch: [4]  [1720/3542]  eta: 0:26:09  lr: 0.000027  min_lr: 0.000027  loss: 1.8662 (1.9051)  loss_scale: 16384.0000 (14399.0703)  weight_decay: 0.0500 (0.0500)  time: 0.6865  data: 0.0992  max mem: 13008
Epoch: [4]  [1730/3542]  eta: 0:25:59  lr: 0.000027  min_lr: 0.000027  loss: 1.9277 (1.9051)  loss_scale: 16384.0000 (14410.5373)  weight_decay: 0.0500 (0.0500)  time: 0.7047  data: 0.1170  max mem: 13008
Epoch: [4]  [1740/3542]  eta: 0:25:49  lr: 0.000027  min_lr: 0.000027  loss: 1.9541 (1.9056)  loss_scale: 16384.0000 (14421.8725)  weight_decay: 0.0500 (0.0500)  time: 0.7001  data: 0.1133  max mem: 13008
Epoch: [4]  [1750/3542]  eta: 0:25:39  lr: 0.000027  min_lr: 0.000027  loss: 1.9219 (1.9054)  loss_scale: 16384.0000 (14433.0782)  weight_decay: 0.0500 (0.0500)  time: 0.7041  data: 0.1168  max mem: 13008
Epoch: [4]  [1760/3542]  eta: 0:25:28  lr: 0.000027  min_lr: 0.000027  loss: 1.8730 (1.9055)  loss_scale: 16384.0000 (14444.1567)  weight_decay: 0.0500 (0.0500)  time: 0.6843  data: 0.0972  max mem: 13008
Epoch: [4]  [1770/3542]  eta: 0:25:18  lr: 0.000027  min_lr: 0.000027  loss: 1.8789 (1.9053)  loss_scale: 16384.0000 (14455.1101)  weight_decay: 0.0500 (0.0500)  time: 0.6719  data: 0.0854  max mem: 13008
Epoch: [4]  [1780/3542]  eta: 0:25:07  lr: 0.000027  min_lr: 0.000027  loss: 1.8799 (1.9054)  loss_scale: 16384.0000 (14465.9405)  weight_decay: 0.0500 (0.0500)  time: 0.6766  data: 0.0893  max mem: 13008
Epoch: [4]  [1790/3542]  eta: 0:24:58  lr: 0.000027  min_lr: 0.000027  loss: 1.9688 (1.9060)  loss_scale: 16384.0000 (14476.6499)  weight_decay: 0.0500 (0.0500)  time: 0.7085  data: 0.1213  max mem: 13008
Epoch: [4]  [1800/3542]  eta: 0:24:48  lr: 0.000027  min_lr: 0.000027  loss: 1.9160 (1.9061)  loss_scale: 16384.0000 (14487.2404)  weight_decay: 0.0500 (0.0500)  time: 0.7138  data: 0.1273  max mem: 13008
Epoch: [4]  [1810/3542]  eta: 0:24:38  lr: 0.000027  min_lr: 0.000027  loss: 1.9160 (1.9065)  loss_scale: 16384.0000 (14497.7140)  weight_decay: 0.0500 (0.0500)  time: 0.6992  data: 0.1130  max mem: 13008
Epoch: [4]  [1820/3542]  eta: 0:24:27  lr: 0.000027  min_lr: 0.000027  loss: 1.9268 (1.9067)  loss_scale: 16384.0000 (14508.0725)  weight_decay: 0.0500 (0.0500)  time: 0.6897  data: 0.1040  max mem: 13008
Epoch: [4]  [1830/3542]  eta: 0:24:17  lr: 0.000027  min_lr: 0.000027  loss: 1.9707 (1.9072)  loss_scale: 16384.0000 (14518.3179)  weight_decay: 0.0500 (0.0500)  time: 0.6717  data: 0.0861  max mem: 13008
[2023-05-16 09:32:22,940] [INFO] [logging.py:60:log_dist] [Rank 0] step=16000, skipped=16, lr=[2.7060926441098284e-05, 2.7060926441098284e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 09:32:22,943] [INFO] [timer.py:157:stop] 0/16000, SamplesPerSec=55.55911002453313
Epoch: [4]  [1840/3542]  eta: 0:24:08  lr: 0.000027  min_lr: 0.000027  loss: 1.9795 (1.9075)  loss_scale: 16384.0000 (14528.4519)  weight_decay: 0.0500 (0.0500)  time: 0.7322  data: 0.1457  max mem: 13008
[2023-05-16 09:32:36,351] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 16018
[2023-05-16 09:32:36,351] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 09:32:36,352] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [4]  [1850/3542]  eta: 0:23:57  lr: 0.000027  min_lr: 0.000027  loss: 1.9160 (1.9071)  loss_scale: 16384.0000 (14534.0508)  weight_decay: 0.0500 (0.0500)  time: 0.6997  data: 0.1129  max mem: 13008
Epoch: [4]  [1860/3542]  eta: 0:23:47  lr: 0.000027  min_lr: 0.000027  loss: 1.9229 (1.9076)  loss_scale: 8192.0000 (14499.9721)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0196  max mem: 13008
Epoch: [4]  [1870/3542]  eta: 0:23:37  lr: 0.000027  min_lr: 0.000027  loss: 1.9482 (1.9078)  loss_scale: 8192.0000 (14466.2576)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.0460  max mem: 13008
Epoch: [4]  [1880/3542]  eta: 0:23:27  lr: 0.000027  min_lr: 0.000027  loss: 1.8936 (1.9076)  loss_scale: 8192.0000 (14432.9016)  weight_decay: 0.0500 (0.0500)  time: 0.6713  data: 0.0840  max mem: 13008
Epoch: [4]  [1890/3542]  eta: 0:23:17  lr: 0.000027  min_lr: 0.000027  loss: 1.9023 (1.9077)  loss_scale: 8192.0000 (14399.8985)  weight_decay: 0.0500 (0.0500)  time: 0.7164  data: 0.1298  max mem: 13008
Epoch: [4]  [1900/3542]  eta: 0:23:07  lr: 0.000027  min_lr: 0.000027  loss: 1.9004 (1.9073)  loss_scale: 8192.0000 (14367.2425)  weight_decay: 0.0500 (0.0500)  time: 0.7247  data: 0.1395  max mem: 13008
Epoch: [4]  [1910/3542]  eta: 0:22:58  lr: 0.000027  min_lr: 0.000027  loss: 1.8105 (1.9071)  loss_scale: 8192.0000 (14334.9283)  weight_decay: 0.0500 (0.0500)  time: 0.6893  data: 0.1032  max mem: 13008
Epoch: [4]  [1920/3542]  eta: 0:22:48  lr: 0.000027  min_lr: 0.000027  loss: 1.8545 (1.9070)  loss_scale: 8192.0000 (14302.9505)  weight_decay: 0.0500 (0.0500)  time: 0.7152  data: 0.1292  max mem: 13008
Epoch: [4]  [1930/3542]  eta: 0:22:39  lr: 0.000027  min_lr: 0.000027  loss: 1.8955 (1.9071)  loss_scale: 8192.0000 (14271.3040)  weight_decay: 0.0500 (0.0500)  time: 0.7103  data: 0.1252  max mem: 13008
Epoch: [4]  [1940/3542]  eta: 0:22:29  lr: 0.000027  min_lr: 0.000027  loss: 1.8955 (1.9069)  loss_scale: 8192.0000 (14239.9835)  weight_decay: 0.0500 (0.0500)  time: 0.6915  data: 0.1050  max mem: 13008
Epoch: [4]  [1950/3542]  eta: 0:22:19  lr: 0.000027  min_lr: 0.000027  loss: 1.8438 (1.9066)  loss_scale: 8192.0000 (14208.9841)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.0798  max mem: 13008
Epoch: [4]  [1960/3542]  eta: 0:22:09  lr: 0.000027  min_lr: 0.000027  loss: 1.8691 (1.9068)  loss_scale: 8192.0000 (14178.3009)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.0808  max mem: 13008
Epoch: [4]  [1970/3542]  eta: 0:22:00  lr: 0.000027  min_lr: 0.000027  loss: 1.8828 (1.9066)  loss_scale: 8192.0000 (14147.9290)  weight_decay: 0.0500 (0.0500)  time: 0.7007  data: 0.1139  max mem: 13008
Epoch: [4]  [1980/3542]  eta: 0:21:50  lr: 0.000027  min_lr: 0.000027  loss: 1.8320 (1.9063)  loss_scale: 8192.0000 (14117.8637)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.0907  max mem: 13008
Epoch: [4]  [1990/3542]  eta: 0:21:41  lr: 0.000027  min_lr: 0.000027  loss: 1.7969 (1.9063)  loss_scale: 8192.0000 (14088.1005)  weight_decay: 0.0500 (0.0500)  time: 0.6918  data: 0.1045  max mem: 13008
Epoch: [4]  [2000/3542]  eta: 0:21:31  lr: 0.000027  min_lr: 0.000027  loss: 1.9102 (1.9066)  loss_scale: 8192.0000 (14058.6347)  weight_decay: 0.0500 (0.0500)  time: 0.6963  data: 0.1085  max mem: 13008
Epoch: [4]  [2010/3542]  eta: 0:21:21  lr: 0.000027  min_lr: 0.000027  loss: 1.9297 (1.9067)  loss_scale: 8192.0000 (14029.4620)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.0874  max mem: 13008
Epoch: [4]  [2020/3542]  eta: 0:21:12  lr: 0.000027  min_lr: 0.000027  loss: 1.9053 (1.9063)  loss_scale: 8192.0000 (14000.5779)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.0838  max mem: 13008
Epoch: [4]  [2030/3542]  eta: 0:21:02  lr: 0.000027  min_lr: 0.000027  loss: 1.8447 (1.9060)  loss_scale: 8192.0000 (13971.9783)  weight_decay: 0.0500 (0.0500)  time: 0.6766  data: 0.0892  max mem: 13008
Epoch: [4]  [2040/3542]  eta: 0:20:53  lr: 0.000027  min_lr: 0.000027  loss: 1.8906 (1.9060)  loss_scale: 8192.0000 (13943.6590)  weight_decay: 0.0500 (0.0500)  time: 0.6735  data: 0.0864  max mem: 13008
Epoch: [4]  [2050/3542]  eta: 0:20:43  lr: 0.000027  min_lr: 0.000027  loss: 1.8906 (1.9056)  loss_scale: 8192.0000 (13915.6158)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.0653  max mem: 13008
Epoch: [4]  [2060/3542]  eta: 0:20:33  lr: 0.000027  min_lr: 0.000027  loss: 1.9648 (1.9062)  loss_scale: 8192.0000 (13887.8447)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0740  max mem: 13008
Epoch: [4]  [2070/3542]  eta: 0:20:24  lr: 0.000027  min_lr: 0.000027  loss: 1.9648 (1.9060)  loss_scale: 8192.0000 (13860.3419)  weight_decay: 0.0500 (0.0500)  time: 0.6768  data: 0.0896  max mem: 13008
Epoch: [4]  [2080/3542]  eta: 0:20:15  lr: 0.000027  min_lr: 0.000027  loss: 1.8828 (1.9061)  loss_scale: 8192.0000 (13833.1033)  weight_decay: 0.0500 (0.0500)  time: 0.6731  data: 0.0851  max mem: 13008
Epoch: [4]  [2090/3542]  eta: 0:20:05  lr: 0.000027  min_lr: 0.000027  loss: 1.8984 (1.9063)  loss_scale: 8192.0000 (13806.1253)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.0747  max mem: 13008
Epoch: [4]  [2100/3542]  eta: 0:19:55  lr: 0.000027  min_lr: 0.000027  loss: 1.9023 (1.9063)  loss_scale: 8192.0000 (13779.4041)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.0684  max mem: 13008
Epoch: [4]  [2110/3542]  eta: 0:19:46  lr: 0.000027  min_lr: 0.000027  loss: 1.9023 (1.9064)  loss_scale: 8192.0000 (13752.9360)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.0774  max mem: 13008
Epoch: [4]  [2120/3542]  eta: 0:19:37  lr: 0.000027  min_lr: 0.000027  loss: 1.9199 (1.9066)  loss_scale: 8192.0000 (13726.7176)  weight_decay: 0.0500 (0.0500)  time: 0.6775  data: 0.0909  max mem: 13008
Epoch: [4]  [2130/3542]  eta: 0:19:27  lr: 0.000027  min_lr: 0.000027  loss: 1.9395 (1.9070)  loss_scale: 8192.0000 (13700.7452)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.0746  max mem: 13008
Epoch: [4]  [2140/3542]  eta: 0:19:18  lr: 0.000026  min_lr: 0.000026  loss: 1.8525 (1.9068)  loss_scale: 8192.0000 (13675.0154)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0758  max mem: 13008
Epoch: [4]  [2150/3542]  eta: 0:19:08  lr: 0.000026  min_lr: 0.000026  loss: 1.7998 (1.9067)  loss_scale: 8192.0000 (13649.5249)  weight_decay: 0.0500 (0.0500)  time: 0.6337  data: 0.0464  max mem: 13008
Epoch: [4]  [2160/3542]  eta: 0:18:59  lr: 0.000026  min_lr: 0.000026  loss: 1.8467 (1.9066)  loss_scale: 8192.0000 (13624.2702)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0248  max mem: 13008
Epoch: [4]  [2170/3542]  eta: 0:18:50  lr: 0.000026  min_lr: 0.000026  loss: 1.8467 (1.9064)  loss_scale: 8192.0000 (13599.2483)  weight_decay: 0.0500 (0.0500)  time: 0.6631  data: 0.0756  max mem: 13008
Epoch: [4]  [2180/3542]  eta: 0:18:41  lr: 0.000026  min_lr: 0.000026  loss: 1.8428 (1.9066)  loss_scale: 8192.0000 (13574.4558)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.0888  max mem: 13008
Epoch: [4]  [2190/3542]  eta: 0:18:31  lr: 0.000026  min_lr: 0.000026  loss: 1.9277 (1.9069)  loss_scale: 8192.0000 (13549.8895)  weight_decay: 0.0500 (0.0500)  time: 0.6719  data: 0.0834  max mem: 13008
Epoch: [4]  [2200/3542]  eta: 0:18:22  lr: 0.000026  min_lr: 0.000026  loss: 1.8838 (1.9066)  loss_scale: 8192.0000 (13525.5466)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.0716  max mem: 13008
Epoch: [4]  [2210/3542]  eta: 0:18:13  lr: 0.000026  min_lr: 0.000026  loss: 1.8574 (1.9065)  loss_scale: 8192.0000 (13501.4238)  weight_decay: 0.0500 (0.0500)  time: 0.6581  data: 0.0715  max mem: 13008
Epoch: [4]  [2220/3542]  eta: 0:18:04  lr: 0.000026  min_lr: 0.000026  loss: 1.8955 (1.9064)  loss_scale: 8192.0000 (13477.5182)  weight_decay: 0.0500 (0.0500)  time: 0.6797  data: 0.0929  max mem: 13008
Epoch: [4]  [2230/3542]  eta: 0:17:55  lr: 0.000026  min_lr: 0.000026  loss: 1.8633 (1.9061)  loss_scale: 8192.0000 (13453.8270)  weight_decay: 0.0500 (0.0500)  time: 0.6815  data: 0.0958  max mem: 13008
Epoch: [4]  [2240/3542]  eta: 0:17:46  lr: 0.000026  min_lr: 0.000026  loss: 1.8467 (1.9062)  loss_scale: 8192.0000 (13430.3472)  weight_decay: 0.0500 (0.0500)  time: 0.6861  data: 0.1004  max mem: 13008
Epoch: [4]  [2250/3542]  eta: 0:17:37  lr: 0.000026  min_lr: 0.000026  loss: 1.8467 (1.9062)  loss_scale: 8192.0000 (13407.0760)  weight_decay: 0.0500 (0.0500)  time: 0.7221  data: 0.1359  max mem: 13008
Epoch: [4]  [2260/3542]  eta: 0:17:29  lr: 0.000026  min_lr: 0.000026  loss: 1.8467 (1.9060)  loss_scale: 8192.0000 (13384.0106)  weight_decay: 0.0500 (0.0500)  time: 0.7693  data: 0.1833  max mem: 13008
Epoch: [4]  [2270/3542]  eta: 0:17:20  lr: 0.000026  min_lr: 0.000026  loss: 1.9199 (1.9059)  loss_scale: 8192.0000 (13361.1484)  weight_decay: 0.0500 (0.0500)  time: 0.7225  data: 0.1361  max mem: 13008
Epoch: [4]  [2280/3542]  eta: 0:17:11  lr: 0.000026  min_lr: 0.000026  loss: 1.9492 (1.9059)  loss_scale: 8192.0000 (13338.4866)  weight_decay: 0.0500 (0.0500)  time: 0.6752  data: 0.0889  max mem: 13008
Epoch: [4]  [2290/3542]  eta: 0:17:02  lr: 0.000026  min_lr: 0.000026  loss: 1.8896 (1.9056)  loss_scale: 8192.0000 (13316.0227)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0725  max mem: 13008
Epoch: [4]  [2300/3542]  eta: 0:16:53  lr: 0.000026  min_lr: 0.000026  loss: 1.8018 (1.9052)  loss_scale: 8192.0000 (13293.7540)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.0943  max mem: 13008
Epoch: [4]  [2310/3542]  eta: 0:16:44  lr: 0.000026  min_lr: 0.000026  loss: 1.8359 (1.9053)  loss_scale: 8192.0000 (13271.6781)  weight_decay: 0.0500 (0.0500)  time: 0.6994  data: 0.1131  max mem: 13008
Epoch: [4]  [2320/3542]  eta: 0:16:35  lr: 0.000026  min_lr: 0.000026  loss: 1.8691 (1.9051)  loss_scale: 8192.0000 (13249.7923)  weight_decay: 0.0500 (0.0500)  time: 0.6546  data: 0.0690  max mem: 13008
Epoch: [4]  [2330/3542]  eta: 0:16:26  lr: 0.000026  min_lr: 0.000026  loss: 1.9023 (1.9051)  loss_scale: 8192.0000 (13228.0944)  weight_decay: 0.0500 (0.0500)  time: 0.6690  data: 0.0838  max mem: 13008
Epoch: [4]  [2340/3542]  eta: 0:16:18  lr: 0.000026  min_lr: 0.000026  loss: 1.8848 (1.9048)  loss_scale: 8192.0000 (13206.5818)  weight_decay: 0.0500 (0.0500)  time: 0.7034  data: 0.1177  max mem: 13008
Epoch: [4]  [2350/3542]  eta: 0:16:09  lr: 0.000026  min_lr: 0.000026  loss: 1.8496 (1.9048)  loss_scale: 8192.0000 (13185.2522)  weight_decay: 0.0500 (0.0500)  time: 0.7096  data: 0.1238  max mem: 13008
Epoch: [4]  [2360/3542]  eta: 0:16:00  lr: 0.000026  min_lr: 0.000026  loss: 1.9141 (1.9051)  loss_scale: 8192.0000 (13164.1033)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.0769  max mem: 13008
Epoch: [4]  [2370/3542]  eta: 0:15:51  lr: 0.000026  min_lr: 0.000026  loss: 1.8750 (1.9048)  loss_scale: 8192.0000 (13143.1329)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.0661  max mem: 13008
Epoch: [4]  [2380/3542]  eta: 0:15:43  lr: 0.000026  min_lr: 0.000026  loss: 1.7832 (1.9045)  loss_scale: 8192.0000 (13122.3385)  weight_decay: 0.0500 (0.0500)  time: 0.6886  data: 0.1017  max mem: 13008
Epoch: [4]  [2390/3542]  eta: 0:15:34  lr: 0.000026  min_lr: 0.000026  loss: 1.8799 (1.9044)  loss_scale: 8192.0000 (13101.7181)  weight_decay: 0.0500 (0.0500)  time: 0.6922  data: 0.1049  max mem: 13008
Epoch: [4]  [2400/3542]  eta: 0:15:25  lr: 0.000026  min_lr: 0.000026  loss: 1.8311 (1.9041)  loss_scale: 8192.0000 (13081.2695)  weight_decay: 0.0500 (0.0500)  time: 0.6805  data: 0.0938  max mem: 13008
Epoch: [4]  [2410/3542]  eta: 0:15:16  lr: 0.000026  min_lr: 0.000026  loss: 1.8271 (1.9040)  loss_scale: 8192.0000 (13060.9905)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.0906  max mem: 13008
Epoch: [4]  [2420/3542]  eta: 0:15:08  lr: 0.000026  min_lr: 0.000026  loss: 1.8799 (1.9041)  loss_scale: 8192.0000 (13040.8790)  weight_decay: 0.0500 (0.0500)  time: 0.6864  data: 0.0996  max mem: 13008
Epoch: [4]  [2430/3542]  eta: 0:14:59  lr: 0.000026  min_lr: 0.000026  loss: 1.8721 (1.9042)  loss_scale: 8192.0000 (13020.9329)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.0977  max mem: 13008
Epoch: [4]  [2440/3542]  eta: 0:14:50  lr: 0.000026  min_lr: 0.000026  loss: 1.9131 (1.9045)  loss_scale: 8192.0000 (13001.1503)  weight_decay: 0.0500 (0.0500)  time: 0.6639  data: 0.0774  max mem: 13008
Epoch: [4]  [2450/3542]  eta: 0:14:41  lr: 0.000026  min_lr: 0.000026  loss: 2.0195 (1.9047)  loss_scale: 8192.0000 (12981.5292)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.0721  max mem: 13008
Epoch: [4]  [2460/3542]  eta: 0:14:33  lr: 0.000026  min_lr: 0.000026  loss: 1.9580 (1.9049)  loss_scale: 8192.0000 (12962.0675)  weight_decay: 0.0500 (0.0500)  time: 0.7087  data: 0.1219  max mem: 13008
Epoch: [4]  [2470/3542]  eta: 0:14:24  lr: 0.000026  min_lr: 0.000026  loss: 1.8369 (1.9047)  loss_scale: 8192.0000 (12942.7633)  weight_decay: 0.0500 (0.0500)  time: 0.6887  data: 0.1018  max mem: 13008
Epoch: [4]  [2480/3542]  eta: 0:14:15  lr: 0.000026  min_lr: 0.000026  loss: 1.8369 (1.9050)  loss_scale: 8192.0000 (12923.6147)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.0437  max mem: 13008
Epoch: [4]  [2490/3542]  eta: 0:14:07  lr: 0.000026  min_lr: 0.000026  loss: 1.9307 (1.9048)  loss_scale: 8192.0000 (12904.6198)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.1012  max mem: 13008
Epoch: [4]  [2500/3542]  eta: 0:13:59  lr: 0.000026  min_lr: 0.000026  loss: 1.8516 (1.9046)  loss_scale: 8192.0000 (12885.7769)  weight_decay: 0.0500 (0.0500)  time: 0.7256  data: 0.1390  max mem: 13008
Epoch: [4]  [2510/3542]  eta: 0:13:50  lr: 0.000026  min_lr: 0.000026  loss: 1.8564 (1.9047)  loss_scale: 8192.0000 (12867.0840)  weight_decay: 0.0500 (0.0500)  time: 0.6746  data: 0.0878  max mem: 13008
Epoch: [4]  [2520/3542]  eta: 0:13:41  lr: 0.000026  min_lr: 0.000026  loss: 1.9424 (1.9048)  loss_scale: 8192.0000 (12848.5395)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0784  max mem: 13008
Epoch: [4]  [2530/3542]  eta: 0:13:33  lr: 0.000026  min_lr: 0.000026  loss: 1.9482 (1.9047)  loss_scale: 8192.0000 (12830.1414)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.0873  max mem: 13008
Epoch: [4]  [2540/3542]  eta: 0:13:24  lr: 0.000026  min_lr: 0.000026  loss: 1.8701 (1.9045)  loss_scale: 8192.0000 (12811.8882)  weight_decay: 0.0500 (0.0500)  time: 0.6875  data: 0.1017  max mem: 13008
Epoch: [4]  [2550/3542]  eta: 0:13:16  lr: 0.000026  min_lr: 0.000026  loss: 1.8896 (1.9046)  loss_scale: 8192.0000 (12793.7781)  weight_decay: 0.0500 (0.0500)  time: 0.6889  data: 0.1033  max mem: 13008
Epoch: [4]  [2560/3542]  eta: 0:13:07  lr: 0.000026  min_lr: 0.000026  loss: 1.8906 (1.9047)  loss_scale: 8192.0000 (12775.8094)  weight_decay: 0.0500 (0.0500)  time: 0.6847  data: 0.0987  max mem: 13008
Epoch: [4]  [2570/3542]  eta: 0:12:59  lr: 0.000026  min_lr: 0.000026  loss: 1.8906 (1.9046)  loss_scale: 8192.0000 (12757.9806)  weight_decay: 0.0500 (0.0500)  time: 0.7147  data: 0.1279  max mem: 13008
Epoch: [4]  [2580/3542]  eta: 0:12:50  lr: 0.000026  min_lr: 0.000026  loss: 1.8467 (1.9043)  loss_scale: 8192.0000 (12740.2898)  weight_decay: 0.0500 (0.0500)  time: 0.6731  data: 0.0862  max mem: 13008
Epoch: [4]  [2590/3542]  eta: 0:12:42  lr: 0.000026  min_lr: 0.000026  loss: 1.8838 (1.9046)  loss_scale: 8192.0000 (12722.7356)  weight_decay: 0.0500 (0.0500)  time: 0.6234  data: 0.0368  max mem: 13008
Epoch: [4]  [2600/3542]  eta: 0:12:33  lr: 0.000026  min_lr: 0.000026  loss: 1.8838 (1.9045)  loss_scale: 8192.0000 (12705.3164)  weight_decay: 0.0500 (0.0500)  time: 0.6719  data: 0.0855  max mem: 13008
Epoch: [4]  [2610/3542]  eta: 0:12:25  lr: 0.000026  min_lr: 0.000026  loss: 1.9229 (1.9048)  loss_scale: 8192.0000 (12688.0306)  weight_decay: 0.0500 (0.0500)  time: 0.6952  data: 0.1083  max mem: 13008
Epoch: [4]  [2620/3542]  eta: 0:12:16  lr: 0.000026  min_lr: 0.000026  loss: 1.9229 (1.9047)  loss_scale: 8192.0000 (12670.8768)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.0659  max mem: 13008
Epoch: [4]  [2630/3542]  eta: 0:12:08  lr: 0.000026  min_lr: 0.000026  loss: 1.9102 (1.9050)  loss_scale: 8192.0000 (12653.8533)  weight_decay: 0.0500 (0.0500)  time: 0.6640  data: 0.0767  max mem: 13008
Epoch: [4]  [2640/3542]  eta: 0:12:00  lr: 0.000026  min_lr: 0.000026  loss: 1.9141 (1.9049)  loss_scale: 8192.0000 (12636.9587)  weight_decay: 0.0500 (0.0500)  time: 0.6643  data: 0.0767  max mem: 13008
Epoch: [4]  [2650/3542]  eta: 0:11:51  lr: 0.000026  min_lr: 0.000026  loss: 1.8701 (1.9051)  loss_scale: 8192.0000 (12620.1916)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.0503  max mem: 13008
Epoch: [4]  [2660/3542]  eta: 0:11:43  lr: 0.000026  min_lr: 0.000026  loss: 1.8662 (1.9050)  loss_scale: 8192.0000 (12603.5505)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.0676  max mem: 13008
Epoch: [4]  [2670/3542]  eta: 0:11:34  lr: 0.000026  min_lr: 0.000026  loss: 1.9092 (1.9052)  loss_scale: 8192.0000 (12587.0341)  weight_decay: 0.0500 (0.0500)  time: 0.6561  data: 0.0698  max mem: 13008
Epoch: [4]  [2680/3542]  eta: 0:11:26  lr: 0.000026  min_lr: 0.000026  loss: 1.9365 (1.9052)  loss_scale: 8192.0000 (12570.6408)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.0826  max mem: 13008
Epoch: [4]  [2690/3542]  eta: 0:11:17  lr: 0.000025  min_lr: 0.000025  loss: 1.8965 (1.9053)  loss_scale: 8192.0000 (12554.3694)  weight_decay: 0.0500 (0.0500)  time: 0.6845  data: 0.0975  max mem: 13008
Epoch: [4]  [2700/3542]  eta: 0:11:09  lr: 0.000025  min_lr: 0.000025  loss: 1.7568 (1.9047)  loss_scale: 8192.0000 (12538.2184)  weight_decay: 0.0500 (0.0500)  time: 0.6723  data: 0.0863  max mem: 13008
Epoch: [4]  [2710/3542]  eta: 0:11:01  lr: 0.000025  min_lr: 0.000025  loss: 1.7871 (1.9048)  loss_scale: 8192.0000 (12522.1866)  weight_decay: 0.0500 (0.0500)  time: 0.6840  data: 0.0979  max mem: 13008
Epoch: [4]  [2720/3542]  eta: 0:10:53  lr: 0.000025  min_lr: 0.000025  loss: 1.9287 (1.9050)  loss_scale: 8192.0000 (12506.2727)  weight_decay: 0.0500 (0.0500)  time: 0.7114  data: 0.1249  max mem: 13008
Epoch: [4]  [2730/3542]  eta: 0:10:45  lr: 0.000025  min_lr: 0.000025  loss: 1.8896 (1.9051)  loss_scale: 8192.0000 (12490.4753)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.1328  max mem: 13008
Epoch: [4]  [2740/3542]  eta: 0:10:36  lr: 0.000025  min_lr: 0.000025  loss: 1.8867 (1.9049)  loss_scale: 8192.0000 (12474.7931)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.0624  max mem: 13008
Epoch: [4]  [2750/3542]  eta: 0:10:28  lr: 0.000025  min_lr: 0.000025  loss: 1.8555 (1.9047)  loss_scale: 8192.0000 (12459.2250)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.0509  max mem: 13008
Epoch: [4]  [2760/3542]  eta: 0:10:20  lr: 0.000025  min_lr: 0.000025  loss: 1.8340 (1.9045)  loss_scale: 8192.0000 (12443.7696)  weight_decay: 0.0500 (0.0500)  time: 0.6913  data: 0.1040  max mem: 13008
Epoch: [4]  [2770/3542]  eta: 0:10:11  lr: 0.000025  min_lr: 0.000025  loss: 1.8340 (1.9047)  loss_scale: 8192.0000 (12428.4258)  weight_decay: 0.0500 (0.0500)  time: 0.6849  data: 0.0973  max mem: 13008
Epoch: [4]  [2780/3542]  eta: 0:10:03  lr: 0.000025  min_lr: 0.000025  loss: 1.8613 (1.9047)  loss_scale: 8192.0000 (12413.1924)  weight_decay: 0.0500 (0.0500)  time: 0.6884  data: 0.1022  max mem: 13008
Epoch: [4]  [2790/3542]  eta: 0:09:55  lr: 0.000025  min_lr: 0.000025  loss: 1.8555 (1.9045)  loss_scale: 8192.0000 (12398.0681)  weight_decay: 0.0500 (0.0500)  time: 0.7144  data: 0.1286  max mem: 13008
Epoch: [4]  [2800/3542]  eta: 0:09:47  lr: 0.000025  min_lr: 0.000025  loss: 1.8750 (1.9045)  loss_scale: 8192.0000 (12383.0518)  weight_decay: 0.0500 (0.0500)  time: 0.6968  data: 0.1109  max mem: 13008
Epoch: [4]  [2810/3542]  eta: 0:09:38  lr: 0.000025  min_lr: 0.000025  loss: 1.8408 (1.9042)  loss_scale: 8192.0000 (12368.1423)  weight_decay: 0.0500 (0.0500)  time: 0.6509  data: 0.0650  max mem: 13008
Epoch: [4]  [2820/3542]  eta: 0:09:30  lr: 0.000025  min_lr: 0.000025  loss: 1.8877 (1.9045)  loss_scale: 8192.0000 (12353.3385)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0865  max mem: 13008
Epoch: [4]  [2830/3542]  eta: 0:09:22  lr: 0.000025  min_lr: 0.000025  loss: 1.8877 (1.9045)  loss_scale: 8192.0000 (12338.6394)  weight_decay: 0.0500 (0.0500)  time: 0.7186  data: 0.1320  max mem: 13008
[2023-05-16 09:43:41,642] [INFO] [logging.py:60:log_dist] [Rank 0] step=17000, skipped=17, lr=[2.5222324901631522e-05, 2.5222324901631522e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 09:43:41,645] [INFO] [timer.py:157:stop] 0/17000, SamplesPerSec=55.554557303866304
Epoch: [4]  [2840/3542]  eta: 0:09:14  lr: 0.000025  min_lr: 0.000025  loss: 1.8760 (1.9046)  loss_scale: 8192.0000 (12324.0436)  weight_decay: 0.0500 (0.0500)  time: 0.7242  data: 0.1381  max mem: 13008
Epoch: [4]  [2850/3542]  eta: 0:09:06  lr: 0.000025  min_lr: 0.000025  loss: 1.8926 (1.9045)  loss_scale: 8192.0000 (12309.5503)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.0794  max mem: 13008
[2023-05-16 09:43:54,948] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 09:43:54,948] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [4]  [2860/3542]  eta: 0:08:58  lr: 0.000025  min_lr: 0.000025  loss: 1.8301 (1.9044)  loss_scale: 8192.0000 (12323.7917)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.0547  max mem: 13008
Epoch: [4]  [2870/3542]  eta: 0:08:50  lr: 0.000025  min_lr: 0.000025  loss: 1.8896 (1.9044)  loss_scale: 16384.0000 (12337.9338)  weight_decay: 0.0500 (0.0500)  time: 0.6801  data: 0.0521  max mem: 13008
Epoch: [4]  [2880/3542]  eta: 0:08:41  lr: 0.000025  min_lr: 0.000025  loss: 1.9170 (1.9045)  loss_scale: 16384.0000 (12351.9778)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.0437  max mem: 13008
Epoch: [4]  [2890/3542]  eta: 0:08:33  lr: 0.000025  min_lr: 0.000025  loss: 1.8652 (1.9043)  loss_scale: 16384.0000 (12365.9246)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0735  max mem: 13008
Epoch: [4]  [2900/3542]  eta: 0:08:25  lr: 0.000025  min_lr: 0.000025  loss: 1.8516 (1.9042)  loss_scale: 16384.0000 (12379.7752)  weight_decay: 0.0500 (0.0500)  time: 0.6830  data: 0.0955  max mem: 13008
Epoch: [4]  [2910/3542]  eta: 0:08:17  lr: 0.000025  min_lr: 0.000025  loss: 1.8828 (1.9044)  loss_scale: 16384.0000 (12393.5307)  weight_decay: 0.0500 (0.0500)  time: 0.6820  data: 0.0951  max mem: 13008
Epoch: [4]  [2920/3542]  eta: 0:08:09  lr: 0.000025  min_lr: 0.000025  loss: 1.8672 (1.9041)  loss_scale: 16384.0000 (12407.1921)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0788  max mem: 13008
Epoch: [4]  [2930/3542]  eta: 0:08:01  lr: 0.000025  min_lr: 0.000025  loss: 1.8018 (1.9040)  loss_scale: 16384.0000 (12420.7602)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.0498  max mem: 13008
Epoch: [4]  [2940/3542]  eta: 0:07:53  lr: 0.000025  min_lr: 0.000025  loss: 1.8447 (1.9039)  loss_scale: 16384.0000 (12434.2360)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.0572  max mem: 13008
Epoch: [4]  [2950/3542]  eta: 0:07:44  lr: 0.000025  min_lr: 0.000025  loss: 1.7793 (1.9037)  loss_scale: 16384.0000 (12447.6205)  weight_decay: 0.0500 (0.0500)  time: 0.6887  data: 0.1016  max mem: 13008
Epoch: [4]  [2960/3542]  eta: 0:07:36  lr: 0.000025  min_lr: 0.000025  loss: 1.8379 (1.9038)  loss_scale: 16384.0000 (12460.9146)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.1020  max mem: 13008
Epoch: [4]  [2970/3542]  eta: 0:07:28  lr: 0.000025  min_lr: 0.000025  loss: 1.9287 (1.9039)  loss_scale: 16384.0000 (12474.1192)  weight_decay: 0.0500 (0.0500)  time: 0.6836  data: 0.0959  max mem: 13008
[2023-05-16 09:45:20,241] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 17146
[2023-05-16 09:45:20,241] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 09:45:20,241] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [4]  [2980/3542]  eta: 0:07:20  lr: 0.000025  min_lr: 0.000025  loss: 1.8223 (1.9033)  loss_scale: 16384.0000 (12478.9909)  weight_decay: 0.0500 (0.0500)  time: 0.6610  data: 0.0748  max mem: 13008
Epoch: [4]  [2990/3542]  eta: 0:07:12  lr: 0.000025  min_lr: 0.000025  loss: 1.8174 (1.9034)  loss_scale: 8192.0000 (12464.6580)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0751  max mem: 13008
Epoch: [4]  [3000/3542]  eta: 0:07:04  lr: 0.000025  min_lr: 0.000025  loss: 1.9092 (1.9032)  loss_scale: 8192.0000 (12450.4205)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0861  max mem: 13008
Epoch: [4]  [3010/3542]  eta: 0:06:56  lr: 0.000025  min_lr: 0.000025  loss: 1.8711 (1.9032)  loss_scale: 8192.0000 (12436.2776)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.0773  max mem: 13008
Epoch: [4]  [3020/3542]  eta: 0:06:48  lr: 0.000025  min_lr: 0.000025  loss: 1.8779 (1.9031)  loss_scale: 8192.0000 (12422.2284)  weight_decay: 0.0500 (0.0500)  time: 0.6785  data: 0.0915  max mem: 13008
Epoch: [4]  [3030/3542]  eta: 0:06:40  lr: 0.000025  min_lr: 0.000025  loss: 1.9043 (1.9033)  loss_scale: 8192.0000 (12408.2719)  weight_decay: 0.0500 (0.0500)  time: 0.6937  data: 0.1066  max mem: 13008
Epoch: [4]  [3040/3542]  eta: 0:06:32  lr: 0.000025  min_lr: 0.000025  loss: 1.8594 (1.9033)  loss_scale: 8192.0000 (12394.4071)  weight_decay: 0.0500 (0.0500)  time: 0.7095  data: 0.1224  max mem: 13008
Epoch: [4]  [3050/3542]  eta: 0:06:24  lr: 0.000025  min_lr: 0.000025  loss: 1.8594 (1.9030)  loss_scale: 8192.0000 (12380.6332)  weight_decay: 0.0500 (0.0500)  time: 0.6822  data: 0.0955  max mem: 13008
Epoch: [4]  [3060/3542]  eta: 0:06:16  lr: 0.000025  min_lr: 0.000025  loss: 1.8867 (1.9029)  loss_scale: 8192.0000 (12366.9494)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.0727  max mem: 13008
Epoch: [4]  [3070/3542]  eta: 0:06:08  lr: 0.000025  min_lr: 0.000025  loss: 1.8682 (1.9027)  loss_scale: 8192.0000 (12353.3546)  weight_decay: 0.0500 (0.0500)  time: 0.6847  data: 0.0991  max mem: 13008
Epoch: [4]  [3080/3542]  eta: 0:06:00  lr: 0.000025  min_lr: 0.000025  loss: 1.8271 (1.9026)  loss_scale: 8192.0000 (12339.8481)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.0771  max mem: 13008
Epoch: [4]  [3090/3542]  eta: 0:05:52  lr: 0.000025  min_lr: 0.000025  loss: 1.8223 (1.9024)  loss_scale: 8192.0000 (12326.4290)  weight_decay: 0.0500 (0.0500)  time: 0.6510  data: 0.0651  max mem: 13008
Epoch: [4]  [3100/3542]  eta: 0:05:44  lr: 0.000025  min_lr: 0.000025  loss: 1.8184 (1.9024)  loss_scale: 8192.0000 (12313.0964)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.0959  max mem: 13008
Epoch: [4]  [3110/3542]  eta: 0:05:36  lr: 0.000025  min_lr: 0.000025  loss: 1.9541 (1.9026)  loss_scale: 8192.0000 (12299.8496)  weight_decay: 0.0500 (0.0500)  time: 0.7010  data: 0.1147  max mem: 13008
Epoch: [4]  [3120/3542]  eta: 0:05:28  lr: 0.000025  min_lr: 0.000025  loss: 1.9463 (1.9025)  loss_scale: 8192.0000 (12286.6876)  weight_decay: 0.0500 (0.0500)  time: 0.6879  data: 0.1007  max mem: 13008
Epoch: [4]  [3130/3542]  eta: 0:05:21  lr: 0.000025  min_lr: 0.000025  loss: 1.8691 (1.9025)  loss_scale: 8192.0000 (12273.6097)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.0911  max mem: 13008
Epoch: [4]  [3140/3542]  eta: 0:05:13  lr: 0.000025  min_lr: 0.000025  loss: 1.9365 (1.9027)  loss_scale: 8192.0000 (12260.6151)  weight_decay: 0.0500 (0.0500)  time: 0.6720  data: 0.0860  max mem: 13008
Epoch: [4]  [3150/3542]  eta: 0:05:05  lr: 0.000025  min_lr: 0.000025  loss: 1.9121 (1.9027)  loss_scale: 8192.0000 (12247.7030)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.0480  max mem: 13008
Epoch: [4]  [3160/3542]  eta: 0:04:57  lr: 0.000025  min_lr: 0.000025  loss: 1.8770 (1.9027)  loss_scale: 8192.0000 (12234.8725)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.0644  max mem: 13008
Epoch: [4]  [3170/3542]  eta: 0:04:49  lr: 0.000025  min_lr: 0.000025  loss: 1.8672 (1.9024)  loss_scale: 8192.0000 (12222.1230)  weight_decay: 0.0500 (0.0500)  time: 0.6736  data: 0.0869  max mem: 13008
Epoch: [4]  [3180/3542]  eta: 0:04:41  lr: 0.000025  min_lr: 0.000025  loss: 1.7881 (1.9022)  loss_scale: 8192.0000 (12209.4536)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0789  max mem: 13008
Epoch: [4]  [3190/3542]  eta: 0:04:33  lr: 0.000025  min_lr: 0.000025  loss: 1.8711 (1.9023)  loss_scale: 8192.0000 (12196.8637)  weight_decay: 0.0500 (0.0500)  time: 0.6780  data: 0.0923  max mem: 13008
Epoch: [4]  [3200/3542]  eta: 0:04:25  lr: 0.000025  min_lr: 0.000025  loss: 1.9092 (1.9022)  loss_scale: 8192.0000 (12184.3524)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.0781  max mem: 13008
Epoch: [4]  [3210/3542]  eta: 0:04:17  lr: 0.000025  min_lr: 0.000025  loss: 1.8984 (1.9023)  loss_scale: 8192.0000 (12171.9190)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.0731  max mem: 13008
Epoch: [4]  [3220/3542]  eta: 0:04:09  lr: 0.000024  min_lr: 0.000024  loss: 1.8857 (1.9021)  loss_scale: 8192.0000 (12159.5629)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.0637  max mem: 13008
Epoch: [4]  [3230/3542]  eta: 0:04:01  lr: 0.000024  min_lr: 0.000024  loss: 1.8643 (1.9021)  loss_scale: 8192.0000 (12147.2832)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.0690  max mem: 13008
Epoch: [4]  [3240/3542]  eta: 0:03:54  lr: 0.000024  min_lr: 0.000024  loss: 1.8516 (1.9019)  loss_scale: 8192.0000 (12135.0793)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.0924  max mem: 13008
Epoch: [4]  [3250/3542]  eta: 0:03:46  lr: 0.000024  min_lr: 0.000024  loss: 1.8662 (1.9019)  loss_scale: 8192.0000 (12122.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6712  data: 0.0840  max mem: 13008
Epoch: [4]  [3260/3542]  eta: 0:03:38  lr: 0.000024  min_lr: 0.000024  loss: 1.8682 (1.9017)  loss_scale: 8192.0000 (12110.8960)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.0618  max mem: 13008
Epoch: [4]  [3270/3542]  eta: 0:03:30  lr: 0.000024  min_lr: 0.000024  loss: 1.9463 (1.9020)  loss_scale: 8192.0000 (12098.9153)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.0691  max mem: 13008
Epoch: [4]  [3280/3542]  eta: 0:03:22  lr: 0.000024  min_lr: 0.000024  loss: 1.9463 (1.9020)  loss_scale: 8192.0000 (12087.0076)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.1052  max mem: 13008
Epoch: [4]  [3290/3542]  eta: 0:03:14  lr: 0.000024  min_lr: 0.000024  loss: 1.8623 (1.9018)  loss_scale: 8192.0000 (12075.1723)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.0995  max mem: 13008
Epoch: [4]  [3300/3542]  eta: 0:03:07  lr: 0.000024  min_lr: 0.000024  loss: 1.8750 (1.9018)  loss_scale: 8192.0000 (12063.4087)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.1044  max mem: 13008
Epoch: [4]  [3310/3542]  eta: 0:02:59  lr: 0.000024  min_lr: 0.000024  loss: 1.8955 (1.9018)  loss_scale: 8192.0000 (12051.7161)  weight_decay: 0.0500 (0.0500)  time: 0.7083  data: 0.1223  max mem: 13008
Epoch: [4]  [3320/3542]  eta: 0:02:51  lr: 0.000024  min_lr: 0.000024  loss: 1.8994 (1.9017)  loss_scale: 8192.0000 (12040.0939)  weight_decay: 0.0500 (0.0500)  time: 0.6942  data: 0.1083  max mem: 13008
Epoch: [4]  [3330/3542]  eta: 0:02:43  lr: 0.000024  min_lr: 0.000024  loss: 1.8916 (1.9017)  loss_scale: 8192.0000 (12028.5416)  weight_decay: 0.0500 (0.0500)  time: 0.6794  data: 0.0937  max mem: 13008
Epoch: [4]  [3340/3542]  eta: 0:02:36  lr: 0.000024  min_lr: 0.000024  loss: 1.8564 (1.9017)  loss_scale: 8192.0000 (12017.0584)  weight_decay: 0.0500 (0.0500)  time: 0.6886  data: 0.1024  max mem: 13008
Epoch: [4]  [3350/3542]  eta: 0:02:28  lr: 0.000024  min_lr: 0.000024  loss: 1.8564 (1.9015)  loss_scale: 8192.0000 (12005.6437)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.0612  max mem: 13008
Epoch: [4]  [3360/3542]  eta: 0:02:20  lr: 0.000024  min_lr: 0.000024  loss: 1.8389 (1.9014)  loss_scale: 8192.0000 (11994.2969)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.0301  max mem: 13008
Epoch: [4]  [3370/3542]  eta: 0:02:12  lr: 0.000024  min_lr: 0.000024  loss: 1.8604 (1.9015)  loss_scale: 8192.0000 (11983.0175)  weight_decay: 0.0500 (0.0500)  time: 0.6780  data: 0.0923  max mem: 13008
Epoch: [4]  [3380/3542]  eta: 0:02:04  lr: 0.000024  min_lr: 0.000024  loss: 1.7988 (1.9011)  loss_scale: 8192.0000 (11971.8048)  weight_decay: 0.0500 (0.0500)  time: 0.6547  data: 0.0684  max mem: 13008
Epoch: [4]  [3390/3542]  eta: 0:01:57  lr: 0.000024  min_lr: 0.000024  loss: 1.8213 (1.9010)  loss_scale: 8192.0000 (11960.6582)  weight_decay: 0.0500 (0.0500)  time: 0.6069  data: 0.0204  max mem: 13008
Epoch: [4]  [3400/3542]  eta: 0:01:49  lr: 0.000024  min_lr: 0.000024  loss: 1.8633 (1.9010)  loss_scale: 8192.0000 (11949.5772)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.0575  max mem: 13008
Epoch: [4]  [3410/3542]  eta: 0:01:41  lr: 0.000024  min_lr: 0.000024  loss: 1.9033 (1.9010)  loss_scale: 8192.0000 (11938.5611)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.0689  max mem: 13008
Epoch: [4]  [3420/3542]  eta: 0:01:33  lr: 0.000024  min_lr: 0.000024  loss: 1.8389 (1.9009)  loss_scale: 8192.0000 (11927.6095)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.0797  max mem: 13008
Epoch: [4]  [3430/3542]  eta: 0:01:26  lr: 0.000024  min_lr: 0.000024  loss: 1.7646 (1.9007)  loss_scale: 8192.0000 (11916.7217)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.1031  max mem: 13008
Epoch: [4]  [3440/3542]  eta: 0:01:18  lr: 0.000024  min_lr: 0.000024  loss: 1.8096 (1.9006)  loss_scale: 8192.0000 (11905.8971)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.0783  max mem: 13008
Epoch: [4]  [3450/3542]  eta: 0:01:10  lr: 0.000024  min_lr: 0.000024  loss: 1.8906 (1.9008)  loss_scale: 8192.0000 (11895.1353)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.0589  max mem: 13008
Epoch: [4]  [3460/3542]  eta: 0:01:03  lr: 0.000024  min_lr: 0.000024  loss: 1.9531 (1.9009)  loss_scale: 8192.0000 (11884.4357)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.0818  max mem: 13008
Epoch: [4]  [3470/3542]  eta: 0:00:55  lr: 0.000024  min_lr: 0.000024  loss: 1.9141 (1.9008)  loss_scale: 8192.0000 (11873.7978)  weight_decay: 0.0500 (0.0500)  time: 0.6828  data: 0.0965  max mem: 13008
Epoch: [4]  [3480/3542]  eta: 0:00:47  lr: 0.000024  min_lr: 0.000024  loss: 1.8809 (1.9007)  loss_scale: 8192.0000 (11863.2209)  weight_decay: 0.0500 (0.0500)  time: 0.6655  data: 0.0792  max mem: 13008
Epoch: [4]  [3490/3542]  eta: 0:00:39  lr: 0.000024  min_lr: 0.000024  loss: 1.8633 (1.9007)  loss_scale: 8192.0000 (11852.7047)  weight_decay: 0.0500 (0.0500)  time: 0.6446  data: 0.0582  max mem: 13008
Epoch: [4]  [3500/3542]  eta: 0:00:32  lr: 0.000024  min_lr: 0.000024  loss: 1.8555 (1.9006)  loss_scale: 8192.0000 (11842.2485)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.0473  max mem: 13008
Epoch: [4]  [3510/3542]  eta: 0:00:24  lr: 0.000024  min_lr: 0.000024  loss: 1.8223 (1.9005)  loss_scale: 8192.0000 (11831.8519)  weight_decay: 0.0500 (0.0500)  time: 0.6666  data: 0.0800  max mem: 13008
Epoch: [4]  [3520/3542]  eta: 0:00:16  lr: 0.000024  min_lr: 0.000024  loss: 1.8408 (1.9004)  loss_scale: 8192.0000 (11821.5143)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.0682  max mem: 13008
Epoch: [4]  [3530/3542]  eta: 0:00:09  lr: 0.000024  min_lr: 0.000024  loss: 1.8770 (1.9006)  loss_scale: 8192.0000 (11811.2353)  weight_decay: 0.0500 (0.0500)  time: 0.6307  data: 0.0442  max mem: 13008
Epoch: [4]  [3540/3542]  eta: 0:00:01  lr: 0.000024  min_lr: 0.000024  loss: 1.9707 (1.9008)  loss_scale: 8192.0000 (11801.0144)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.0369  max mem: 13008
Epoch: [4]  [3541/3542]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000024  loss: 1.9707 (1.9008)  loss_scale: 8192.0000 (11799.9955)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.0368  max mem: 13008
Epoch: [4] Total time: 0:45:12 (0.7658 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000024  loss: 1.9707 (1.9008)  loss_scale: 8192.0000 (11799.9955)  weight_decay: 0.0500 (0.0500)
[2023-05-16 09:51:35,407] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-4/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:33:21    time: 12.8302  data: 8.9873  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:10:50    time: 4.4575  data: 0.8172  max mem: 13008
Test:  [ 20/156]  eta: 0:09:10    time: 3.6097  data: 0.0002  max mem: 13008
Test:  [ 30/156]  eta: 0:08:14    time: 3.6269  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 40/156]  eta: 0:07:24    time: 3.6035  data: 0.0002  max mem: 13008
Test:  [ 50/156]  eta: 0:06:43    time: 3.6272  data: 0.0002  max mem: 13008
Test:  [ 60/156]  eta: 0:06:01    time: 3.6380  data: 0.0002  max mem: 13008
Test:  [ 70/156]  eta: 0:05:21    time: 3.5823  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:43    time: 3.5913  data: 0.0002  max mem: 13008
Test:  [ 90/156]  eta: 0:04:05    time: 3.6515  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:27    time: 3.6547  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:50    time: 3.6253  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:13    time: 3.6745  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:36    time: 3.6942  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:00:59    time: 3.6854  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:22    time: 3.6431  data: 0.0002  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.5970  data: 0.0001  max mem: 13008
Test: Total time: 0:09:36 (3.6942 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e4.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e4.json and result file is ./output_freeze/coco_captioning_result_val_e4.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e4.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1168273.79 tokens per second.
PTBTokenizer tokenized 62671 tokens at 432509.16 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 51410, 'reflen': 49391, 'guess': [51410, 46418, 41426, 36434], 'correct': [33422, 17123, 7652, 3124]}
ratio: 1.040877892733473
Bleu_1: 0.650
Bleu_2: 0.490
Bleu_3: 0.354
Bleu_4: 0.248
computing METEOR score...
METEOR: 0.238
computing Rouge score...
ROUGE_L: 0.507
computing CIDEr score...
CIDEr: 0.807
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [20.917 seconds]
SPICE evaluation took: 32.18 s
SPICE: 0.174
Performance of the network on the 5000 val images: 0.8%
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 10:02:08,337] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-best/mp_rank_00_model_states.pt
Max performance: 0.81%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [5]  [   0/3542]  eta: 19:44:26  lr: 0.000024  min_lr: 0.000024  loss: 1.8809 (1.8809)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 20.0640  data: 19.4507  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [5]  [  10/3542]  eta: 3:49:20  lr: 0.000024  min_lr: 0.000024  loss: 1.9219 (1.9122)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 3.8960  data: 3.3097  max mem: 13008
Epoch: [5]  [  20/3542]  eta: 2:47:26  lr: 0.000024  min_lr: 0.000024  loss: 1.9180 (1.9140)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.9919  data: 1.4095  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [5]  [  30/3542]  eta: 2:21:30  lr: 0.000024  min_lr: 0.000024  loss: 1.9004 (1.9077)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6044  data: 1.0235  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [5]  [  40/3542]  eta: 2:06:53  lr: 0.000024  min_lr: 0.000024  loss: 1.7842 (1.8968)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.4616  data: 0.8797  max mem: 13008
Epoch: [5]  [  50/3542]  eta: 1:52:30  lr: 0.000024  min_lr: 0.000024  loss: 1.8408 (1.8904)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1825  data: 0.6001  max mem: 13008
Epoch: [5]  [  60/3542]  eta: 1:44:08  lr: 0.000024  min_lr: 0.000024  loss: 1.8652 (1.8967)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0169  data: 0.4347  max mem: 13008
Epoch: [5]  [  70/3542]  eta: 1:36:32  lr: 0.000024  min_lr: 0.000024  loss: 1.7852 (1.8815)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9933  data: 0.4104  max mem: 13008
Epoch: [5]  [  80/3542]  eta: 1:31:15  lr: 0.000024  min_lr: 0.000024  loss: 1.7705 (1.8740)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9318  data: 0.3492  max mem: 13008
Epoch: [5]  [  90/3542]  eta: 1:26:57  lr: 0.000024  min_lr: 0.000024  loss: 1.8359 (1.8754)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9536  data: 0.3678  max mem: 13008
Epoch: [5]  [ 100/3542]  eta: 1:22:03  lr: 0.000024  min_lr: 0.000024  loss: 1.8730 (1.8757)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8175  data: 0.2306  max mem: 13008
Epoch: [5]  [ 110/3542]  eta: 1:18:30  lr: 0.000024  min_lr: 0.000024  loss: 1.8730 (1.8810)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7411  data: 0.1567  max mem: 13008
Epoch: [5]  [ 120/3542]  eta: 1:15:30  lr: 0.000024  min_lr: 0.000024  loss: 1.8730 (1.8820)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7864  data: 0.2023  max mem: 13008
Epoch: [5]  [ 130/3542]  eta: 1:12:32  lr: 0.000024  min_lr: 0.000024  loss: 1.7939 (1.8777)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7388  data: 0.1550  max mem: 13008
Epoch: [5]  [ 140/3542]  eta: 1:10:21  lr: 0.000024  min_lr: 0.000024  loss: 1.8301 (1.8789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7398  data: 0.1560  max mem: 13008
Epoch: [5]  [ 150/3542]  eta: 1:08:09  lr: 0.000024  min_lr: 0.000024  loss: 1.8047 (1.8734)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7452  data: 0.1617  max mem: 13008
Epoch: [5]  [ 160/3542]  eta: 1:06:06  lr: 0.000024  min_lr: 0.000024  loss: 1.8047 (1.8748)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6929  data: 0.1093  max mem: 13008
Epoch: [5]  [ 170/3542]  eta: 1:04:14  lr: 0.000024  min_lr: 0.000024  loss: 1.8438 (1.8712)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6708  data: 0.0864  max mem: 13008
Epoch: [5]  [ 180/3542]  eta: 1:02:32  lr: 0.000024  min_lr: 0.000024  loss: 1.8779 (1.8763)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6582  data: 0.0731  max mem: 13008
Epoch: [5]  [ 190/3542]  eta: 1:01:20  lr: 0.000024  min_lr: 0.000024  loss: 1.9355 (1.8789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7136  data: 0.1279  max mem: 13008
Epoch: [5]  [ 200/3542]  eta: 0:59:46  lr: 0.000024  min_lr: 0.000024  loss: 1.9219 (1.8794)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6841  data: 0.0991  max mem: 13008
Epoch: [5]  [ 210/3542]  eta: 0:58:50  lr: 0.000023  min_lr: 0.000023  loss: 1.9121 (1.8822)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6937  data: 0.1088  max mem: 13008
Epoch: [5]  [ 220/3542]  eta: 0:57:48  lr: 0.000023  min_lr: 0.000023  loss: 1.9014 (1.8848)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7515  data: 0.1661  max mem: 13008
Epoch: [5]  [ 230/3542]  eta: 0:56:46  lr: 0.000023  min_lr: 0.000023  loss: 1.8408 (1.8839)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7009  data: 0.1165  max mem: 13008
Epoch: [5]  [ 240/3542]  eta: 0:55:40  lr: 0.000023  min_lr: 0.000023  loss: 1.8408 (1.8812)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.0694  max mem: 13008
Epoch: [5]  [ 250/3542]  eta: 0:54:46  lr: 0.000023  min_lr: 0.000023  loss: 1.8594 (1.8818)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6481  data: 0.0628  max mem: 13008
Epoch: [5]  [ 260/3542]  eta: 0:53:50  lr: 0.000023  min_lr: 0.000023  loss: 1.8906 (1.8809)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6553  data: 0.0703  max mem: 13008
Epoch: [5]  [ 270/3542]  eta: 0:53:00  lr: 0.000023  min_lr: 0.000023  loss: 1.8945 (1.8794)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.0547  max mem: 13008
Epoch: [5]  [ 280/3542]  eta: 0:52:16  lr: 0.000023  min_lr: 0.000023  loss: 1.8311 (1.8795)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0780  max mem: 13008
[2023-05-16 10:06:45,512] [INFO] [logging.py:60:log_dist] [Rank 0] step=18000, skipped=18, lr=[2.3337896306854258e-05, 2.3337896306854258e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 10:06:45,515] [INFO] [timer.py:157:stop] 0/18000, SamplesPerSec=55.54980175119669
Epoch: [5]  [ 290/3542]  eta: 0:51:33  lr: 0.000023  min_lr: 0.000023  loss: 1.8311 (1.8820)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0873  max mem: 13008
Epoch: [5]  [ 300/3542]  eta: 0:50:53  lr: 0.000023  min_lr: 0.000023  loss: 1.9316 (1.8839)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6656  data: 0.0806  max mem: 13008
Epoch: [5]  [ 310/3542]  eta: 0:50:16  lr: 0.000023  min_lr: 0.000023  loss: 1.8848 (1.8814)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6708  data: 0.0856  max mem: 13008
Epoch: [5]  [ 320/3542]  eta: 0:49:36  lr: 0.000023  min_lr: 0.000023  loss: 1.7490 (1.8790)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6524  data: 0.0671  max mem: 13008
Epoch: [5]  [ 330/3542]  eta: 0:49:05  lr: 0.000023  min_lr: 0.000023  loss: 1.8555 (1.8779)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.0787  max mem: 13008
Epoch: [5]  [ 340/3542]  eta: 0:48:35  lr: 0.000023  min_lr: 0.000023  loss: 1.8604 (1.8776)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.1093  max mem: 13008
Epoch: [5]  [ 350/3542]  eta: 0:48:05  lr: 0.000023  min_lr: 0.000023  loss: 1.8955 (1.8786)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6864  data: 0.1004  max mem: 13008
Epoch: [5]  [ 360/3542]  eta: 0:47:34  lr: 0.000023  min_lr: 0.000023  loss: 1.8828 (1.8787)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0844  max mem: 13008
Epoch: [5]  [ 370/3542]  eta: 0:47:00  lr: 0.000023  min_lr: 0.000023  loss: 1.8623 (1.8787)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.0445  max mem: 13008
Epoch: [5]  [ 380/3542]  eta: 0:46:30  lr: 0.000023  min_lr: 0.000023  loss: 1.8760 (1.8800)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6197  data: 0.0327  max mem: 13008
Epoch: [5]  [ 390/3542]  eta: 0:46:03  lr: 0.000023  min_lr: 0.000023  loss: 1.8701 (1.8795)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0549  max mem: 13008
Epoch: [5]  [ 400/3542]  eta: 0:45:34  lr: 0.000023  min_lr: 0.000023  loss: 1.8154 (1.8774)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.0498  max mem: 13008
Epoch: [5]  [ 410/3542]  eta: 0:45:13  lr: 0.000023  min_lr: 0.000023  loss: 1.8203 (1.8789)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.0808  max mem: 13008
Epoch: [5]  [ 420/3542]  eta: 0:44:50  lr: 0.000023  min_lr: 0.000023  loss: 1.8477 (1.8788)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.1051  max mem: 13008
Epoch: [5]  [ 430/3542]  eta: 0:44:27  lr: 0.000023  min_lr: 0.000023  loss: 1.8818 (1.8798)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6666  data: 0.0811  max mem: 13008
[2023-05-16 10:08:23,593] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 10:08:23,593] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [5]  [ 440/3542]  eta: 0:44:04  lr: 0.000023  min_lr: 0.000023  loss: 1.8828 (1.8828)  loss_scale: 8192.0000 (8266.3039)  weight_decay: 0.0500 (0.0500)  time: 0.6559  data: 0.0701  max mem: 13008
Epoch: [5]  [ 450/3542]  eta: 0:43:40  lr: 0.000023  min_lr: 0.000023  loss: 1.8721 (1.8820)  loss_scale: 16384.0000 (8446.2971)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.0552  max mem: 13008
Epoch: [5]  [ 460/3542]  eta: 0:43:15  lr: 0.000023  min_lr: 0.000023  loss: 1.9385 (1.8842)  loss_scale: 16384.0000 (8618.4816)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0302  max mem: 13008
Epoch: [5]  [ 470/3542]  eta: 0:42:56  lr: 0.000023  min_lr: 0.000023  loss: 1.9873 (1.8858)  loss_scale: 16384.0000 (8783.3546)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0533  max mem: 13008
Epoch: [5]  [ 480/3542]  eta: 0:42:39  lr: 0.000023  min_lr: 0.000023  loss: 1.9121 (1.8861)  loss_scale: 16384.0000 (8941.3721)  weight_decay: 0.0500 (0.0500)  time: 0.6912  data: 0.1043  max mem: 13008
Epoch: [5]  [ 490/3542]  eta: 0:42:22  lr: 0.000023  min_lr: 0.000023  loss: 1.9121 (1.8871)  loss_scale: 16384.0000 (9092.9532)  weight_decay: 0.0500 (0.0500)  time: 0.6982  data: 0.1120  max mem: 13008
Epoch: [5]  [ 500/3542]  eta: 0:42:05  lr: 0.000023  min_lr: 0.000023  loss: 1.9121 (1.8874)  loss_scale: 16384.0000 (9238.4830)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.1054  max mem: 13008
Epoch: [5]  [ 510/3542]  eta: 0:41:46  lr: 0.000023  min_lr: 0.000023  loss: 1.9121 (1.8887)  loss_scale: 16384.0000 (9378.3170)  weight_decay: 0.0500 (0.0500)  time: 0.6738  data: 0.0885  max mem: 13008
Epoch: [5]  [ 520/3542]  eta: 0:41:32  lr: 0.000023  min_lr: 0.000023  loss: 1.9600 (1.8887)  loss_scale: 16384.0000 (9512.7831)  weight_decay: 0.0500 (0.0500)  time: 0.6861  data: 0.1016  max mem: 13008
Epoch: [5]  [ 530/3542]  eta: 0:41:16  lr: 0.000023  min_lr: 0.000023  loss: 1.8604 (1.8879)  loss_scale: 16384.0000 (9642.1846)  weight_decay: 0.0500 (0.0500)  time: 0.7012  data: 0.1157  max mem: 13008
Epoch: [5]  [ 540/3542]  eta: 0:41:04  lr: 0.000023  min_lr: 0.000023  loss: 1.8242 (1.8864)  loss_scale: 16384.0000 (9766.8022)  weight_decay: 0.0500 (0.0500)  time: 0.7243  data: 0.1383  max mem: 13008
Epoch: [5]  [ 550/3542]  eta: 0:40:51  lr: 0.000023  min_lr: 0.000023  loss: 1.8418 (1.8864)  loss_scale: 16384.0000 (9886.8966)  weight_decay: 0.0500 (0.0500)  time: 0.7458  data: 0.1595  max mem: 13008
Epoch: [5]  [ 560/3542]  eta: 0:40:33  lr: 0.000023  min_lr: 0.000023  loss: 1.8535 (1.8861)  loss_scale: 16384.0000 (10002.7094)  weight_decay: 0.0500 (0.0500)  time: 0.6862  data: 0.1001  max mem: 13008
Epoch: [5]  [ 570/3542]  eta: 0:40:18  lr: 0.000023  min_lr: 0.000023  loss: 1.8330 (1.8858)  loss_scale: 16384.0000 (10114.4658)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0741  max mem: 13008
Epoch: [5]  [ 580/3542]  eta: 0:40:02  lr: 0.000023  min_lr: 0.000023  loss: 1.8701 (1.8856)  loss_scale: 16384.0000 (10222.3752)  weight_decay: 0.0500 (0.0500)  time: 0.6674  data: 0.0802  max mem: 13008
Epoch: [5]  [ 590/3542]  eta: 0:39:48  lr: 0.000023  min_lr: 0.000023  loss: 1.8779 (1.8857)  loss_scale: 16384.0000 (10326.6328)  weight_decay: 0.0500 (0.0500)  time: 0.6728  data: 0.0859  max mem: 13008
Epoch: [5]  [ 600/3542]  eta: 0:39:36  lr: 0.000023  min_lr: 0.000023  loss: 1.8672 (1.8846)  loss_scale: 16384.0000 (10427.4210)  weight_decay: 0.0500 (0.0500)  time: 0.7092  data: 0.1233  max mem: 13008
Epoch: [5]  [ 610/3542]  eta: 0:39:22  lr: 0.000023  min_lr: 0.000023  loss: 1.8418 (1.8851)  loss_scale: 16384.0000 (10524.9100)  weight_decay: 0.0500 (0.0500)  time: 0.7055  data: 0.1205  max mem: 13008
Epoch: [5]  [ 620/3542]  eta: 0:39:06  lr: 0.000023  min_lr: 0.000023  loss: 1.8438 (1.8852)  loss_scale: 16384.0000 (10619.2593)  weight_decay: 0.0500 (0.0500)  time: 0.6628  data: 0.0781  max mem: 13008
Epoch: [5]  [ 630/3542]  eta: 0:38:54  lr: 0.000023  min_lr: 0.000023  loss: 1.8438 (1.8852)  loss_scale: 16384.0000 (10710.6181)  weight_decay: 0.0500 (0.0500)  time: 0.6800  data: 0.0949  max mem: 13008
Epoch: [5]  [ 640/3542]  eta: 0:38:39  lr: 0.000023  min_lr: 0.000023  loss: 1.8184 (1.8839)  loss_scale: 16384.0000 (10799.1264)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.0964  max mem: 13008
Epoch: [5]  [ 650/3542]  eta: 0:38:26  lr: 0.000023  min_lr: 0.000023  loss: 1.8721 (1.8851)  loss_scale: 16384.0000 (10884.9155)  weight_decay: 0.0500 (0.0500)  time: 0.6675  data: 0.0813  max mem: 13008
Epoch: [5]  [ 660/3542]  eta: 0:38:13  lr: 0.000023  min_lr: 0.000023  loss: 1.8711 (1.8842)  loss_scale: 16384.0000 (10968.1089)  weight_decay: 0.0500 (0.0500)  time: 0.6870  data: 0.1002  max mem: 13008
Epoch: [5]  [ 670/3542]  eta: 0:38:03  lr: 0.000023  min_lr: 0.000023  loss: 1.8711 (1.8846)  loss_scale: 16384.0000 (11048.8227)  weight_decay: 0.0500 (0.0500)  time: 0.7220  data: 0.1352  max mem: 13008
Epoch: [5]  [ 680/3542]  eta: 0:37:49  lr: 0.000023  min_lr: 0.000023  loss: 1.8506 (1.8841)  loss_scale: 16384.0000 (11127.1659)  weight_decay: 0.0500 (0.0500)  time: 0.6982  data: 0.1116  max mem: 13008
Epoch: [5]  [ 690/3542]  eta: 0:37:34  lr: 0.000023  min_lr: 0.000023  loss: 1.8506 (1.8840)  loss_scale: 16384.0000 (11203.2417)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.0449  max mem: 13008
Epoch: [5]  [ 700/3542]  eta: 0:37:21  lr: 0.000023  min_lr: 0.000023  loss: 1.8877 (1.8840)  loss_scale: 16384.0000 (11277.1469)  weight_decay: 0.0500 (0.0500)  time: 0.6497  data: 0.0646  max mem: 13008
Epoch: [5]  [ 710/3542]  eta: 0:37:09  lr: 0.000023  min_lr: 0.000023  loss: 1.9189 (1.8845)  loss_scale: 16384.0000 (11348.9733)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.0898  max mem: 13008
Epoch: [5]  [ 720/3542]  eta: 0:36:57  lr: 0.000023  min_lr: 0.000023  loss: 1.8066 (1.8830)  loss_scale: 16384.0000 (11418.8072)  weight_decay: 0.0500 (0.0500)  time: 0.6832  data: 0.0976  max mem: 13008
Epoch: [5]  [ 730/3542]  eta: 0:36:46  lr: 0.000022  min_lr: 0.000022  loss: 1.7891 (1.8814)  loss_scale: 16384.0000 (11486.7305)  weight_decay: 0.0500 (0.0500)  time: 0.6978  data: 0.1126  max mem: 13008
Epoch: [5]  [ 740/3542]  eta: 0:36:34  lr: 0.000022  min_lr: 0.000022  loss: 1.8203 (1.8817)  loss_scale: 16384.0000 (11552.8205)  weight_decay: 0.0500 (0.0500)  time: 0.6830  data: 0.0968  max mem: 13008
Epoch: [5]  [ 750/3542]  eta: 0:36:23  lr: 0.000022  min_lr: 0.000022  loss: 1.8857 (1.8824)  loss_scale: 16384.0000 (11617.1505)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.0968  max mem: 13008
Epoch: [5]  [ 760/3542]  eta: 0:36:12  lr: 0.000022  min_lr: 0.000022  loss: 1.8643 (1.8824)  loss_scale: 16384.0000 (11679.7898)  weight_decay: 0.0500 (0.0500)  time: 0.6924  data: 0.1061  max mem: 13008
Epoch: [5]  [ 770/3542]  eta: 0:36:00  lr: 0.000022  min_lr: 0.000022  loss: 1.8779 (1.8825)  loss_scale: 16384.0000 (11740.8042)  weight_decay: 0.0500 (0.0500)  time: 0.6750  data: 0.0886  max mem: 13008
Epoch: [5]  [ 780/3542]  eta: 0:35:53  lr: 0.000022  min_lr: 0.000022  loss: 1.8779 (1.8818)  loss_scale: 16384.0000 (11800.2561)  weight_decay: 0.0500 (0.0500)  time: 0.7349  data: 0.1478  max mem: 13008
Epoch: [5]  [ 790/3542]  eta: 0:35:40  lr: 0.000022  min_lr: 0.000022  loss: 1.8018 (1.8815)  loss_scale: 16384.0000 (11858.2048)  weight_decay: 0.0500 (0.0500)  time: 0.7254  data: 0.1383  max mem: 13008
Epoch: [5]  [ 800/3542]  eta: 0:35:38  lr: 0.000022  min_lr: 0.000022  loss: 1.7773 (1.8803)  loss_scale: 16384.0000 (11914.7066)  weight_decay: 0.0500 (0.0500)  time: 0.7934  data: 0.2067  max mem: 13008
Epoch: [5]  [ 810/3542]  eta: 0:35:30  lr: 0.000022  min_lr: 0.000022  loss: 1.8545 (1.8811)  loss_scale: 16384.0000 (11969.8150)  weight_decay: 0.0500 (0.0500)  time: 0.8528  data: 0.2669  max mem: 13008
Epoch: [5]  [ 820/3542]  eta: 0:35:37  lr: 0.000022  min_lr: 0.000022  loss: 1.9355 (1.8815)  loss_scale: 16384.0000 (12023.5810)  weight_decay: 0.0500 (0.0500)  time: 0.9939  data: 0.4090  max mem: 13008
Epoch: [5]  [ 830/3542]  eta: 0:35:39  lr: 0.000022  min_lr: 0.000022  loss: 1.8643 (1.8811)  loss_scale: 16384.0000 (12076.0529)  weight_decay: 0.0500 (0.0500)  time: 1.1606  data: 0.5759  max mem: 13008
Epoch: [5]  [ 840/3542]  eta: 0:35:40  lr: 0.000022  min_lr: 0.000022  loss: 1.9277 (1.8823)  loss_scale: 16384.0000 (12127.2771)  weight_decay: 0.0500 (0.0500)  time: 1.0819  data: 0.4980  max mem: 13008
Epoch: [5]  [ 850/3542]  eta: 0:35:43  lr: 0.000022  min_lr: 0.000022  loss: 1.9248 (1.8820)  loss_scale: 16384.0000 (12177.2973)  weight_decay: 0.0500 (0.0500)  time: 1.1018  data: 0.5157  max mem: 13008
Epoch: [5]  [ 860/3542]  eta: 0:35:40  lr: 0.000022  min_lr: 0.000022  loss: 1.7988 (1.8815)  loss_scale: 16384.0000 (12226.1556)  weight_decay: 0.0500 (0.0500)  time: 1.0451  data: 0.4593  max mem: 13008
Epoch: [5]  [ 870/3542]  eta: 0:35:52  lr: 0.000022  min_lr: 0.000022  loss: 1.8145 (1.8811)  loss_scale: 16384.0000 (12273.8921)  weight_decay: 0.0500 (0.0500)  time: 1.1924  data: 0.6095  max mem: 13008
Epoch: [5]  [ 880/3542]  eta: 0:36:07  lr: 0.000022  min_lr: 0.000022  loss: 1.7734 (1.8798)  loss_scale: 16384.0000 (12320.5448)  weight_decay: 0.0500 (0.0500)  time: 1.5043  data: 0.9220  max mem: 13008
Epoch: [5]  [ 890/3542]  eta: 0:36:16  lr: 0.000022  min_lr: 0.000022  loss: 1.7734 (1.8798)  loss_scale: 16384.0000 (12366.1504)  weight_decay: 0.0500 (0.0500)  time: 1.4835  data: 0.8995  max mem: 13008
Epoch: [5]  [ 900/3542]  eta: 0:36:19  lr: 0.000022  min_lr: 0.000022  loss: 1.8408 (1.8791)  loss_scale: 16384.0000 (12410.7436)  weight_decay: 0.0500 (0.0500)  time: 1.3062  data: 0.7223  max mem: 13008
Epoch: [5]  [ 910/3542]  eta: 0:36:19  lr: 0.000022  min_lr: 0.000022  loss: 1.9082 (1.8804)  loss_scale: 16384.0000 (12454.3578)  weight_decay: 0.0500 (0.0500)  time: 1.1552  data: 0.5733  max mem: 13008
Epoch: [5]  [ 920/3542]  eta: 0:36:17  lr: 0.000022  min_lr: 0.000022  loss: 1.9336 (1.8804)  loss_scale: 16384.0000 (12497.0250)  weight_decay: 0.0500 (0.0500)  time: 1.0782  data: 0.4964  max mem: 13008
Epoch: [5]  [ 930/3542]  eta: 0:36:10  lr: 0.000022  min_lr: 0.000022  loss: 1.8301 (1.8804)  loss_scale: 16384.0000 (12538.7755)  weight_decay: 0.0500 (0.0500)  time: 0.9689  data: 0.3847  max mem: 13008
Epoch: [5]  [ 940/3542]  eta: 0:36:08  lr: 0.000022  min_lr: 0.000022  loss: 1.8154 (1.8799)  loss_scale: 16384.0000 (12579.6387)  weight_decay: 0.0500 (0.0500)  time: 0.9575  data: 0.3733  max mem: 13008
Epoch: [5]  [ 950/3542]  eta: 0:36:05  lr: 0.000022  min_lr: 0.000022  loss: 1.8203 (1.8798)  loss_scale: 16384.0000 (12619.6425)  weight_decay: 0.0500 (0.0500)  time: 1.0366  data: 0.4542  max mem: 13008
Epoch: [5]  [ 960/3542]  eta: 0:35:59  lr: 0.000022  min_lr: 0.000022  loss: 1.8604 (1.8802)  loss_scale: 16384.0000 (12658.8137)  weight_decay: 0.0500 (0.0500)  time: 0.9838  data: 0.4004  max mem: 13008
Epoch: [5]  [ 970/3542]  eta: 0:35:51  lr: 0.000022  min_lr: 0.000022  loss: 1.8252 (1.8796)  loss_scale: 16384.0000 (12697.1782)  weight_decay: 0.0500 (0.0500)  time: 0.8872  data: 0.3036  max mem: 13008
Epoch: [5]  [ 980/3542]  eta: 0:35:51  lr: 0.000022  min_lr: 0.000022  loss: 1.8223 (1.8796)  loss_scale: 16384.0000 (12734.7604)  weight_decay: 0.0500 (0.0500)  time: 0.9942  data: 0.4094  max mem: 13008
Epoch: [5]  [ 990/3542]  eta: 0:35:49  lr: 0.000022  min_lr: 0.000022  loss: 1.8398 (1.8791)  loss_scale: 16384.0000 (12771.5843)  weight_decay: 0.0500 (0.0500)  time: 1.1207  data: 0.5368  max mem: 13008
Epoch: [5]  [1000/3542]  eta: 0:35:53  lr: 0.000022  min_lr: 0.000022  loss: 1.8398 (1.8793)  loss_scale: 16384.0000 (12807.6723)  weight_decay: 0.0500 (0.0500)  time: 1.2101  data: 0.6279  max mem: 13008
Epoch: [5]  [1010/3542]  eta: 0:35:51  lr: 0.000022  min_lr: 0.000022  loss: 1.9160 (1.8792)  loss_scale: 16384.0000 (12843.0465)  weight_decay: 0.0500 (0.0500)  time: 1.2267  data: 0.6442  max mem: 13008
Epoch: [5]  [1020/3542]  eta: 0:35:49  lr: 0.000022  min_lr: 0.000022  loss: 1.9141 (1.8791)  loss_scale: 16384.0000 (12877.7277)  weight_decay: 0.0500 (0.0500)  time: 1.1227  data: 0.5388  max mem: 13008
Epoch: [5]  [1030/3542]  eta: 0:35:48  lr: 0.000022  min_lr: 0.000022  loss: 1.9141 (1.8796)  loss_scale: 16384.0000 (12911.7362)  weight_decay: 0.0500 (0.0500)  time: 1.1429  data: 0.5590  max mem: 13008
Epoch: [5]  [1040/3542]  eta: 0:35:43  lr: 0.000022  min_lr: 0.000022  loss: 1.8809 (1.8796)  loss_scale: 16384.0000 (12945.0913)  weight_decay: 0.0500 (0.0500)  time: 1.0696  data: 0.4867  max mem: 13008
Epoch: [5]  [1050/3542]  eta: 0:35:37  lr: 0.000022  min_lr: 0.000022  loss: 1.8359 (1.8794)  loss_scale: 16384.0000 (12977.8116)  weight_decay: 0.0500 (0.0500)  time: 0.9736  data: 0.3909  max mem: 13008
[2023-05-16 10:17:21,509] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 18770
[2023-05-16 10:17:21,510] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 10:17:21,510] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [5]  [1060/3542]  eta: 0:35:31  lr: 0.000022  min_lr: 0.000022  loss: 1.8252 (1.8794)  loss_scale: 16384.0000 (13002.1942)  weight_decay: 0.0500 (0.0500)  time: 0.9836  data: 0.4016  max mem: 13008
Epoch: [5]  [1070/3542]  eta: 0:35:25  lr: 0.000022  min_lr: 0.000022  loss: 1.8691 (1.8800)  loss_scale: 8192.0000 (12957.2810)  weight_decay: 0.0500 (0.0500)  time: 0.9726  data: 0.3903  max mem: 13008
Epoch: [5]  [1080/3542]  eta: 0:35:14  lr: 0.000022  min_lr: 0.000022  loss: 1.8809 (1.8798)  loss_scale: 8192.0000 (12913.1989)  weight_decay: 0.0500 (0.0500)  time: 0.8479  data: 0.2656  max mem: 13008
Epoch: [5]  [1090/3542]  eta: 0:35:04  lr: 0.000022  min_lr: 0.000022  loss: 1.9014 (1.8807)  loss_scale: 8192.0000 (12869.9248)  weight_decay: 0.0500 (0.0500)  time: 0.7773  data: 0.1949  max mem: 13008
Epoch: [5]  [1100/3542]  eta: 0:34:56  lr: 0.000022  min_lr: 0.000022  loss: 1.9482 (1.8811)  loss_scale: 8192.0000 (12827.4369)  weight_decay: 0.0500 (0.0500)  time: 0.8499  data: 0.2670  max mem: 13008
Epoch: [5]  [1110/3542]  eta: 0:34:44  lr: 0.000022  min_lr: 0.000022  loss: 1.8994 (1.8815)  loss_scale: 8192.0000 (12785.7138)  weight_decay: 0.0500 (0.0500)  time: 0.7930  data: 0.2103  max mem: 13008
Epoch: [5]  [1120/3542]  eta: 0:34:34  lr: 0.000022  min_lr: 0.000022  loss: 1.8799 (1.8812)  loss_scale: 8192.0000 (12744.7351)  weight_decay: 0.0500 (0.0500)  time: 0.7473  data: 0.1640  max mem: 13008
Epoch: [5]  [1130/3542]  eta: 0:34:24  lr: 0.000022  min_lr: 0.000022  loss: 1.8555 (1.8808)  loss_scale: 8192.0000 (12704.4810)  weight_decay: 0.0500 (0.0500)  time: 0.7930  data: 0.2094  max mem: 13008
Epoch: [5]  [1140/3542]  eta: 0:34:11  lr: 0.000022  min_lr: 0.000022  loss: 1.8730 (1.8812)  loss_scale: 8192.0000 (12664.9325)  weight_decay: 0.0500 (0.0500)  time: 0.7231  data: 0.1396  max mem: 13008
Epoch: [5]  [1150/3542]  eta: 0:34:00  lr: 0.000022  min_lr: 0.000022  loss: 1.9307 (1.8811)  loss_scale: 8192.0000 (12626.0712)  weight_decay: 0.0500 (0.0500)  time: 0.6819  data: 0.0980  max mem: 13008
Epoch: [5]  [1160/3542]  eta: 0:33:48  lr: 0.000022  min_lr: 0.000022  loss: 1.8301 (1.8809)  loss_scale: 8192.0000 (12587.8794)  weight_decay: 0.0500 (0.0500)  time: 0.7109  data: 0.1271  max mem: 13008
Epoch: [5]  [1170/3542]  eta: 0:33:39  lr: 0.000022  min_lr: 0.000022  loss: 1.8330 (1.8805)  loss_scale: 8192.0000 (12550.3399)  weight_decay: 0.0500 (0.0500)  time: 0.7524  data: 0.1681  max mem: 13008
Epoch: [5]  [1180/3542]  eta: 0:33:27  lr: 0.000022  min_lr: 0.000022  loss: 1.8643 (1.8807)  loss_scale: 8192.0000 (12513.4361)  weight_decay: 0.0500 (0.0500)  time: 0.7526  data: 0.1676  max mem: 13008
Epoch: [5]  [1190/3542]  eta: 0:33:16  lr: 0.000022  min_lr: 0.000022  loss: 1.9072 (1.8815)  loss_scale: 8192.0000 (12477.1520)  weight_decay: 0.0500 (0.0500)  time: 0.7123  data: 0.1269  max mem: 13008
Epoch: [5]  [1200/3542]  eta: 0:33:06  lr: 0.000022  min_lr: 0.000022  loss: 1.8691 (1.8812)  loss_scale: 8192.0000 (12441.4721)  weight_decay: 0.0500 (0.0500)  time: 0.7286  data: 0.1437  max mem: 13008
Epoch: [5]  [1210/3542]  eta: 0:32:54  lr: 0.000022  min_lr: 0.000022  loss: 1.8691 (1.8817)  loss_scale: 8192.0000 (12406.3815)  weight_decay: 0.0500 (0.0500)  time: 0.7220  data: 0.1372  max mem: 13008
Epoch: [5]  [1220/3542]  eta: 0:32:44  lr: 0.000022  min_lr: 0.000022  loss: 1.9219 (1.8825)  loss_scale: 8192.0000 (12371.8657)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.1338  max mem: 13008
Epoch: [5]  [1230/3542]  eta: 0:32:33  lr: 0.000022  min_lr: 0.000022  loss: 1.9111 (1.8825)  loss_scale: 8192.0000 (12337.9106)  weight_decay: 0.0500 (0.0500)  time: 0.7314  data: 0.1465  max mem: 13008
Epoch: [5]  [1240/3542]  eta: 0:32:22  lr: 0.000022  min_lr: 0.000022  loss: 1.9111 (1.8829)  loss_scale: 8192.0000 (12304.5028)  weight_decay: 0.0500 (0.0500)  time: 0.7053  data: 0.1202  max mem: 13008
Epoch: [5]  [1250/3542]  eta: 0:32:13  lr: 0.000022  min_lr: 0.000022  loss: 1.8848 (1.8828)  loss_scale: 8192.0000 (12271.6291)  weight_decay: 0.0500 (0.0500)  time: 0.7509  data: 0.1659  max mem: 13008
Epoch: [5]  [1260/3542]  eta: 0:32:03  lr: 0.000021  min_lr: 0.000021  loss: 1.9121 (1.8830)  loss_scale: 8192.0000 (12239.2768)  weight_decay: 0.0500 (0.0500)  time: 0.7830  data: 0.1987  max mem: 13008
Epoch: [5]  [1270/3542]  eta: 0:31:52  lr: 0.000021  min_lr: 0.000021  loss: 1.8613 (1.8827)  loss_scale: 8192.0000 (12207.4335)  weight_decay: 0.0500 (0.0500)  time: 0.7457  data: 0.1610  max mem: 13008
Epoch: [5]  [1280/3542]  eta: 0:31:42  lr: 0.000021  min_lr: 0.000021  loss: 1.8301 (1.8822)  loss_scale: 8192.0000 (12176.0874)  weight_decay: 0.0500 (0.0500)  time: 0.7249  data: 0.1399  max mem: 13008
[2023-05-16 10:20:13,619] [INFO] [logging.py:60:log_dist] [Rank 0] step=19000, skipped=19, lr=[2.1425927796097307e-05, 2.1425927796097307e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 10:20:13,622] [INFO] [timer.py:157:stop] 0/19000, SamplesPerSec=55.553557234289755
Epoch: [5]  [1290/3542]  eta: 0:31:34  lr: 0.000021  min_lr: 0.000021  loss: 1.8438 (1.8820)  loss_scale: 8192.0000 (12145.2270)  weight_decay: 0.0500 (0.0500)  time: 0.7958  data: 0.2116  max mem: 13008
Epoch: [5]  [1300/3542]  eta: 0:31:24  lr: 0.000021  min_lr: 0.000021  loss: 1.8438 (1.8822)  loss_scale: 8192.0000 (12114.8409)  weight_decay: 0.0500 (0.0500)  time: 0.8283  data: 0.2444  max mem: 13008
Epoch: [5]  [1310/3542]  eta: 0:31:13  lr: 0.000021  min_lr: 0.000021  loss: 1.9072 (1.8817)  loss_scale: 8192.0000 (12084.9184)  weight_decay: 0.0500 (0.0500)  time: 0.7310  data: 0.1466  max mem: 13008
Epoch: [5]  [1320/3542]  eta: 0:31:05  lr: 0.000021  min_lr: 0.000021  loss: 1.8271 (1.8814)  loss_scale: 8192.0000 (12055.4489)  weight_decay: 0.0500 (0.0500)  time: 0.7600  data: 0.1759  max mem: 13008
Epoch: [5]  [1330/3542]  eta: 0:30:56  lr: 0.000021  min_lr: 0.000021  loss: 1.8623 (1.8818)  loss_scale: 8192.0000 (12026.4222)  weight_decay: 0.0500 (0.0500)  time: 0.8301  data: 0.2465  max mem: 13008
Epoch: [5]  [1340/3542]  eta: 0:30:47  lr: 0.000021  min_lr: 0.000021  loss: 1.9473 (1.8822)  loss_scale: 8192.0000 (11997.8285)  weight_decay: 0.0500 (0.0500)  time: 0.8112  data: 0.2266  max mem: 13008
Epoch: [5]  [1350/3542]  eta: 0:30:38  lr: 0.000021  min_lr: 0.000021  loss: 1.9238 (1.8820)  loss_scale: 8192.0000 (11969.6580)  weight_decay: 0.0500 (0.0500)  time: 0.7972  data: 0.2121  max mem: 13008
Epoch: [5]  [1360/3542]  eta: 0:30:29  lr: 0.000021  min_lr: 0.000021  loss: 1.8428 (1.8820)  loss_scale: 8192.0000 (11941.9015)  weight_decay: 0.0500 (0.0500)  time: 0.7831  data: 0.1984  max mem: 13008
Epoch: [5]  [1370/3542]  eta: 0:30:19  lr: 0.000021  min_lr: 0.000021  loss: 1.8428 (1.8820)  loss_scale: 8192.0000 (11914.5500)  weight_decay: 0.0500 (0.0500)  time: 0.7724  data: 0.1873  max mem: 13008
Epoch: [5]  [1380/3542]  eta: 0:30:10  lr: 0.000021  min_lr: 0.000021  loss: 1.7715 (1.8811)  loss_scale: 8192.0000 (11887.5945)  weight_decay: 0.0500 (0.0500)  time: 0.7803  data: 0.1958  max mem: 13008
Epoch: [5]  [1390/3542]  eta: 0:30:02  lr: 0.000021  min_lr: 0.000021  loss: 1.8623 (1.8811)  loss_scale: 8192.0000 (11861.0266)  weight_decay: 0.0500 (0.0500)  time: 0.8214  data: 0.2372  max mem: 13008
Epoch: [5]  [1400/3542]  eta: 0:29:55  lr: 0.000021  min_lr: 0.000021  loss: 1.8818 (1.8809)  loss_scale: 8192.0000 (11834.8380)  weight_decay: 0.0500 (0.0500)  time: 0.8921  data: 0.3069  max mem: 13008
Epoch: [5]  [1410/3542]  eta: 0:29:47  lr: 0.000021  min_lr: 0.000021  loss: 1.8818 (1.8811)  loss_scale: 8192.0000 (11809.0206)  weight_decay: 0.0500 (0.0500)  time: 0.9137  data: 0.3285  max mem: 13008
Epoch: [5]  [1420/3542]  eta: 0:29:41  lr: 0.000021  min_lr: 0.000021  loss: 1.8984 (1.8808)  loss_scale: 8192.0000 (11783.5665)  weight_decay: 0.0500 (0.0500)  time: 0.9461  data: 0.3595  max mem: 13008
Epoch: [5]  [1430/3542]  eta: 0:29:36  lr: 0.000021  min_lr: 0.000021  loss: 1.8623 (1.8806)  loss_scale: 8192.0000 (11758.4682)  weight_decay: 0.0500 (0.0500)  time: 1.0038  data: 0.4175  max mem: 13008
Epoch: [5]  [1440/3542]  eta: 0:29:27  lr: 0.000021  min_lr: 0.000021  loss: 1.8262 (1.8803)  loss_scale: 8192.0000 (11733.7183)  weight_decay: 0.0500 (0.0500)  time: 0.9280  data: 0.3439  max mem: 13008
Epoch: [5]  [1450/3542]  eta: 0:29:21  lr: 0.000021  min_lr: 0.000021  loss: 1.8262 (1.8805)  loss_scale: 8192.0000 (11709.3094)  weight_decay: 0.0500 (0.0500)  time: 0.9088  data: 0.3252  max mem: 13008
Epoch: [5]  [1460/3542]  eta: 0:29:15  lr: 0.000021  min_lr: 0.000021  loss: 1.8574 (1.8806)  loss_scale: 8192.0000 (11685.2348)  weight_decay: 0.0500 (0.0500)  time: 0.9875  data: 0.4044  max mem: 13008
Epoch: [5]  [1470/3542]  eta: 0:29:08  lr: 0.000021  min_lr: 0.000021  loss: 1.9150 (1.8810)  loss_scale: 8192.0000 (11661.4874)  weight_decay: 0.0500 (0.0500)  time: 1.0050  data: 0.4215  max mem: 13008
Epoch: [5]  [1480/3542]  eta: 0:29:01  lr: 0.000021  min_lr: 0.000021  loss: 1.9141 (1.8808)  loss_scale: 8192.0000 (11638.0608)  weight_decay: 0.0500 (0.0500)  time: 0.9591  data: 0.3757  max mem: 13008
Epoch: [5]  [1490/3542]  eta: 0:28:55  lr: 0.000021  min_lr: 0.000021  loss: 1.8496 (1.8808)  loss_scale: 8192.0000 (11614.9484)  weight_decay: 0.0500 (0.0500)  time: 0.9521  data: 0.3688  max mem: 13008
Epoch: [5]  [1500/3542]  eta: 0:28:48  lr: 0.000021  min_lr: 0.000021  loss: 1.8193 (1.8806)  loss_scale: 8192.0000 (11592.1439)  weight_decay: 0.0500 (0.0500)  time: 0.9881  data: 0.4052  max mem: 13008
Epoch: [5]  [1510/3542]  eta: 0:28:41  lr: 0.000021  min_lr: 0.000021  loss: 1.9111 (1.8810)  loss_scale: 8192.0000 (11569.6413)  weight_decay: 0.0500 (0.0500)  time: 0.9756  data: 0.3922  max mem: 13008
Epoch: [5]  [1520/3542]  eta: 0:28:33  lr: 0.000021  min_lr: 0.000021  loss: 1.9229 (1.8812)  loss_scale: 8192.0000 (11547.4346)  weight_decay: 0.0500 (0.0500)  time: 0.9377  data: 0.3543  max mem: 13008
Epoch: [5]  [1530/3542]  eta: 0:28:24  lr: 0.000021  min_lr: 0.000021  loss: 1.9121 (1.8814)  loss_scale: 8192.0000 (11525.5180)  weight_decay: 0.0500 (0.0500)  time: 0.8313  data: 0.2476  max mem: 13008
Epoch: [5]  [1540/3542]  eta: 0:28:14  lr: 0.000021  min_lr: 0.000021  loss: 1.9443 (1.8820)  loss_scale: 8192.0000 (11503.8858)  weight_decay: 0.0500 (0.0500)  time: 0.7622  data: 0.1782  max mem: 13008
Epoch: [5]  [1550/3542]  eta: 0:28:06  lr: 0.000021  min_lr: 0.000021  loss: 1.9482 (1.8825)  loss_scale: 8192.0000 (11482.5326)  weight_decay: 0.0500 (0.0500)  time: 0.8174  data: 0.2341  max mem: 13008
Epoch: [5]  [1560/3542]  eta: 0:27:57  lr: 0.000021  min_lr: 0.000021  loss: 1.8682 (1.8824)  loss_scale: 8192.0000 (11461.4529)  weight_decay: 0.0500 (0.0500)  time: 0.8251  data: 0.2410  max mem: 13008
Epoch: [5]  [1570/3542]  eta: 0:27:48  lr: 0.000021  min_lr: 0.000021  loss: 1.8594 (1.8823)  loss_scale: 8192.0000 (11440.6416)  weight_decay: 0.0500 (0.0500)  time: 0.7861  data: 0.2017  max mem: 13008
Epoch: [5]  [1580/3542]  eta: 0:27:37  lr: 0.000021  min_lr: 0.000021  loss: 1.8447 (1.8822)  loss_scale: 8192.0000 (11420.0936)  weight_decay: 0.0500 (0.0500)  time: 0.7170  data: 0.1327  max mem: 13008
Epoch: [5]  [1590/3542]  eta: 0:27:28  lr: 0.000021  min_lr: 0.000021  loss: 1.8203 (1.8820)  loss_scale: 8192.0000 (11399.8039)  weight_decay: 0.0500 (0.0500)  time: 0.7556  data: 0.1694  max mem: 13008
Epoch: [5]  [1600/3542]  eta: 0:27:18  lr: 0.000021  min_lr: 0.000021  loss: 1.8818 (1.8822)  loss_scale: 8192.0000 (11379.7676)  weight_decay: 0.0500 (0.0500)  time: 0.7862  data: 0.1998  max mem: 13008
Epoch: [5]  [1610/3542]  eta: 0:27:09  lr: 0.000021  min_lr: 0.000021  loss: 1.9424 (1.8824)  loss_scale: 8192.0000 (11359.9801)  weight_decay: 0.0500 (0.0500)  time: 0.7421  data: 0.1577  max mem: 13008
Epoch: [5]  [1620/3542]  eta: 0:26:59  lr: 0.000021  min_lr: 0.000021  loss: 1.9307 (1.8828)  loss_scale: 8192.0000 (11340.4368)  weight_decay: 0.0500 (0.0500)  time: 0.7493  data: 0.1654  max mem: 13008
Epoch: [5]  [1630/3542]  eta: 0:26:49  lr: 0.000021  min_lr: 0.000021  loss: 1.8848 (1.8826)  loss_scale: 8192.0000 (11321.1330)  weight_decay: 0.0500 (0.0500)  time: 0.6902  data: 0.1065  max mem: 13008
Epoch: [5]  [1640/3542]  eta: 0:26:38  lr: 0.000021  min_lr: 0.000021  loss: 1.8848 (1.8828)  loss_scale: 8192.0000 (11302.0646)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.0892  max mem: 13008
Epoch: [5]  [1650/3542]  eta: 0:26:29  lr: 0.000021  min_lr: 0.000021  loss: 1.8906 (1.8829)  loss_scale: 8192.0000 (11283.2271)  weight_decay: 0.0500 (0.0500)  time: 0.7189  data: 0.1346  max mem: 13008
Epoch: [5]  [1660/3542]  eta: 0:26:19  lr: 0.000021  min_lr: 0.000021  loss: 1.8740 (1.8831)  loss_scale: 8192.0000 (11264.6165)  weight_decay: 0.0500 (0.0500)  time: 0.7285  data: 0.1440  max mem: 13008
Epoch: [5]  [1670/3542]  eta: 0:26:09  lr: 0.000021  min_lr: 0.000021  loss: 1.8633 (1.8831)  loss_scale: 8192.0000 (11246.2286)  weight_decay: 0.0500 (0.0500)  time: 0.7192  data: 0.1339  max mem: 13008
Epoch: [5]  [1680/3542]  eta: 0:26:00  lr: 0.000021  min_lr: 0.000021  loss: 1.8643 (1.8831)  loss_scale: 8192.0000 (11228.0595)  weight_decay: 0.0500 (0.0500)  time: 0.7323  data: 0.1459  max mem: 13008
Epoch: [5]  [1690/3542]  eta: 0:25:50  lr: 0.000021  min_lr: 0.000021  loss: 1.8643 (1.8830)  loss_scale: 8192.0000 (11210.1053)  weight_decay: 0.0500 (0.0500)  time: 0.7107  data: 0.1242  max mem: 13008
Epoch: [5]  [1700/3542]  eta: 0:25:40  lr: 0.000021  min_lr: 0.000021  loss: 1.8652 (1.8829)  loss_scale: 8192.0000 (11192.3621)  weight_decay: 0.0500 (0.0500)  time: 0.6778  data: 0.0918  max mem: 13008
Epoch: [5]  [1710/3542]  eta: 0:25:30  lr: 0.000021  min_lr: 0.000021  loss: 1.9023 (1.8828)  loss_scale: 8192.0000 (11174.8264)  weight_decay: 0.0500 (0.0500)  time: 0.6898  data: 0.1043  max mem: 13008
Epoch: [5]  [1720/3542]  eta: 0:25:20  lr: 0.000021  min_lr: 0.000021  loss: 1.8945 (1.8829)  loss_scale: 8192.0000 (11157.4945)  weight_decay: 0.0500 (0.0500)  time: 0.6884  data: 0.1026  max mem: 13008
Epoch: [5]  [1730/3542]  eta: 0:25:11  lr: 0.000021  min_lr: 0.000021  loss: 1.9111 (1.8833)  loss_scale: 8192.0000 (11140.3628)  weight_decay: 0.0500 (0.0500)  time: 0.7228  data: 0.1368  max mem: 13008
Epoch: [5]  [1740/3542]  eta: 0:25:01  lr: 0.000021  min_lr: 0.000021  loss: 1.9561 (1.8835)  loss_scale: 8192.0000 (11123.4279)  weight_decay: 0.0500 (0.0500)  time: 0.7505  data: 0.1641  max mem: 13008
Epoch: [5]  [1750/3542]  eta: 0:24:52  lr: 0.000021  min_lr: 0.000021  loss: 1.8418 (1.8830)  loss_scale: 8192.0000 (11106.6865)  weight_decay: 0.0500 (0.0500)  time: 0.7237  data: 0.1374  max mem: 13008
Epoch: [5]  [1760/3542]  eta: 0:24:43  lr: 0.000021  min_lr: 0.000021  loss: 1.8184 (1.8829)  loss_scale: 8192.0000 (11090.1352)  weight_decay: 0.0500 (0.0500)  time: 0.7465  data: 0.1611  max mem: 13008
Epoch: [5]  [1770/3542]  eta: 0:24:33  lr: 0.000021  min_lr: 0.000021  loss: 1.8223 (1.8828)  loss_scale: 8192.0000 (11073.7708)  weight_decay: 0.0500 (0.0500)  time: 0.7189  data: 0.1334  max mem: 13008
Epoch: [5]  [1780/3542]  eta: 0:24:23  lr: 0.000020  min_lr: 0.000020  loss: 1.8896 (1.8830)  loss_scale: 8192.0000 (11057.5901)  weight_decay: 0.0500 (0.0500)  time: 0.6854  data: 0.1003  max mem: 13008
Epoch: [5]  [1790/3542]  eta: 0:24:15  lr: 0.000020  min_lr: 0.000020  loss: 1.8975 (1.8829)  loss_scale: 8192.0000 (11041.5902)  weight_decay: 0.0500 (0.0500)  time: 0.7542  data: 0.1697  max mem: 13008
Epoch: [5]  [1800/3542]  eta: 0:24:08  lr: 0.000020  min_lr: 0.000020  loss: 1.8584 (1.8833)  loss_scale: 8192.0000 (11025.7679)  weight_decay: 0.0500 (0.0500)  time: 0.8723  data: 0.2853  max mem: 13008
Epoch: [5]  [1810/3542]  eta: 0:24:00  lr: 0.000020  min_lr: 0.000020  loss: 1.7764 (1.8826)  loss_scale: 8192.0000 (11010.1204)  weight_decay: 0.0500 (0.0500)  time: 0.9098  data: 0.3233  max mem: 13008
Epoch: [5]  [1820/3542]  eta: 0:23:52  lr: 0.000020  min_lr: 0.000020  loss: 1.7783 (1.8826)  loss_scale: 8192.0000 (10994.6447)  weight_decay: 0.0500 (0.0500)  time: 0.8933  data: 0.3101  max mem: 13008
Epoch: [5]  [1830/3542]  eta: 0:23:44  lr: 0.000020  min_lr: 0.000020  loss: 1.8516 (1.8827)  loss_scale: 8192.0000 (10979.3381)  weight_decay: 0.0500 (0.0500)  time: 0.8913  data: 0.3084  max mem: 13008
Epoch: [5]  [1840/3542]  eta: 0:23:37  lr: 0.000020  min_lr: 0.000020  loss: 1.8467 (1.8825)  loss_scale: 8192.0000 (10964.1977)  weight_decay: 0.0500 (0.0500)  time: 0.9196  data: 0.3355  max mem: 13008
Epoch: [5]  [1850/3542]  eta: 0:23:28  lr: 0.000020  min_lr: 0.000020  loss: 1.8467 (1.8824)  loss_scale: 8192.0000 (10949.2210)  weight_decay: 0.0500 (0.0500)  time: 0.8618  data: 0.2774  max mem: 13008
Epoch: [5]  [1860/3542]  eta: 0:23:20  lr: 0.000020  min_lr: 0.000020  loss: 1.8242 (1.8821)  loss_scale: 8192.0000 (10934.4052)  weight_decay: 0.0500 (0.0500)  time: 0.8032  data: 0.2192  max mem: 13008
Epoch: [5]  [1870/3542]  eta: 0:23:11  lr: 0.000020  min_lr: 0.000020  loss: 1.7627 (1.8817)  loss_scale: 8192.0000 (10919.7477)  weight_decay: 0.0500 (0.0500)  time: 0.8223  data: 0.2380  max mem: 13008
Epoch: [5]  [1880/3542]  eta: 0:23:02  lr: 0.000020  min_lr: 0.000020  loss: 1.8115 (1.8816)  loss_scale: 8192.0000 (10905.2461)  weight_decay: 0.0500 (0.0500)  time: 0.7861  data: 0.2017  max mem: 13008
Epoch: [5]  [1890/3542]  eta: 0:22:53  lr: 0.000020  min_lr: 0.000020  loss: 1.8418 (1.8818)  loss_scale: 8192.0000 (10890.8979)  weight_decay: 0.0500 (0.0500)  time: 0.7527  data: 0.1682  max mem: 13008
Epoch: [5]  [1900/3542]  eta: 0:22:45  lr: 0.000020  min_lr: 0.000020  loss: 1.8105 (1.8812)  loss_scale: 8192.0000 (10876.7007)  weight_decay: 0.0500 (0.0500)  time: 0.7682  data: 0.1835  max mem: 13008
Epoch: [5]  [1910/3542]  eta: 0:22:36  lr: 0.000020  min_lr: 0.000020  loss: 1.7959 (1.8809)  loss_scale: 8192.0000 (10862.6520)  weight_decay: 0.0500 (0.0500)  time: 0.7742  data: 0.1898  max mem: 13008
Epoch: [5]  [1920/3542]  eta: 0:22:27  lr: 0.000020  min_lr: 0.000020  loss: 1.8516 (1.8806)  loss_scale: 8192.0000 (10848.7496)  weight_decay: 0.0500 (0.0500)  time: 0.7651  data: 0.1807  max mem: 13008
Epoch: [5]  [1930/3542]  eta: 0:22:18  lr: 0.000020  min_lr: 0.000020  loss: 1.8838 (1.8806)  loss_scale: 8192.0000 (10834.9912)  weight_decay: 0.0500 (0.0500)  time: 0.7460  data: 0.1607  max mem: 13008
Epoch: [5]  [1940/3542]  eta: 0:22:09  lr: 0.000020  min_lr: 0.000020  loss: 1.8994 (1.8809)  loss_scale: 8192.0000 (10821.3745)  weight_decay: 0.0500 (0.0500)  time: 0.7532  data: 0.1681  max mem: 13008
Epoch: [5]  [1950/3542]  eta: 0:22:01  lr: 0.000020  min_lr: 0.000020  loss: 1.9014 (1.8811)  loss_scale: 8192.0000 (10807.8975)  weight_decay: 0.0500 (0.0500)  time: 0.8258  data: 0.2413  max mem: 13008
Epoch: [5]  [1960/3542]  eta: 0:21:52  lr: 0.000020  min_lr: 0.000020  loss: 1.9014 (1.8808)  loss_scale: 8192.0000 (10794.5579)  weight_decay: 0.0500 (0.0500)  time: 0.7882  data: 0.2036  max mem: 13008
Epoch: [5]  [1970/3542]  eta: 0:21:43  lr: 0.000020  min_lr: 0.000020  loss: 1.7705 (1.8803)  loss_scale: 8192.0000 (10781.3536)  weight_decay: 0.0500 (0.0500)  time: 0.7113  data: 0.1260  max mem: 13008
Epoch: [5]  [1980/3542]  eta: 0:21:33  lr: 0.000020  min_lr: 0.000020  loss: 1.7920 (1.8804)  loss_scale: 8192.0000 (10768.2827)  weight_decay: 0.0500 (0.0500)  time: 0.6919  data: 0.1070  max mem: 13008
Epoch: [5]  [1990/3542]  eta: 0:21:25  lr: 0.000020  min_lr: 0.000020  loss: 1.9199 (1.8803)  loss_scale: 8192.0000 (10755.3430)  weight_decay: 0.0500 (0.0500)  time: 0.7609  data: 0.1760  max mem: 13008
Epoch: [5]  [2000/3542]  eta: 0:21:16  lr: 0.000020  min_lr: 0.000020  loss: 1.9521 (1.8805)  loss_scale: 8192.0000 (10742.5327)  weight_decay: 0.0500 (0.0500)  time: 0.7835  data: 0.1979  max mem: 13008
Epoch: [5]  [2010/3542]  eta: 0:21:07  lr: 0.000020  min_lr: 0.000020  loss: 1.9521 (1.8806)  loss_scale: 8192.0000 (10729.8498)  weight_decay: 0.0500 (0.0500)  time: 0.7671  data: 0.1807  max mem: 13008
Epoch: [5]  [2020/3542]  eta: 0:20:58  lr: 0.000020  min_lr: 0.000020  loss: 1.8477 (1.8805)  loss_scale: 8192.0000 (10717.2924)  weight_decay: 0.0500 (0.0500)  time: 0.7725  data: 0.1866  max mem: 13008
Epoch: [5]  [2030/3542]  eta: 0:20:49  lr: 0.000020  min_lr: 0.000020  loss: 1.8818 (1.8804)  loss_scale: 8192.0000 (10704.8587)  weight_decay: 0.0500 (0.0500)  time: 0.7232  data: 0.1383  max mem: 13008
Epoch: [5]  [2040/3542]  eta: 0:20:41  lr: 0.000020  min_lr: 0.000020  loss: 1.8828 (1.8803)  loss_scale: 8192.0000 (10692.5468)  weight_decay: 0.0500 (0.0500)  time: 0.7496  data: 0.1643  max mem: 13008
Epoch: [5]  [2050/3542]  eta: 0:20:33  lr: 0.000020  min_lr: 0.000020  loss: 1.8574 (1.8802)  loss_scale: 8192.0000 (10680.3549)  weight_decay: 0.0500 (0.0500)  time: 0.8117  data: 0.2272  max mem: 13008
Epoch: [5]  [2060/3542]  eta: 0:20:24  lr: 0.000020  min_lr: 0.000020  loss: 1.8848 (1.8805)  loss_scale: 8192.0000 (10668.2814)  weight_decay: 0.0500 (0.0500)  time: 0.7981  data: 0.2138  max mem: 13008
[2023-05-16 10:30:33,488] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 10:30:33,489] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [5]  [2070/3542]  eta: 0:20:15  lr: 0.000020  min_lr: 0.000020  loss: 1.9131 (1.8805)  loss_scale: 8192.0000 (10695.8803)  weight_decay: 0.0500 (0.0500)  time: 0.7878  data: 0.2032  max mem: 13008
Epoch: [5]  [2080/3542]  eta: 0:20:06  lr: 0.000020  min_lr: 0.000020  loss: 1.8770 (1.8806)  loss_scale: 16384.0000 (10723.2138)  weight_decay: 0.0500 (0.0500)  time: 0.7621  data: 0.1771  max mem: 13008
Epoch: [5]  [2090/3542]  eta: 0:19:59  lr: 0.000020  min_lr: 0.000020  loss: 1.8721 (1.8806)  loss_scale: 16384.0000 (10750.2860)  weight_decay: 0.0500 (0.0500)  time: 0.8192  data: 0.2339  max mem: 13008
Epoch: [5]  [2100/3542]  eta: 0:19:50  lr: 0.000020  min_lr: 0.000020  loss: 1.8574 (1.8806)  loss_scale: 16384.0000 (10777.1004)  weight_decay: 0.0500 (0.0500)  time: 0.8285  data: 0.2440  max mem: 13008
Epoch: [5]  [2110/3542]  eta: 0:19:41  lr: 0.000020  min_lr: 0.000020  loss: 1.8125 (1.8803)  loss_scale: 16384.0000 (10803.6608)  weight_decay: 0.0500 (0.0500)  time: 0.7391  data: 0.1551  max mem: 13008
Epoch: [5]  [2120/3542]  eta: 0:19:33  lr: 0.000020  min_lr: 0.000020  loss: 1.8428 (1.8804)  loss_scale: 16384.0000 (10829.9708)  weight_decay: 0.0500 (0.0500)  time: 0.7732  data: 0.1888  max mem: 13008
Epoch: [5]  [2130/3542]  eta: 0:19:24  lr: 0.000020  min_lr: 0.000020  loss: 1.8633 (1.8806)  loss_scale: 16384.0000 (10856.0338)  weight_decay: 0.0500 (0.0500)  time: 0.7617  data: 0.1767  max mem: 13008
Epoch: [5]  [2140/3542]  eta: 0:19:15  lr: 0.000020  min_lr: 0.000020  loss: 1.8457 (1.8804)  loss_scale: 16384.0000 (10881.8533)  weight_decay: 0.0500 (0.0500)  time: 0.7328  data: 0.1475  max mem: 13008
Epoch: [5]  [2150/3542]  eta: 0:19:07  lr: 0.000020  min_lr: 0.000020  loss: 1.7930 (1.8804)  loss_scale: 16384.0000 (10907.4328)  weight_decay: 0.0500 (0.0500)  time: 0.8150  data: 0.2293  max mem: 13008
Epoch: [5]  [2160/3542]  eta: 0:18:58  lr: 0.000020  min_lr: 0.000020  loss: 1.8184 (1.8804)  loss_scale: 16384.0000 (10932.7756)  weight_decay: 0.0500 (0.0500)  time: 0.7848  data: 0.1997  max mem: 13008
Epoch: [5]  [2170/3542]  eta: 0:18:49  lr: 0.000020  min_lr: 0.000020  loss: 1.8184 (1.8800)  loss_scale: 16384.0000 (10957.8848)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.0992  max mem: 13008
Epoch: [5]  [2180/3542]  eta: 0:18:40  lr: 0.000020  min_lr: 0.000020  loss: 1.8281 (1.8801)  loss_scale: 16384.0000 (10982.7639)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.1040  max mem: 13008
Epoch: [5]  [2190/3542]  eta: 0:18:31  lr: 0.000020  min_lr: 0.000020  loss: 1.8438 (1.8800)  loss_scale: 16384.0000 (11007.4158)  weight_decay: 0.0500 (0.0500)  time: 0.6900  data: 0.1044  max mem: 13008
Epoch: [5]  [2200/3542]  eta: 0:18:22  lr: 0.000020  min_lr: 0.000020  loss: 1.8340 (1.8801)  loss_scale: 16384.0000 (11031.8437)  weight_decay: 0.0500 (0.0500)  time: 0.7312  data: 0.1462  max mem: 13008
Epoch: [5]  [2210/3542]  eta: 0:18:14  lr: 0.000020  min_lr: 0.000020  loss: 1.8340 (1.8799)  loss_scale: 16384.0000 (11056.0507)  weight_decay: 0.0500 (0.0500)  time: 0.8286  data: 0.2435  max mem: 13008
Epoch: [5]  [2220/3542]  eta: 0:18:06  lr: 0.000020  min_lr: 0.000020  loss: 1.8545 (1.8797)  loss_scale: 16384.0000 (11080.0396)  weight_decay: 0.0500 (0.0500)  time: 0.8276  data: 0.2427  max mem: 13008
Epoch: [5]  [2230/3542]  eta: 0:17:57  lr: 0.000020  min_lr: 0.000020  loss: 1.9238 (1.8800)  loss_scale: 16384.0000 (11103.8135)  weight_decay: 0.0500 (0.0500)  time: 0.7724  data: 0.1880  max mem: 13008
Epoch: [5]  [2240/3542]  eta: 0:17:49  lr: 0.000020  min_lr: 0.000020  loss: 1.8760 (1.8797)  loss_scale: 16384.0000 (11127.3753)  weight_decay: 0.0500 (0.0500)  time: 0.8160  data: 0.2313  max mem: 13008
Epoch: [5]  [2250/3542]  eta: 0:17:41  lr: 0.000020  min_lr: 0.000020  loss: 1.8057 (1.8794)  loss_scale: 16384.0000 (11150.7277)  weight_decay: 0.0500 (0.0500)  time: 0.8593  data: 0.2734  max mem: 13008
Epoch: [5]  [2260/3542]  eta: 0:17:33  lr: 0.000020  min_lr: 0.000020  loss: 1.8057 (1.8792)  loss_scale: 16384.0000 (11173.8735)  weight_decay: 0.0500 (0.0500)  time: 0.8573  data: 0.2722  max mem: 13008
Epoch: [5]  [2270/3542]  eta: 0:17:25  lr: 0.000020  min_lr: 0.000020  loss: 1.8203 (1.8791)  loss_scale: 16384.0000 (11196.8155)  weight_decay: 0.0500 (0.0500)  time: 0.8271  data: 0.2429  max mem: 13008
Epoch: [5]  [2280/3542]  eta: 0:17:17  lr: 0.000020  min_lr: 0.000020  loss: 1.8770 (1.8793)  loss_scale: 16384.0000 (11219.5563)  weight_decay: 0.0500 (0.0500)  time: 0.8001  data: 0.2155  max mem: 13008
[2023-05-16 10:33:31,393] [INFO] [logging.py:60:log_dist] [Rank 0] step=20000, skipped=19, lr=[1.9504973765451497e-05, 1.9504973765451497e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 10:33:31,395] [INFO] [timer.py:157:stop] 0/20000, SamplesPerSec=55.55785317693241
Epoch: [5]  [2290/3542]  eta: 0:17:08  lr: 0.000020  min_lr: 0.000020  loss: 1.9141 (1.8796)  loss_scale: 16384.0000 (11242.0986)  weight_decay: 0.0500 (0.0500)  time: 0.7476  data: 0.1637  max mem: 13008
Epoch: [5]  [2300/3542]  eta: 0:17:00  lr: 0.000019  min_lr: 0.000019  loss: 1.8477 (1.8794)  loss_scale: 16384.0000 (11264.4450)  weight_decay: 0.0500 (0.0500)  time: 0.7829  data: 0.1602  max mem: 13008
Epoch: [5]  [2310/3542]  eta: 0:16:50  lr: 0.000019  min_lr: 0.000019  loss: 1.7988 (1.8794)  loss_scale: 16384.0000 (11286.5980)  weight_decay: 0.0500 (0.0500)  time: 0.7267  data: 0.1033  max mem: 13008
Epoch: [5]  [2320/3542]  eta: 0:16:42  lr: 0.000019  min_lr: 0.000019  loss: 1.8369 (1.8793)  loss_scale: 16384.0000 (11308.5601)  weight_decay: 0.0500 (0.0500)  time: 0.6669  data: 0.0821  max mem: 13008
Epoch: [5]  [2330/3542]  eta: 0:16:34  lr: 0.000019  min_lr: 0.000019  loss: 1.8369 (1.8791)  loss_scale: 16384.0000 (11330.3338)  weight_decay: 0.0500 (0.0500)  time: 0.8324  data: 0.2471  max mem: 13008
Epoch: [5]  [2340/3542]  eta: 0:16:26  lr: 0.000019  min_lr: 0.000019  loss: 1.8613 (1.8791)  loss_scale: 16384.0000 (11351.9214)  weight_decay: 0.0500 (0.0500)  time: 0.8556  data: 0.2702  max mem: 13008
Epoch: [5]  [2350/3542]  eta: 0:16:18  lr: 0.000019  min_lr: 0.000019  loss: 1.8721 (1.8791)  loss_scale: 16384.0000 (11373.3254)  weight_decay: 0.0500 (0.0500)  time: 0.8798  data: 0.2943  max mem: 13008
Epoch: [5]  [2360/3542]  eta: 0:16:10  lr: 0.000019  min_lr: 0.000019  loss: 1.8584 (1.8787)  loss_scale: 16384.0000 (11394.5481)  weight_decay: 0.0500 (0.0500)  time: 0.8820  data: 0.2967  max mem: 13008
Epoch: [5]  [2370/3542]  eta: 0:16:02  lr: 0.000019  min_lr: 0.000019  loss: 1.8682 (1.8785)  loss_scale: 16384.0000 (11415.5917)  weight_decay: 0.0500 (0.0500)  time: 0.8563  data: 0.2721  max mem: 13008
Epoch: [5]  [2380/3542]  eta: 0:15:54  lr: 0.000019  min_lr: 0.000019  loss: 1.8711 (1.8785)  loss_scale: 16384.0000 (11436.4586)  weight_decay: 0.0500 (0.0500)  time: 0.9120  data: 0.3276  max mem: 13008
Epoch: [5]  [2390/3542]  eta: 0:15:47  lr: 0.000019  min_lr: 0.000019  loss: 1.8467 (1.8786)  loss_scale: 16384.0000 (11457.1510)  weight_decay: 0.0500 (0.0500)  time: 0.9010  data: 0.3171  max mem: 13008
Epoch: [5]  [2400/3542]  eta: 0:15:38  lr: 0.000019  min_lr: 0.000019  loss: 1.8887 (1.8785)  loss_scale: 16384.0000 (11477.6710)  weight_decay: 0.0500 (0.0500)  time: 0.8610  data: 0.2781  max mem: 13008
Epoch: [5]  [2410/3542]  eta: 0:15:31  lr: 0.000019  min_lr: 0.000019  loss: 1.8701 (1.8782)  loss_scale: 16384.0000 (11498.0207)  weight_decay: 0.0500 (0.0500)  time: 0.8805  data: 0.2952  max mem: 13008
Epoch: [5]  [2420/3542]  eta: 0:15:23  lr: 0.000019  min_lr: 0.000019  loss: 1.8779 (1.8781)  loss_scale: 16384.0000 (11518.2024)  weight_decay: 0.0500 (0.0500)  time: 0.9081  data: 0.3224  max mem: 13008
Epoch: [5]  [2430/3542]  eta: 0:15:15  lr: 0.000019  min_lr: 0.000019  loss: 1.8848 (1.8784)  loss_scale: 16384.0000 (11538.2180)  weight_decay: 0.0500 (0.0500)  time: 0.9552  data: 0.3722  max mem: 13008
Epoch: [5]  [2440/3542]  eta: 0:15:07  lr: 0.000019  min_lr: 0.000019  loss: 1.9102 (1.8785)  loss_scale: 16384.0000 (11558.0696)  weight_decay: 0.0500 (0.0500)  time: 0.8920  data: 0.3088  max mem: 13008
[2023-05-16 10:35:40,810] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 20151
[2023-05-16 10:35:40,811] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 10:35:40,811] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [5]  [2450/3542]  eta: 0:14:59  lr: 0.000019  min_lr: 0.000019  loss: 1.9609 (1.8790)  loss_scale: 8192.0000 (11544.3362)  weight_decay: 0.0500 (0.0500)  time: 0.8675  data: 0.2840  max mem: 13008
Epoch: [5]  [2460/3542]  eta: 0:14:51  lr: 0.000019  min_lr: 0.000019  loss: 1.9336 (1.8793)  loss_scale: 8192.0000 (11530.7143)  weight_decay: 0.0500 (0.0500)  time: 0.8960  data: 0.3123  max mem: 13008
Epoch: [5]  [2470/3542]  eta: 0:14:43  lr: 0.000019  min_lr: 0.000019  loss: 1.9287 (1.8793)  loss_scale: 8192.0000 (11517.2028)  weight_decay: 0.0500 (0.0500)  time: 0.8830  data: 0.2975  max mem: 13008
Epoch: [5]  [2480/3542]  eta: 0:14:35  lr: 0.000019  min_lr: 0.000019  loss: 1.9375 (1.8796)  loss_scale: 8192.0000 (11503.8001)  weight_decay: 0.0500 (0.0500)  time: 0.8938  data: 0.3081  max mem: 13008
Epoch: [5]  [2490/3542]  eta: 0:14:27  lr: 0.000019  min_lr: 0.000019  loss: 1.8916 (1.8797)  loss_scale: 8192.0000 (11490.5050)  weight_decay: 0.0500 (0.0500)  time: 0.8237  data: 0.2393  max mem: 13008
Epoch: [5]  [2500/3542]  eta: 0:14:18  lr: 0.000019  min_lr: 0.000019  loss: 1.8574 (1.8795)  loss_scale: 8192.0000 (11477.3163)  weight_decay: 0.0500 (0.0500)  time: 0.7692  data: 0.1848  max mem: 13008
Epoch: [5]  [2510/3542]  eta: 0:14:10  lr: 0.000019  min_lr: 0.000019  loss: 1.8701 (1.8798)  loss_scale: 8192.0000 (11464.2326)  weight_decay: 0.0500 (0.0500)  time: 0.7373  data: 0.1527  max mem: 13008
Epoch: [5]  [2520/3542]  eta: 0:14:01  lr: 0.000019  min_lr: 0.000019  loss: 1.9170 (1.8802)  loss_scale: 8192.0000 (11451.2527)  weight_decay: 0.0500 (0.0500)  time: 0.7499  data: 0.1645  max mem: 13008
Epoch: [5]  [2530/3542]  eta: 0:13:52  lr: 0.000019  min_lr: 0.000019  loss: 1.8525 (1.8796)  loss_scale: 8192.0000 (11438.3753)  weight_decay: 0.0500 (0.0500)  time: 0.7388  data: 0.1536  max mem: 13008
Epoch: [5]  [2540/3542]  eta: 0:13:44  lr: 0.000019  min_lr: 0.000019  loss: 1.8320 (1.8798)  loss_scale: 8192.0000 (11425.5994)  weight_decay: 0.0500 (0.0500)  time: 0.7352  data: 0.1515  max mem: 13008
Epoch: [5]  [2550/3542]  eta: 0:13:35  lr: 0.000019  min_lr: 0.000019  loss: 1.8506 (1.8798)  loss_scale: 8192.0000 (11412.9236)  weight_decay: 0.0500 (0.0500)  time: 0.7134  data: 0.1296  max mem: 13008
Epoch: [5]  [2560/3542]  eta: 0:13:27  lr: 0.000019  min_lr: 0.000019  loss: 1.8877 (1.8801)  loss_scale: 8192.0000 (11400.3467)  weight_decay: 0.0500 (0.0500)  time: 0.7033  data: 0.1190  max mem: 13008
Epoch: [5]  [2570/3542]  eta: 0:13:18  lr: 0.000019  min_lr: 0.000019  loss: 1.8984 (1.8800)  loss_scale: 8192.0000 (11387.8678)  weight_decay: 0.0500 (0.0500)  time: 0.7141  data: 0.1302  max mem: 13008
Epoch: [5]  [2580/3542]  eta: 0:13:09  lr: 0.000019  min_lr: 0.000019  loss: 1.8291 (1.8799)  loss_scale: 8192.0000 (11375.4855)  weight_decay: 0.0500 (0.0500)  time: 0.7151  data: 0.1311  max mem: 13008
Epoch: [5]  [2590/3542]  eta: 0:13:01  lr: 0.000019  min_lr: 0.000019  loss: 1.8291 (1.8798)  loss_scale: 8192.0000 (11363.1988)  weight_decay: 0.0500 (0.0500)  time: 0.7327  data: 0.1482  max mem: 13008
Epoch: [5]  [2600/3542]  eta: 0:12:52  lr: 0.000019  min_lr: 0.000019  loss: 1.8340 (1.8797)  loss_scale: 8192.0000 (11351.0065)  weight_decay: 0.0500 (0.0500)  time: 0.7316  data: 0.1464  max mem: 13008
Epoch: [5]  [2610/3542]  eta: 0:12:43  lr: 0.000019  min_lr: 0.000019  loss: 1.8555 (1.8801)  loss_scale: 8192.0000 (11338.9077)  weight_decay: 0.0500 (0.0500)  time: 0.6891  data: 0.1030  max mem: 13008
Epoch: [5]  [2620/3542]  eta: 0:12:35  lr: 0.000019  min_lr: 0.000019  loss: 1.8232 (1.8797)  loss_scale: 8192.0000 (11326.9012)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.1054  max mem: 13008
Epoch: [5]  [2630/3542]  eta: 0:12:26  lr: 0.000019  min_lr: 0.000019  loss: 1.8232 (1.8798)  loss_scale: 8192.0000 (11314.9859)  weight_decay: 0.0500 (0.0500)  time: 0.7013  data: 0.1162  max mem: 13008
Epoch: [5]  [2640/3542]  eta: 0:12:17  lr: 0.000019  min_lr: 0.000019  loss: 1.8242 (1.8796)  loss_scale: 8192.0000 (11303.1609)  weight_decay: 0.0500 (0.0500)  time: 0.6567  data: 0.0710  max mem: 13008
Epoch: [5]  [2650/3542]  eta: 0:12:09  lr: 0.000019  min_lr: 0.000019  loss: 1.7803 (1.8792)  loss_scale: 8192.0000 (11291.4251)  weight_decay: 0.0500 (0.0500)  time: 0.6695  data: 0.0839  max mem: 13008
Epoch: [5]  [2660/3542]  eta: 0:12:00  lr: 0.000019  min_lr: 0.000019  loss: 1.8555 (1.8793)  loss_scale: 8192.0000 (11279.7775)  weight_decay: 0.0500 (0.0500)  time: 0.6910  data: 0.1049  max mem: 13008
Epoch: [5]  [2670/3542]  eta: 0:11:52  lr: 0.000019  min_lr: 0.000019  loss: 1.9639 (1.8798)  loss_scale: 8192.0000 (11268.2171)  weight_decay: 0.0500 (0.0500)  time: 0.6740  data: 0.0881  max mem: 13008
Epoch: [5]  [2680/3542]  eta: 0:11:43  lr: 0.000019  min_lr: 0.000019  loss: 1.9639 (1.8799)  loss_scale: 8192.0000 (11256.7430)  weight_decay: 0.0500 (0.0500)  time: 0.6526  data: 0.0670  max mem: 13008
Epoch: [5]  [2690/3542]  eta: 0:11:34  lr: 0.000019  min_lr: 0.000019  loss: 1.8516 (1.8798)  loss_scale: 8192.0000 (11245.3541)  weight_decay: 0.0500 (0.0500)  time: 0.6397  data: 0.0533  max mem: 13008
Epoch: [5]  [2700/3542]  eta: 0:11:25  lr: 0.000019  min_lr: 0.000019  loss: 1.7725 (1.8796)  loss_scale: 8192.0000 (11234.0496)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.0509  max mem: 13008
Epoch: [5]  [2710/3542]  eta: 0:11:17  lr: 0.000019  min_lr: 0.000019  loss: 1.8057 (1.8798)  loss_scale: 8192.0000 (11222.8285)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.0593  max mem: 13008
Epoch: [5]  [2720/3542]  eta: 0:11:08  lr: 0.000019  min_lr: 0.000019  loss: 1.8789 (1.8798)  loss_scale: 8192.0000 (11211.6898)  weight_decay: 0.0500 (0.0500)  time: 0.6656  data: 0.0807  max mem: 13008
Epoch: [5]  [2730/3542]  eta: 0:10:59  lr: 0.000019  min_lr: 0.000019  loss: 1.9062 (1.8800)  loss_scale: 8192.0000 (11200.6327)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0467  max mem: 13008
Epoch: [5]  [2740/3542]  eta: 0:10:51  lr: 0.000019  min_lr: 0.000019  loss: 1.9062 (1.8800)  loss_scale: 8192.0000 (11189.6563)  weight_decay: 0.0500 (0.0500)  time: 0.6407  data: 0.0553  max mem: 13008
Epoch: [5]  [2750/3542]  eta: 0:10:42  lr: 0.000019  min_lr: 0.000019  loss: 1.8418 (1.8800)  loss_scale: 8192.0000 (11178.7597)  weight_decay: 0.0500 (0.0500)  time: 0.6808  data: 0.0956  max mem: 13008
Epoch: [5]  [2760/3542]  eta: 0:10:34  lr: 0.000019  min_lr: 0.000019  loss: 1.8535 (1.8799)  loss_scale: 8192.0000 (11167.9420)  weight_decay: 0.0500 (0.0500)  time: 0.6716  data: 0.0865  max mem: 13008
Epoch: [5]  [2770/3542]  eta: 0:10:25  lr: 0.000019  min_lr: 0.000019  loss: 1.8633 (1.8800)  loss_scale: 8192.0000 (11157.2025)  weight_decay: 0.0500 (0.0500)  time: 0.6600  data: 0.0740  max mem: 13008
Epoch: [5]  [2780/3542]  eta: 0:10:17  lr: 0.000019  min_lr: 0.000019  loss: 1.8633 (1.8800)  loss_scale: 8192.0000 (11146.5401)  weight_decay: 0.0500 (0.0500)  time: 0.6401  data: 0.0534  max mem: 13008
Epoch: [5]  [2790/3542]  eta: 0:10:08  lr: 0.000019  min_lr: 0.000019  loss: 1.8545 (1.8800)  loss_scale: 8192.0000 (11135.9541)  weight_decay: 0.0500 (0.0500)  time: 0.6613  data: 0.0748  max mem: 13008
Epoch: [5]  [2800/3542]  eta: 0:10:00  lr: 0.000019  min_lr: 0.000019  loss: 1.8320 (1.8798)  loss_scale: 8192.0000 (11125.4438)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.1053  max mem: 13008
Epoch: [5]  [2810/3542]  eta: 0:09:51  lr: 0.000019  min_lr: 0.000019  loss: 1.8594 (1.8798)  loss_scale: 8192.0000 (11115.0082)  weight_decay: 0.0500 (0.0500)  time: 0.6563  data: 0.0709  max mem: 13008
Epoch: [5]  [2820/3542]  eta: 0:09:43  lr: 0.000018  min_lr: 0.000018  loss: 1.8906 (1.8797)  loss_scale: 8192.0000 (11104.6466)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.0578  max mem: 13008
Epoch: [5]  [2830/3542]  eta: 0:09:34  lr: 0.000018  min_lr: 0.000018  loss: 1.8252 (1.8796)  loss_scale: 8192.0000 (11094.3582)  weight_decay: 0.0500 (0.0500)  time: 0.6516  data: 0.0658  max mem: 13008
Epoch: [5]  [2840/3542]  eta: 0:09:26  lr: 0.000018  min_lr: 0.000018  loss: 1.8525 (1.8797)  loss_scale: 8192.0000 (11084.1422)  weight_decay: 0.0500 (0.0500)  time: 0.6535  data: 0.0683  max mem: 13008
Epoch: [5]  [2850/3542]  eta: 0:09:17  lr: 0.000018  min_lr: 0.000018  loss: 1.8525 (1.8798)  loss_scale: 8192.0000 (11073.9979)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.0635  max mem: 13008
Epoch: [5]  [2860/3542]  eta: 0:09:09  lr: 0.000018  min_lr: 0.000018  loss: 1.9170 (1.8802)  loss_scale: 8192.0000 (11063.9245)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.0751  max mem: 13008
Epoch: [5]  [2870/3542]  eta: 0:09:01  lr: 0.000018  min_lr: 0.000018  loss: 1.9619 (1.8804)  loss_scale: 8192.0000 (11053.9213)  weight_decay: 0.0500 (0.0500)  time: 0.7047  data: 0.1184  max mem: 13008
Epoch: [5]  [2880/3542]  eta: 0:08:53  lr: 0.000018  min_lr: 0.000018  loss: 1.8799 (1.8801)  loss_scale: 8192.0000 (11043.9875)  weight_decay: 0.0500 (0.0500)  time: 0.7028  data: 0.1165  max mem: 13008
Epoch: [5]  [2890/3542]  eta: 0:08:44  lr: 0.000018  min_lr: 0.000018  loss: 1.8242 (1.8799)  loss_scale: 8192.0000 (11034.1224)  weight_decay: 0.0500 (0.0500)  time: 0.6899  data: 0.1034  max mem: 13008
Epoch: [5]  [2900/3542]  eta: 0:08:36  lr: 0.000018  min_lr: 0.000018  loss: 1.7773 (1.8795)  loss_scale: 8192.0000 (11024.3254)  weight_decay: 0.0500 (0.0500)  time: 0.6972  data: 0.1107  max mem: 13008
Epoch: [5]  [2910/3542]  eta: 0:08:28  lr: 0.000018  min_lr: 0.000018  loss: 1.8467 (1.8795)  loss_scale: 8192.0000 (11014.5957)  weight_decay: 0.0500 (0.0500)  time: 0.7020  data: 0.1156  max mem: 13008
Epoch: [5]  [2920/3542]  eta: 0:08:19  lr: 0.000018  min_lr: 0.000018  loss: 1.8740 (1.8794)  loss_scale: 8192.0000 (11004.9326)  weight_decay: 0.0500 (0.0500)  time: 0.6983  data: 0.1112  max mem: 13008
Epoch: [5]  [2930/3542]  eta: 0:08:11  lr: 0.000018  min_lr: 0.000018  loss: 1.8740 (1.8792)  loss_scale: 8192.0000 (10995.3354)  weight_decay: 0.0500 (0.0500)  time: 0.7157  data: 0.1276  max mem: 13008
Epoch: [5]  [2940/3542]  eta: 0:08:03  lr: 0.000018  min_lr: 0.000018  loss: 1.8877 (1.8793)  loss_scale: 8192.0000 (10985.8035)  weight_decay: 0.0500 (0.0500)  time: 0.7167  data: 0.1284  max mem: 13008
Epoch: [5]  [2950/3542]  eta: 0:07:55  lr: 0.000018  min_lr: 0.000018  loss: 1.8467 (1.8792)  loss_scale: 8192.0000 (10976.3362)  weight_decay: 0.0500 (0.0500)  time: 0.6992  data: 0.1117  max mem: 13008
Epoch: [5]  [2960/3542]  eta: 0:07:47  lr: 0.000018  min_lr: 0.000018  loss: 1.8291 (1.8790)  loss_scale: 8192.0000 (10966.9328)  weight_decay: 0.0500 (0.0500)  time: 0.7010  data: 0.1144  max mem: 13008
Epoch: [5]  [2970/3542]  eta: 0:07:38  lr: 0.000018  min_lr: 0.000018  loss: 1.8320 (1.8791)  loss_scale: 8192.0000 (10957.5927)  weight_decay: 0.0500 (0.0500)  time: 0.7135  data: 0.1276  max mem: 13008
Epoch: [5]  [2980/3542]  eta: 0:07:30  lr: 0.000018  min_lr: 0.000018  loss: 1.8877 (1.8792)  loss_scale: 8192.0000 (10948.3153)  weight_decay: 0.0500 (0.0500)  time: 0.7224  data: 0.1358  max mem: 13008
Epoch: [5]  [2990/3542]  eta: 0:07:22  lr: 0.000018  min_lr: 0.000018  loss: 1.9297 (1.8793)  loss_scale: 8192.0000 (10939.1000)  weight_decay: 0.0500 (0.0500)  time: 0.7253  data: 0.1388  max mem: 13008
Epoch: [5]  [3000/3542]  eta: 0:07:14  lr: 0.000018  min_lr: 0.000018  loss: 1.8408 (1.8792)  loss_scale: 8192.0000 (10929.9460)  weight_decay: 0.0500 (0.0500)  time: 0.6687  data: 0.0816  max mem: 13008
Epoch: [5]  [3010/3542]  eta: 0:07:05  lr: 0.000018  min_lr: 0.000018  loss: 1.8740 (1.8793)  loss_scale: 8192.0000 (10920.8529)  weight_decay: 0.0500 (0.0500)  time: 0.6694  data: 0.0817  max mem: 13008
Epoch: [5]  [3020/3542]  eta: 0:06:57  lr: 0.000018  min_lr: 0.000018  loss: 1.9258 (1.8794)  loss_scale: 8192.0000 (10911.8199)  weight_decay: 0.0500 (0.0500)  time: 0.7072  data: 0.1204  max mem: 13008
Epoch: [5]  [3030/3542]  eta: 0:06:49  lr: 0.000018  min_lr: 0.000018  loss: 1.8828 (1.8792)  loss_scale: 8192.0000 (10902.8466)  weight_decay: 0.0500 (0.0500)  time: 0.6991  data: 0.1119  max mem: 13008
Epoch: [5]  [3040/3542]  eta: 0:06:41  lr: 0.000018  min_lr: 0.000018  loss: 1.8906 (1.8793)  loss_scale: 8192.0000 (10893.9323)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.0946  max mem: 13008
Epoch: [5]  [3050/3542]  eta: 0:06:33  lr: 0.000018  min_lr: 0.000018  loss: 1.8643 (1.8792)  loss_scale: 8192.0000 (10885.0764)  weight_decay: 0.0500 (0.0500)  time: 0.6934  data: 0.1070  max mem: 13008
Epoch: [5]  [3060/3542]  eta: 0:06:25  lr: 0.000018  min_lr: 0.000018  loss: 1.9062 (1.8792)  loss_scale: 8192.0000 (10876.2783)  weight_decay: 0.0500 (0.0500)  time: 0.7320  data: 0.1460  max mem: 13008
Epoch: [5]  [3070/3542]  eta: 0:06:17  lr: 0.000018  min_lr: 0.000018  loss: 1.9072 (1.8792)  loss_scale: 8192.0000 (10867.5376)  weight_decay: 0.0500 (0.0500)  time: 0.7235  data: 0.1372  max mem: 13008
Epoch: [5]  [3080/3542]  eta: 0:06:08  lr: 0.000018  min_lr: 0.000018  loss: 1.8350 (1.8789)  loss_scale: 8192.0000 (10858.8536)  weight_decay: 0.0500 (0.0500)  time: 0.7273  data: 0.1412  max mem: 13008
Epoch: [5]  [3090/3542]  eta: 0:06:00  lr: 0.000018  min_lr: 0.000018  loss: 1.8359 (1.8787)  loss_scale: 8192.0000 (10850.2258)  weight_decay: 0.0500 (0.0500)  time: 0.7526  data: 0.1668  max mem: 13008
Epoch: [5]  [3100/3542]  eta: 0:05:52  lr: 0.000018  min_lr: 0.000018  loss: 1.8594 (1.8786)  loss_scale: 8192.0000 (10841.6537)  weight_decay: 0.0500 (0.0500)  time: 0.7434  data: 0.1577  max mem: 13008
Epoch: [5]  [3110/3542]  eta: 0:05:44  lr: 0.000018  min_lr: 0.000018  loss: 1.9160 (1.8787)  loss_scale: 8192.0000 (10833.1366)  weight_decay: 0.0500 (0.0500)  time: 0.7185  data: 0.1324  max mem: 13008
Epoch: [5]  [3120/3542]  eta: 0:05:36  lr: 0.000018  min_lr: 0.000018  loss: 1.8594 (1.8787)  loss_scale: 8192.0000 (10824.6741)  weight_decay: 0.0500 (0.0500)  time: 0.7188  data: 0.1331  max mem: 13008
Epoch: [5]  [3130/3542]  eta: 0:05:28  lr: 0.000018  min_lr: 0.000018  loss: 1.7686 (1.8784)  loss_scale: 8192.0000 (10816.2657)  weight_decay: 0.0500 (0.0500)  time: 0.7083  data: 0.1226  max mem: 13008
Epoch: [5]  [3140/3542]  eta: 0:05:20  lr: 0.000018  min_lr: 0.000018  loss: 1.7783 (1.8784)  loss_scale: 8192.0000 (10807.9109)  weight_decay: 0.0500 (0.0500)  time: 0.7162  data: 0.1292  max mem: 13008
Epoch: [5]  [3150/3542]  eta: 0:05:12  lr: 0.000018  min_lr: 0.000018  loss: 1.8633 (1.8784)  loss_scale: 8192.0000 (10799.6090)  weight_decay: 0.0500 (0.0500)  time: 0.7424  data: 0.1556  max mem: 13008
Epoch: [5]  [3160/3542]  eta: 0:05:04  lr: 0.000018  min_lr: 0.000018  loss: 1.8848 (1.8782)  loss_scale: 8192.0000 (10791.3597)  weight_decay: 0.0500 (0.0500)  time: 0.7196  data: 0.1329  max mem: 13008
Epoch: [5]  [3170/3542]  eta: 0:04:56  lr: 0.000018  min_lr: 0.000018  loss: 1.8154 (1.8780)  loss_scale: 8192.0000 (10783.1624)  weight_decay: 0.0500 (0.0500)  time: 0.6935  data: 0.1067  max mem: 13008
Epoch: [5]  [3180/3542]  eta: 0:04:48  lr: 0.000018  min_lr: 0.000018  loss: 1.8232 (1.8779)  loss_scale: 8192.0000 (10775.0167)  weight_decay: 0.0500 (0.0500)  time: 0.7009  data: 0.1144  max mem: 13008
Epoch: [5]  [3190/3542]  eta: 0:04:40  lr: 0.000018  min_lr: 0.000018  loss: 1.8418 (1.8779)  loss_scale: 8192.0000 (10766.9220)  weight_decay: 0.0500 (0.0500)  time: 0.7030  data: 0.1171  max mem: 13008
Epoch: [5]  [3200/3542]  eta: 0:04:32  lr: 0.000018  min_lr: 0.000018  loss: 1.8926 (1.8779)  loss_scale: 8192.0000 (10758.8779)  weight_decay: 0.0500 (0.0500)  time: 0.7241  data: 0.1386  max mem: 13008
Epoch: [5]  [3210/3542]  eta: 0:04:24  lr: 0.000018  min_lr: 0.000018  loss: 1.8828 (1.8777)  loss_scale: 8192.0000 (10750.8838)  weight_decay: 0.0500 (0.0500)  time: 0.7263  data: 0.1400  max mem: 13008
Epoch: [5]  [3220/3542]  eta: 0:04:15  lr: 0.000018  min_lr: 0.000018  loss: 1.7715 (1.8774)  loss_scale: 8192.0000 (10742.9395)  weight_decay: 0.0500 (0.0500)  time: 0.6579  data: 0.0713  max mem: 13008
Epoch: [5]  [3230/3542]  eta: 0:04:07  lr: 0.000018  min_lr: 0.000018  loss: 1.8105 (1.8774)  loss_scale: 8192.0000 (10735.0443)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.0993  max mem: 13008
Epoch: [5]  [3240/3542]  eta: 0:03:59  lr: 0.000018  min_lr: 0.000018  loss: 1.8818 (1.8775)  loss_scale: 8192.0000 (10727.1978)  weight_decay: 0.0500 (0.0500)  time: 0.7221  data: 0.1360  max mem: 13008
Epoch: [5]  [3250/3542]  eta: 0:03:51  lr: 0.000018  min_lr: 0.000018  loss: 1.9316 (1.8777)  loss_scale: 8192.0000 (10719.3996)  weight_decay: 0.0500 (0.0500)  time: 0.7386  data: 0.1534  max mem: 13008
Epoch: [5]  [3260/3542]  eta: 0:03:44  lr: 0.000018  min_lr: 0.000018  loss: 1.9307 (1.8777)  loss_scale: 8192.0000 (10711.6492)  weight_decay: 0.0500 (0.0500)  time: 0.7733  data: 0.1878  max mem: 13008
Epoch: [5]  [3270/3542]  eta: 0:03:35  lr: 0.000018  min_lr: 0.000018  loss: 1.9121 (1.8778)  loss_scale: 8192.0000 (10703.9462)  weight_decay: 0.0500 (0.0500)  time: 0.7348  data: 0.1488  max mem: 13008
Epoch: [5]  [3280/3542]  eta: 0:03:27  lr: 0.000018  min_lr: 0.000018  loss: 1.8887 (1.8780)  loss_scale: 8192.0000 (10696.2902)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.1181  max mem: 13008
[2023-05-16 10:45:42,310] [INFO] [logging.py:60:log_dist] [Rank 0] step=21000, skipped=20, lr=[1.7593675809564603e-05, 1.7593675809564603e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 10:45:42,313] [INFO] [timer.py:157:stop] 0/21000, SamplesPerSec=55.55433348519018
Epoch: [5]  [3290/3542]  eta: 0:03:20  lr: 0.000018  min_lr: 0.000018  loss: 1.9268 (1.8783)  loss_scale: 8192.0000 (10688.6806)  weight_decay: 0.0500 (0.0500)  time: 0.7462  data: 0.1603  max mem: 13008
Epoch: [5]  [3300/3542]  eta: 0:03:12  lr: 0.000018  min_lr: 0.000018  loss: 1.9268 (1.8782)  loss_scale: 8192.0000 (10681.1172)  weight_decay: 0.0500 (0.0500)  time: 0.7426  data: 0.1559  max mem: 13008
Epoch: [5]  [3310/3542]  eta: 0:03:04  lr: 0.000018  min_lr: 0.000018  loss: 1.8398 (1.8783)  loss_scale: 8192.0000 (10673.5995)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.1026  max mem: 13008
Epoch: [5]  [3320/3542]  eta: 0:02:56  lr: 0.000018  min_lr: 0.000018  loss: 1.8398 (1.8784)  loss_scale: 8192.0000 (10666.1271)  weight_decay: 0.0500 (0.0500)  time: 0.7385  data: 0.1517  max mem: 13008
Epoch: [5]  [3330/3542]  eta: 0:02:48  lr: 0.000018  min_lr: 0.000018  loss: 1.8887 (1.8783)  loss_scale: 8192.0000 (10658.6995)  weight_decay: 0.0500 (0.0500)  time: 0.7254  data: 0.1390  max mem: 13008
Epoch: [5]  [3340/3542]  eta: 0:02:40  lr: 0.000017  min_lr: 0.000017  loss: 1.8955 (1.8784)  loss_scale: 8192.0000 (10651.3164)  weight_decay: 0.0500 (0.0500)  time: 0.6799  data: 0.0933  max mem: 13008
Epoch: [5]  [3350/3542]  eta: 0:02:32  lr: 0.000017  min_lr: 0.000017  loss: 1.8955 (1.8786)  loss_scale: 8192.0000 (10643.9773)  weight_decay: 0.0500 (0.0500)  time: 0.7110  data: 0.1239  max mem: 13008
Epoch: [5]  [3360/3542]  eta: 0:02:24  lr: 0.000017  min_lr: 0.000017  loss: 1.8828 (1.8787)  loss_scale: 8192.0000 (10636.6819)  weight_decay: 0.0500 (0.0500)  time: 0.7195  data: 0.1326  max mem: 13008
Epoch: [5]  [3370/3542]  eta: 0:02:16  lr: 0.000017  min_lr: 0.000017  loss: 1.8574 (1.8786)  loss_scale: 8192.0000 (10629.4298)  weight_decay: 0.0500 (0.0500)  time: 0.6645  data: 0.0783  max mem: 13008
Epoch: [5]  [3380/3542]  eta: 0:02:08  lr: 0.000017  min_lr: 0.000017  loss: 1.7832 (1.8784)  loss_scale: 8192.0000 (10622.2206)  weight_decay: 0.0500 (0.0500)  time: 0.6679  data: 0.0814  max mem: 13008
Epoch: [5]  [3390/3542]  eta: 0:02:00  lr: 0.000017  min_lr: 0.000017  loss: 1.8086 (1.8784)  loss_scale: 8192.0000 (10615.0540)  weight_decay: 0.0500 (0.0500)  time: 0.7097  data: 0.1232  max mem: 13008
Epoch: [5]  [3400/3542]  eta: 0:01:52  lr: 0.000017  min_lr: 0.000017  loss: 1.9170 (1.8787)  loss_scale: 8192.0000 (10607.9294)  weight_decay: 0.0500 (0.0500)  time: 0.6750  data: 0.0883  max mem: 13008
Epoch: [5]  [3410/3542]  eta: 0:01:44  lr: 0.000017  min_lr: 0.000017  loss: 1.9521 (1.8791)  loss_scale: 8192.0000 (10600.8467)  weight_decay: 0.0500 (0.0500)  time: 0.6902  data: 0.1032  max mem: 13008
Epoch: [5]  [3420/3542]  eta: 0:01:36  lr: 0.000017  min_lr: 0.000017  loss: 1.9043 (1.8790)  loss_scale: 8192.0000 (10593.8053)  weight_decay: 0.0500 (0.0500)  time: 0.7495  data: 0.1628  max mem: 13008
Epoch: [5]  [3430/3542]  eta: 0:01:28  lr: 0.000017  min_lr: 0.000017  loss: 1.8604 (1.8789)  loss_scale: 8192.0000 (10586.8050)  weight_decay: 0.0500 (0.0500)  time: 0.7295  data: 0.1433  max mem: 13008
Epoch: [5]  [3440/3542]  eta: 0:01:20  lr: 0.000017  min_lr: 0.000017  loss: 1.8018 (1.8788)  loss_scale: 8192.0000 (10579.8454)  weight_decay: 0.0500 (0.0500)  time: 0.6954  data: 0.1098  max mem: 13008
[2023-05-16 10:47:29,444] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 10:47:29,444] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [5]  [3450/3542]  eta: 0:01:12  lr: 0.000017  min_lr: 0.000017  loss: 1.8477 (1.8789)  loss_scale: 8192.0000 (10594.2904)  weight_decay: 0.0500 (0.0500)  time: 0.7110  data: 0.1256  max mem: 13008
Epoch: [5]  [3460/3542]  eta: 0:01:04  lr: 0.000017  min_lr: 0.000017  loss: 1.8486 (1.8789)  loss_scale: 16384.0000 (10611.0188)  weight_decay: 0.0500 (0.0500)  time: 0.7354  data: 0.1496  max mem: 13008
Epoch: [5]  [3470/3542]  eta: 0:00:56  lr: 0.000017  min_lr: 0.000017  loss: 1.9004 (1.8788)  loss_scale: 16384.0000 (10627.6508)  weight_decay: 0.0500 (0.0500)  time: 0.6746  data: 0.0878  max mem: 13008
Epoch: [5]  [3480/3542]  eta: 0:00:48  lr: 0.000017  min_lr: 0.000017  loss: 1.9004 (1.8789)  loss_scale: 16384.0000 (10644.1873)  weight_decay: 0.0500 (0.0500)  time: 0.6532  data: 0.0656  max mem: 13008
Epoch: [5]  [3490/3542]  eta: 0:00:40  lr: 0.000017  min_lr: 0.000017  loss: 1.8086 (1.8787)  loss_scale: 16384.0000 (10660.6290)  weight_decay: 0.0500 (0.0500)  time: 0.6874  data: 0.1006  max mem: 13008
Epoch: [5]  [3500/3542]  eta: 0:00:33  lr: 0.000017  min_lr: 0.000017  loss: 1.8125 (1.8788)  loss_scale: 16384.0000 (10676.9769)  weight_decay: 0.0500 (0.0500)  time: 0.6833  data: 0.0967  max mem: 13008
Epoch: [5]  [3510/3542]  eta: 0:00:25  lr: 0.000017  min_lr: 0.000017  loss: 1.8652 (1.8788)  loss_scale: 16384.0000 (10693.2316)  weight_decay: 0.0500 (0.0500)  time: 0.7041  data: 0.1172  max mem: 13008
Epoch: [5]  [3520/3542]  eta: 0:00:17  lr: 0.000017  min_lr: 0.000017  loss: 1.8018 (1.8785)  loss_scale: 16384.0000 (10709.3939)  weight_decay: 0.0500 (0.0500)  time: 0.7231  data: 0.1355  max mem: 13008
Epoch: [5]  [3530/3542]  eta: 0:00:09  lr: 0.000017  min_lr: 0.000017  loss: 1.8291 (1.8786)  loss_scale: 16384.0000 (10725.4647)  weight_decay: 0.0500 (0.0500)  time: 0.7152  data: 0.1273  max mem: 13008
[2023-05-16 10:48:34,706] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 21246
[2023-05-16 10:48:34,706] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 10:48:34,706] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [5]  [3540/3542]  eta: 0:00:01  lr: 0.000017  min_lr: 0.000017  loss: 1.8887 (1.8785)  loss_scale: 16384.0000 (10729.8774)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.0555  max mem: 13008
Epoch: [5]  [3541/3542]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000017  loss: 1.8750 (1.8785)  loss_scale: 16384.0000 (10729.1609)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0555  max mem: 13008
Epoch: [5] Total time: 0:46:28 (0.7872 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000017  loss: 1.8750 (1.8785)  loss_scale: 16384.0000 (10729.1609)  weight_decay: 0.0500 (0.0500)
[2023-05-16 10:48:38,311] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-5/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:37:16    time: 14.3364  data: 10.4904  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:11:12    time: 4.6033  data: 0.9539  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 20/156]  eta: 0:09:22    time: 3.6294  data: 0.0002  max mem: 13008
Test:  [ 30/156]  eta: 0:08:25    time: 3.6840  data: 0.0002  max mem: 13008
Test:  [ 40/156]  eta: 0:07:30    time: 3.6115  data: 0.0003  max mem: 13008
Test:  [ 50/156]  eta: 0:06:47    time: 3.5960  data: 0.0002  max mem: 13008
Test:  [ 60/156]  eta: 0:06:05    time: 3.6497  data: 0.0002  max mem: 13008
Test:  [ 70/156]  eta: 0:05:25    time: 3.6338  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:46    time: 3.6405  data: 0.0002  max mem: 13008
Test:  [ 90/156]  eta: 0:04:07    time: 3.6394  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:29    time: 3.6637  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:52    time: 3.6867  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:14    time: 3.7455  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:37    time: 3.7468  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:00:59    time: 3.6698  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:22    time: 3.6566  data: 0.0002  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.6604  data: 0.0002  max mem: 13008
Test: Total time: 0:09:42 (3.7331 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e5.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e5.json and result file is ./output_freeze/coco_captioning_result_val_e5.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e5.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1149674.49 tokens per second.
PTBTokenizer tokenized 62726 tokens at 440014.95 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 51259, 'reflen': 49364, 'guess': [51259, 46267, 41275, 36283], 'correct': [33512, 17164, 7647, 3133]}
ratio: 1.0383882991653628
Bleu_1: 0.654
Bleu_2: 0.492
Bleu_3: 0.356
Bleu_4: 0.250
computing METEOR score...
METEOR: 0.238
computing Rouge score...
ROUGE_L: 0.506
computing CIDEr score...
CIDEr: 0.805
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [19.204 seconds]
SPICE evaluation took: 31.41 s
SPICE: 0.175
Performance of the network on the 5000 val images: 0.8%
Max performance: 0.81%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [6]  [   0/3542]  eta: 19:36:11  lr: 0.000017  min_lr: 0.000017  loss: 1.6934 (1.6934)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 19.9241  data: 19.2903  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [6]  [  10/3542]  eta: 3:37:01  lr: 0.000017  min_lr: 0.000017  loss: 1.8330 (1.8366)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 3.6866  data: 3.0977  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [6]  [  20/3542]  eta: 2:47:08  lr: 0.000017  min_lr: 0.000017  loss: 1.8535 (1.8464)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.9936  data: 1.4106  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [6]  [  30/3542]  eta: 2:21:00  lr: 0.000017  min_lr: 0.000017  loss: 1.8535 (1.8507)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.7062  data: 1.1240  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [6]  [  40/3542]  eta: 2:03:34  lr: 0.000017  min_lr: 0.000017  loss: 1.7725 (1.8348)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3504  data: 0.7658  max mem: 13008
Epoch: [6]  [  50/3542]  eta: 1:52:27  lr: 0.000017  min_lr: 0.000017  loss: 1.7725 (1.8373)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1936  data: 0.6098  max mem: 13008
Epoch: [6]  [  60/3542]  eta: 1:44:43  lr: 0.000017  min_lr: 0.000017  loss: 1.8271 (1.8557)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1640  data: 0.5826  max mem: 13008
Epoch: [6]  [  70/3542]  eta: 1:35:20  lr: 0.000017  min_lr: 0.000017  loss: 1.9365 (1.8685)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9220  data: 0.3399  max mem: 13008
Epoch: [6]  [  80/3542]  eta: 1:30:42  lr: 0.000017  min_lr: 0.000017  loss: 1.9014 (1.8763)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8622  data: 0.2791  max mem: 13008
Epoch: [6]  [  90/3542]  eta: 1:26:07  lr: 0.000017  min_lr: 0.000017  loss: 1.8828 (1.8795)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9615  data: 0.3785  max mem: 13008
Epoch: [6]  [ 100/3542]  eta: 1:22:07  lr: 0.000017  min_lr: 0.000017  loss: 1.8994 (1.8824)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8636  data: 0.2803  max mem: 13008
Epoch: [6]  [ 110/3542]  eta: 1:19:38  lr: 0.000017  min_lr: 0.000017  loss: 1.9111 (1.8852)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9163  data: 0.3304  max mem: 13008
Epoch: [6]  [ 120/3542]  eta: 1:17:03  lr: 0.000017  min_lr: 0.000017  loss: 1.8477 (1.8795)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9446  data: 0.3590  max mem: 13008
Epoch: [6]  [ 130/3542]  eta: 1:14:41  lr: 0.000017  min_lr: 0.000017  loss: 1.8311 (1.8777)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8767  data: 0.2927  max mem: 13008
Epoch: [6]  [ 140/3542]  eta: 1:12:26  lr: 0.000017  min_lr: 0.000017  loss: 1.8311 (1.8782)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8332  data: 0.2486  max mem: 13008
Epoch: [6]  [ 150/3542]  eta: 1:10:22  lr: 0.000017  min_lr: 0.000017  loss: 1.8086 (1.8756)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7946  data: 0.2103  max mem: 13008
Epoch: [6]  [ 160/3542]  eta: 1:08:21  lr: 0.000017  min_lr: 0.000017  loss: 1.7822 (1.8735)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7542  data: 0.1694  max mem: 13008
Epoch: [6]  [ 170/3542]  eta: 1:06:56  lr: 0.000017  min_lr: 0.000017  loss: 1.7822 (1.8668)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7858  data: 0.2011  max mem: 13008
Epoch: [6]  [ 180/3542]  eta: 1:05:18  lr: 0.000017  min_lr: 0.000017  loss: 1.8154 (1.8682)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7871  data: 0.2024  max mem: 13008
Epoch: [6]  [ 190/3542]  eta: 1:03:45  lr: 0.000017  min_lr: 0.000017  loss: 1.9033 (1.8724)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7138  data: 0.1280  max mem: 13008
Epoch: [6]  [ 200/3542]  eta: 1:02:27  lr: 0.000017  min_lr: 0.000017  loss: 1.8945 (1.8719)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7201  data: 0.1344  max mem: 13008
Epoch: [6]  [ 210/3542]  eta: 1:01:13  lr: 0.000017  min_lr: 0.000017  loss: 1.8164 (1.8669)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7324  data: 0.1470  max mem: 13008
Epoch: [6]  [ 220/3542]  eta: 1:00:06  lr: 0.000017  min_lr: 0.000017  loss: 1.8184 (1.8657)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.1434  max mem: 13008
Epoch: [6]  [ 230/3542]  eta: 0:58:58  lr: 0.000017  min_lr: 0.000017  loss: 1.8691 (1.8673)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7094  data: 0.1253  max mem: 13008
Epoch: [6]  [ 240/3542]  eta: 0:58:00  lr: 0.000017  min_lr: 0.000017  loss: 1.8857 (1.8692)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.1196  max mem: 13008
Epoch: [6]  [ 250/3542]  eta: 0:57:07  lr: 0.000017  min_lr: 0.000017  loss: 1.8438 (1.8671)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7253  data: 0.1405  max mem: 13008
Epoch: [6]  [ 260/3542]  eta: 0:56:16  lr: 0.000017  min_lr: 0.000017  loss: 1.7959 (1.8627)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7256  data: 0.1401  max mem: 13008
Epoch: [6]  [ 270/3542]  eta: 0:55:34  lr: 0.000017  min_lr: 0.000017  loss: 1.7959 (1.8610)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7420  data: 0.1557  max mem: 13008
Epoch: [6]  [ 280/3542]  eta: 0:54:56  lr: 0.000017  min_lr: 0.000017  loss: 1.7910 (1.8583)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7718  data: 0.1852  max mem: 13008
Epoch: [6]  [ 290/3542]  eta: 0:54:05  lr: 0.000017  min_lr: 0.000017  loss: 1.8867 (1.8609)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7152  data: 0.1292  max mem: 13008
Epoch: [6]  [ 300/3542]  eta: 0:53:29  lr: 0.000017  min_lr: 0.000017  loss: 1.8887 (1.8629)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6998  data: 0.1145  max mem: 13008
Epoch: [6]  [ 310/3542]  eta: 0:52:42  lr: 0.000017  min_lr: 0.000017  loss: 1.8740 (1.8619)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6925  data: 0.1076  max mem: 13008
Epoch: [6]  [ 320/3542]  eta: 0:51:55  lr: 0.000017  min_lr: 0.000017  loss: 1.8076 (1.8615)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0376  max mem: 13008
Epoch: [6]  [ 330/3542]  eta: 0:51:24  lr: 0.000016  min_lr: 0.000016  loss: 1.8135 (1.8611)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6790  data: 0.0931  max mem: 13008
Epoch: [6]  [ 340/3542]  eta: 0:50:51  lr: 0.000016  min_lr: 0.000016  loss: 1.7666 (1.8620)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7292  data: 0.1431  max mem: 13008
Epoch: [6]  [ 350/3542]  eta: 0:50:17  lr: 0.000016  min_lr: 0.000016  loss: 1.7666 (1.8618)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6991  data: 0.1126  max mem: 13008
Epoch: [6]  [ 360/3542]  eta: 0:49:44  lr: 0.000016  min_lr: 0.000016  loss: 1.8008 (1.8627)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.0898  max mem: 13008
Epoch: [6]  [ 370/3542]  eta: 0:49:13  lr: 0.000016  min_lr: 0.000016  loss: 1.7988 (1.8615)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6770  data: 0.0911  max mem: 13008
Epoch: [6]  [ 380/3542]  eta: 0:48:36  lr: 0.000016  min_lr: 0.000016  loss: 1.8359 (1.8623)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6423  data: 0.0556  max mem: 13008
Epoch: [6]  [ 390/3542]  eta: 0:48:10  lr: 0.000016  min_lr: 0.000016  loss: 1.8779 (1.8613)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.0703  max mem: 13008
Epoch: [6]  [ 400/3542]  eta: 0:47:43  lr: 0.000016  min_lr: 0.000016  loss: 1.8779 (1.8610)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7030  data: 0.1176  max mem: 13008
Epoch: [6]  [ 410/3542]  eta: 0:47:16  lr: 0.000016  min_lr: 0.000016  loss: 1.8496 (1.8612)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6823  data: 0.0968  max mem: 13008
Epoch: [6]  [ 420/3542]  eta: 0:46:49  lr: 0.000016  min_lr: 0.000016  loss: 1.8916 (1.8626)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6695  data: 0.0830  max mem: 13008
Epoch: [6]  [ 430/3542]  eta: 0:46:26  lr: 0.000016  min_lr: 0.000016  loss: 1.8311 (1.8617)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6860  data: 0.0993  max mem: 13008
Epoch: [6]  [ 440/3542]  eta: 0:46:01  lr: 0.000016  min_lr: 0.000016  loss: 1.7686 (1.8615)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6887  data: 0.1031  max mem: 13008
Epoch: [6]  [ 450/3542]  eta: 0:45:43  lr: 0.000016  min_lr: 0.000016  loss: 1.7939 (1.8628)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7132  data: 0.1269  max mem: 13008
Epoch: [6]  [ 460/3542]  eta: 0:45:19  lr: 0.000016  min_lr: 0.000016  loss: 1.8623 (1.8617)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7068  data: 0.1206  max mem: 13008
Epoch: [6]  [ 470/3542]  eta: 0:44:56  lr: 0.000016  min_lr: 0.000016  loss: 1.8467 (1.8612)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.0790  max mem: 13008
Epoch: [6]  [ 480/3542]  eta: 0:44:37  lr: 0.000016  min_lr: 0.000016  loss: 1.8242 (1.8598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6931  data: 0.1060  max mem: 13008
Epoch: [6]  [ 490/3542]  eta: 0:44:14  lr: 0.000016  min_lr: 0.000016  loss: 1.8242 (1.8595)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6783  data: 0.0918  max mem: 13008
Epoch: [6]  [ 500/3542]  eta: 0:43:53  lr: 0.000016  min_lr: 0.000016  loss: 1.8281 (1.8598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6548  data: 0.0680  max mem: 13008
Epoch: [6]  [ 510/3542]  eta: 0:43:34  lr: 0.000016  min_lr: 0.000016  loss: 1.8281 (1.8592)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6802  data: 0.0936  max mem: 13008
Epoch: [6]  [ 520/3542]  eta: 0:43:14  lr: 0.000016  min_lr: 0.000016  loss: 1.8291 (1.8587)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6796  data: 0.0933  max mem: 13008
Epoch: [6]  [ 530/3542]  eta: 0:42:56  lr: 0.000016  min_lr: 0.000016  loss: 1.8291 (1.8593)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6801  data: 0.0935  max mem: 13008
Epoch: [6]  [ 540/3542]  eta: 0:42:40  lr: 0.000016  min_lr: 0.000016  loss: 1.8408 (1.8598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7056  data: 0.1187  max mem: 13008
Epoch: [6]  [ 550/3542]  eta: 0:42:18  lr: 0.000016  min_lr: 0.000016  loss: 1.8193 (1.8585)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.0738  max mem: 13008
Epoch: [6]  [ 560/3542]  eta: 0:41:57  lr: 0.000016  min_lr: 0.000016  loss: 1.7998 (1.8593)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0277  max mem: 13008
Epoch: [6]  [ 570/3542]  eta: 0:41:36  lr: 0.000016  min_lr: 0.000016  loss: 1.8389 (1.8589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0201  max mem: 13008
Epoch: [6]  [ 580/3542]  eta: 0:41:19  lr: 0.000016  min_lr: 0.000016  loss: 1.8389 (1.8606)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.0508  max mem: 13008
Epoch: [6]  [ 590/3542]  eta: 0:41:01  lr: 0.000016  min_lr: 0.000016  loss: 1.9346 (1.8623)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0743  max mem: 13008
Epoch: [6]  [ 600/3542]  eta: 0:40:46  lr: 0.000016  min_lr: 0.000016  loss: 1.9014 (1.8620)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6630  data: 0.0761  max mem: 13008
Epoch: [6]  [ 610/3542]  eta: 0:40:30  lr: 0.000016  min_lr: 0.000016  loss: 1.8447 (1.8622)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6875  data: 0.1007  max mem: 13008
Epoch: [6]  [ 620/3542]  eta: 0:40:17  lr: 0.000016  min_lr: 0.000016  loss: 1.8408 (1.8618)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7021  data: 0.1162  max mem: 13008
Epoch: [6]  [ 630/3542]  eta: 0:40:03  lr: 0.000016  min_lr: 0.000016  loss: 1.8271 (1.8621)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7136  data: 0.1277  max mem: 13008
Epoch: [6]  [ 640/3542]  eta: 0:39:45  lr: 0.000016  min_lr: 0.000016  loss: 1.8076 (1.8612)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.0776  max mem: 13008
Epoch: [6]  [ 650/3542]  eta: 0:39:27  lr: 0.000016  min_lr: 0.000016  loss: 1.7930 (1.8607)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6032  data: 0.0161  max mem: 13008
Epoch: [6]  [ 660/3542]  eta: 0:39:10  lr: 0.000016  min_lr: 0.000016  loss: 1.8076 (1.8604)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0175  max mem: 13008
Epoch: [6]  [ 670/3542]  eta: 0:38:56  lr: 0.000016  min_lr: 0.000016  loss: 1.8076 (1.8595)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.0662  max mem: 13008
Epoch: [6]  [ 680/3542]  eta: 0:38:42  lr: 0.000016  min_lr: 0.000016  loss: 1.8301 (1.8595)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6804  data: 0.0943  max mem: 13008
Epoch: [6]  [ 690/3542]  eta: 0:38:28  lr: 0.000016  min_lr: 0.000016  loss: 1.8477 (1.8601)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.0869  max mem: 13008
Epoch: [6]  [ 700/3542]  eta: 0:38:14  lr: 0.000016  min_lr: 0.000016  loss: 1.8936 (1.8607)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6660  data: 0.0806  max mem: 13008
Epoch: [6]  [ 710/3542]  eta: 0:38:03  lr: 0.000016  min_lr: 0.000016  loss: 1.8447 (1.8600)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6955  data: 0.1107  max mem: 13008
Epoch: [6]  [ 720/3542]  eta: 0:37:51  lr: 0.000016  min_lr: 0.000016  loss: 1.7871 (1.8597)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7171  data: 0.1316  max mem: 13008
Epoch: [6]  [ 730/3542]  eta: 0:37:38  lr: 0.000016  min_lr: 0.000016  loss: 1.7812 (1.8592)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6959  data: 0.1093  max mem: 13008
Epoch: [6]  [ 740/3542]  eta: 0:37:23  lr: 0.000016  min_lr: 0.000016  loss: 1.8252 (1.8589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.0548  max mem: 13008
[2023-05-16 11:09:14,646] [INFO] [logging.py:60:log_dist] [Rank 0] step=22000, skipped=21, lr=[1.5710581817233738e-05, 1.5710581817233738e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 11:09:14,649] [INFO] [timer.py:157:stop] 0/22000, SamplesPerSec=55.55380096598656
Epoch: [6]  [ 750/3542]  eta: 0:37:10  lr: 0.000016  min_lr: 0.000016  loss: 1.8291 (1.8593)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0479  max mem: 13008
Epoch: [6]  [ 760/3542]  eta: 0:36:58  lr: 0.000016  min_lr: 0.000016  loss: 1.8682 (1.8598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6823  data: 0.0950  max mem: 13008
Epoch: [6]  [ 770/3542]  eta: 0:36:47  lr: 0.000016  min_lr: 0.000016  loss: 1.8643 (1.8593)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7020  data: 0.1153  max mem: 13008
Epoch: [6]  [ 780/3542]  eta: 0:36:36  lr: 0.000016  min_lr: 0.000016  loss: 1.8525 (1.8597)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7082  data: 0.1223  max mem: 13008
Epoch: [6]  [ 790/3542]  eta: 0:36:25  lr: 0.000016  min_lr: 0.000016  loss: 1.8467 (1.8595)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7110  data: 0.1246  max mem: 13008
Epoch: [6]  [ 800/3542]  eta: 0:36:14  lr: 0.000016  min_lr: 0.000016  loss: 1.8467 (1.8602)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7161  data: 0.1292  max mem: 13008
Epoch: [6]  [ 810/3542]  eta: 0:36:00  lr: 0.000016  min_lr: 0.000016  loss: 1.8574 (1.8607)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6658  data: 0.0800  max mem: 13008
Epoch: [6]  [ 820/3542]  eta: 0:35:49  lr: 0.000016  min_lr: 0.000016  loss: 1.8369 (1.8597)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0577  max mem: 13008
Epoch: [6]  [ 830/3542]  eta: 0:35:39  lr: 0.000016  min_lr: 0.000016  loss: 1.8496 (1.8604)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7004  data: 0.1151  max mem: 13008
Epoch: [6]  [ 840/3542]  eta: 0:35:29  lr: 0.000016  min_lr: 0.000016  loss: 1.8945 (1.8611)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7298  data: 0.1438  max mem: 13008
Epoch: [6]  [ 850/3542]  eta: 0:35:17  lr: 0.000016  min_lr: 0.000016  loss: 1.8945 (1.8610)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7007  data: 0.1145  max mem: 13008
Epoch: [6]  [ 860/3542]  eta: 0:35:05  lr: 0.000016  min_lr: 0.000016  loss: 1.7949 (1.8608)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0745  max mem: 13008
Epoch: [6]  [ 870/3542]  eta: 0:34:53  lr: 0.000015  min_lr: 0.000015  loss: 1.7686 (1.8609)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6409  data: 0.0551  max mem: 13008
Epoch: [6]  [ 880/3542]  eta: 0:34:41  lr: 0.000015  min_lr: 0.000015  loss: 1.8594 (1.8619)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6377  data: 0.0508  max mem: 13008
Epoch: [6]  [ 890/3542]  eta: 0:34:30  lr: 0.000015  min_lr: 0.000015  loss: 1.9121 (1.8617)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6609  data: 0.0733  max mem: 13008
Epoch: [6]  [ 900/3542]  eta: 0:34:19  lr: 0.000015  min_lr: 0.000015  loss: 1.8037 (1.8612)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6751  data: 0.0889  max mem: 13008
Epoch: [6]  [ 910/3542]  eta: 0:34:07  lr: 0.000015  min_lr: 0.000015  loss: 1.8545 (1.8618)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.0657  max mem: 13008
Epoch: [6]  [ 920/3542]  eta: 0:33:56  lr: 0.000015  min_lr: 0.000015  loss: 1.8848 (1.8620)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.0660  max mem: 13008
Epoch: [6]  [ 930/3542]  eta: 0:33:46  lr: 0.000015  min_lr: 0.000015  loss: 1.8809 (1.8622)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.0997  max mem: 13008
Epoch: [6]  [ 940/3542]  eta: 0:33:35  lr: 0.000015  min_lr: 0.000015  loss: 1.8301 (1.8619)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6853  data: 0.0989  max mem: 13008
Epoch: [6]  [ 950/3542]  eta: 0:33:25  lr: 0.000015  min_lr: 0.000015  loss: 1.8193 (1.8620)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6825  data: 0.0953  max mem: 13008
Epoch: [6]  [ 960/3542]  eta: 0:33:14  lr: 0.000015  min_lr: 0.000015  loss: 1.8359 (1.8620)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6769  data: 0.0901  max mem: 13008
Epoch: [6]  [ 970/3542]  eta: 0:33:03  lr: 0.000015  min_lr: 0.000015  loss: 1.8340 (1.8612)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.0584  max mem: 13008
Epoch: [6]  [ 980/3542]  eta: 0:32:51  lr: 0.000015  min_lr: 0.000015  loss: 1.8135 (1.8614)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.0415  max mem: 13008
Epoch: [6]  [ 990/3542]  eta: 0:32:42  lr: 0.000015  min_lr: 0.000015  loss: 1.9062 (1.8615)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6545  data: 0.0692  max mem: 13008
[2023-05-16 11:12:02,874] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 11:12:02,874] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [6]  [1000/3542]  eta: 0:32:32  lr: 0.000015  min_lr: 0.000015  loss: 1.8818 (1.8614)  loss_scale: 8192.0000 (8241.1029)  weight_decay: 0.0500 (0.0500)  time: 0.6938  data: 0.1084  max mem: 13008
Epoch: [6]  [1010/3542]  eta: 0:32:21  lr: 0.000015  min_lr: 0.000015  loss: 1.8877 (1.8615)  loss_scale: 16384.0000 (8321.6459)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.0822  max mem: 13008
Epoch: [6]  [1020/3542]  eta: 0:32:12  lr: 0.000015  min_lr: 0.000015  loss: 1.8936 (1.8618)  loss_scale: 16384.0000 (8400.6112)  weight_decay: 0.0500 (0.0500)  time: 0.6686  data: 0.0822  max mem: 13008
Epoch: [6]  [1030/3542]  eta: 0:32:02  lr: 0.000015  min_lr: 0.000015  loss: 1.8662 (1.8614)  loss_scale: 16384.0000 (8478.0446)  weight_decay: 0.0500 (0.0500)  time: 0.6899  data: 0.1040  max mem: 13008
Epoch: [6]  [1040/3542]  eta: 0:31:53  lr: 0.000015  min_lr: 0.000015  loss: 1.8662 (1.8616)  loss_scale: 16384.0000 (8553.9904)  weight_decay: 0.0500 (0.0500)  time: 0.6861  data: 0.1001  max mem: 13008
Epoch: [6]  [1050/3542]  eta: 0:31:44  lr: 0.000015  min_lr: 0.000015  loss: 1.9014 (1.8620)  loss_scale: 16384.0000 (8628.4910)  weight_decay: 0.0500 (0.0500)  time: 0.7027  data: 0.1164  max mem: 13008
Epoch: [6]  [1060/3542]  eta: 0:31:34  lr: 0.000015  min_lr: 0.000015  loss: 1.8760 (1.8623)  loss_scale: 16384.0000 (8701.5872)  weight_decay: 0.0500 (0.0500)  time: 0.7021  data: 0.1153  max mem: 13008
Epoch: [6]  [1070/3542]  eta: 0:31:26  lr: 0.000015  min_lr: 0.000015  loss: 1.8477 (1.8624)  loss_scale: 16384.0000 (8773.3184)  weight_decay: 0.0500 (0.0500)  time: 0.7015  data: 0.1153  max mem: 13008
Epoch: [6]  [1080/3542]  eta: 0:31:16  lr: 0.000015  min_lr: 0.000015  loss: 1.8555 (1.8622)  loss_scale: 16384.0000 (8843.7225)  weight_decay: 0.0500 (0.0500)  time: 0.6856  data: 0.0999  max mem: 13008
Epoch: [6]  [1090/3542]  eta: 0:31:06  lr: 0.000015  min_lr: 0.000015  loss: 1.8398 (1.8617)  loss_scale: 16384.0000 (8912.8359)  weight_decay: 0.0500 (0.0500)  time: 0.6621  data: 0.0762  max mem: 13008
Epoch: [6]  [1100/3542]  eta: 0:30:57  lr: 0.000015  min_lr: 0.000015  loss: 1.8281 (1.8621)  loss_scale: 16384.0000 (8980.6939)  weight_decay: 0.0500 (0.0500)  time: 0.6780  data: 0.0918  max mem: 13008
Epoch: [6]  [1110/3542]  eta: 0:30:48  lr: 0.000015  min_lr: 0.000015  loss: 1.8496 (1.8621)  loss_scale: 16384.0000 (9047.3303)  weight_decay: 0.0500 (0.0500)  time: 0.6918  data: 0.1060  max mem: 13008
Epoch: [6]  [1120/3542]  eta: 0:30:37  lr: 0.000015  min_lr: 0.000015  loss: 1.8555 (1.8621)  loss_scale: 16384.0000 (9112.7779)  weight_decay: 0.0500 (0.0500)  time: 0.6637  data: 0.0778  max mem: 13008
Epoch: [6]  [1130/3542]  eta: 0:30:28  lr: 0.000015  min_lr: 0.000015  loss: 1.8555 (1.8619)  loss_scale: 16384.0000 (9177.0681)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0704  max mem: 13008
Epoch: [6]  [1140/3542]  eta: 0:30:18  lr: 0.000015  min_lr: 0.000015  loss: 1.8525 (1.8619)  loss_scale: 16384.0000 (9240.2314)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.0689  max mem: 13008
Epoch: [6]  [1150/3542]  eta: 0:30:08  lr: 0.000015  min_lr: 0.000015  loss: 1.8564 (1.8617)  loss_scale: 16384.0000 (9302.2971)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.0674  max mem: 13008
Epoch: [6]  [1160/3542]  eta: 0:29:59  lr: 0.000015  min_lr: 0.000015  loss: 1.8525 (1.8613)  loss_scale: 16384.0000 (9363.2937)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0733  max mem: 13008
Epoch: [6]  [1170/3542]  eta: 0:29:48  lr: 0.000015  min_lr: 0.000015  loss: 1.8262 (1.8612)  loss_scale: 16384.0000 (9423.2485)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0448  max mem: 13008
Epoch: [6]  [1180/3542]  eta: 0:29:39  lr: 0.000015  min_lr: 0.000015  loss: 1.8428 (1.8613)  loss_scale: 16384.0000 (9482.1880)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0543  max mem: 13008
[2023-05-16 11:14:09,293] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 22435
[2023-05-16 11:14:09,293] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 11:14:09,293] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [6]  [1190/3542]  eta: 0:29:29  lr: 0.000015  min_lr: 0.000015  loss: 1.8623 (1.8606)  loss_scale: 16384.0000 (9485.1117)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.0616  max mem: 13008
Epoch: [6]  [1200/3542]  eta: 0:29:20  lr: 0.000015  min_lr: 0.000015  loss: 1.7168 (1.8595)  loss_scale: 8192.0000 (9474.3447)  weight_decay: 0.0500 (0.0500)  time: 0.6480  data: 0.0617  max mem: 13008
Epoch: [6]  [1210/3542]  eta: 0:29:10  lr: 0.000015  min_lr: 0.000015  loss: 1.8584 (1.8600)  loss_scale: 8192.0000 (9463.7556)  weight_decay: 0.0500 (0.0500)  time: 0.6611  data: 0.0750  max mem: 13008
Epoch: [6]  [1220/3542]  eta: 0:29:02  lr: 0.000015  min_lr: 0.000015  loss: 1.9111 (1.8602)  loss_scale: 8192.0000 (9453.3399)  weight_decay: 0.0500 (0.0500)  time: 0.6742  data: 0.0881  max mem: 13008
Epoch: [6]  [1230/3542]  eta: 0:28:53  lr: 0.000015  min_lr: 0.000015  loss: 1.8740 (1.8601)  loss_scale: 8192.0000 (9443.0934)  weight_decay: 0.0500 (0.0500)  time: 0.6786  data: 0.0921  max mem: 13008
Epoch: [6]  [1240/3542]  eta: 0:28:44  lr: 0.000015  min_lr: 0.000015  loss: 1.8330 (1.8602)  loss_scale: 8192.0000 (9433.0121)  weight_decay: 0.0500 (0.0500)  time: 0.6872  data: 0.1008  max mem: 13008
Epoch: [6]  [1250/3542]  eta: 0:28:36  lr: 0.000015  min_lr: 0.000015  loss: 1.8330 (1.8604)  loss_scale: 8192.0000 (9423.0919)  weight_decay: 0.0500 (0.0500)  time: 0.7067  data: 0.1211  max mem: 13008
Epoch: [6]  [1260/3542]  eta: 0:28:28  lr: 0.000015  min_lr: 0.000015  loss: 1.8945 (1.8603)  loss_scale: 8192.0000 (9413.3291)  weight_decay: 0.0500 (0.0500)  time: 0.7230  data: 0.1369  max mem: 13008
Epoch: [6]  [1270/3542]  eta: 0:28:18  lr: 0.000015  min_lr: 0.000015  loss: 1.8945 (1.8604)  loss_scale: 8192.0000 (9403.7199)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0889  max mem: 13008
Epoch: [6]  [1280/3542]  eta: 0:28:10  lr: 0.000015  min_lr: 0.000015  loss: 1.9131 (1.8608)  loss_scale: 8192.0000 (9394.2607)  weight_decay: 0.0500 (0.0500)  time: 0.6551  data: 0.0696  max mem: 13008
Epoch: [6]  [1290/3542]  eta: 0:28:00  lr: 0.000015  min_lr: 0.000015  loss: 1.8477 (1.8607)  loss_scale: 8192.0000 (9384.9481)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.0660  max mem: 13008
Epoch: [6]  [1300/3542]  eta: 0:27:53  lr: 0.000015  min_lr: 0.000015  loss: 1.8389 (1.8609)  loss_scale: 8192.0000 (9375.7786)  weight_decay: 0.0500 (0.0500)  time: 0.6698  data: 0.0828  max mem: 13008
Epoch: [6]  [1310/3542]  eta: 0:27:45  lr: 0.000015  min_lr: 0.000015  loss: 1.9131 (1.8612)  loss_scale: 8192.0000 (9366.7490)  weight_decay: 0.0500 (0.0500)  time: 0.7269  data: 0.1404  max mem: 13008
Epoch: [6]  [1320/3542]  eta: 0:27:36  lr: 0.000015  min_lr: 0.000015  loss: 1.8906 (1.8613)  loss_scale: 8192.0000 (9357.8562)  weight_decay: 0.0500 (0.0500)  time: 0.7105  data: 0.1242  max mem: 13008
Epoch: [6]  [1330/3542]  eta: 0:27:28  lr: 0.000015  min_lr: 0.000015  loss: 1.8652 (1.8614)  loss_scale: 8192.0000 (9349.0969)  weight_decay: 0.0500 (0.0500)  time: 0.6809  data: 0.0947  max mem: 13008
Epoch: [6]  [1340/3542]  eta: 0:27:18  lr: 0.000015  min_lr: 0.000015  loss: 1.8594 (1.8611)  loss_scale: 8192.0000 (9340.4683)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0551  max mem: 13008
Epoch: [6]  [1350/3542]  eta: 0:27:09  lr: 0.000015  min_lr: 0.000015  loss: 1.8594 (1.8608)  loss_scale: 8192.0000 (9331.9674)  weight_decay: 0.0500 (0.0500)  time: 0.6378  data: 0.0506  max mem: 13008
Epoch: [6]  [1360/3542]  eta: 0:27:01  lr: 0.000015  min_lr: 0.000015  loss: 1.8135 (1.8609)  loss_scale: 8192.0000 (9323.5915)  weight_decay: 0.0500 (0.0500)  time: 0.6715  data: 0.0858  max mem: 13008
Epoch: [6]  [1370/3542]  eta: 0:26:52  lr: 0.000015  min_lr: 0.000015  loss: 1.8193 (1.8607)  loss_scale: 8192.0000 (9315.3377)  weight_decay: 0.0500 (0.0500)  time: 0.6820  data: 0.0956  max mem: 13008
Epoch: [6]  [1380/3542]  eta: 0:26:45  lr: 0.000015  min_lr: 0.000015  loss: 1.8770 (1.8609)  loss_scale: 8192.0000 (9307.2035)  weight_decay: 0.0500 (0.0500)  time: 0.7162  data: 0.1296  max mem: 13008
Epoch: [6]  [1390/3542]  eta: 0:26:36  lr: 0.000015  min_lr: 0.000015  loss: 1.8770 (1.8610)  loss_scale: 8192.0000 (9299.1862)  weight_decay: 0.0500 (0.0500)  time: 0.7046  data: 0.1180  max mem: 13008
Epoch: [6]  [1400/3542]  eta: 0:26:28  lr: 0.000015  min_lr: 0.000015  loss: 1.8457 (1.8610)  loss_scale: 8192.0000 (9291.2834)  weight_decay: 0.0500 (0.0500)  time: 0.6698  data: 0.0836  max mem: 13008
Epoch: [6]  [1410/3542]  eta: 0:26:20  lr: 0.000014  min_lr: 0.000014  loss: 1.8750 (1.8611)  loss_scale: 8192.0000 (9283.4926)  weight_decay: 0.0500 (0.0500)  time: 0.6947  data: 0.1095  max mem: 13008
Epoch: [6]  [1420/3542]  eta: 0:26:11  lr: 0.000014  min_lr: 0.000014  loss: 1.8965 (1.8613)  loss_scale: 8192.0000 (9275.8114)  weight_decay: 0.0500 (0.0500)  time: 0.6776  data: 0.0929  max mem: 13008
Epoch: [6]  [1430/3542]  eta: 0:26:03  lr: 0.000014  min_lr: 0.000014  loss: 1.8633 (1.8608)  loss_scale: 8192.0000 (9268.2376)  weight_decay: 0.0500 (0.0500)  time: 0.6740  data: 0.0888  max mem: 13008
Epoch: [6]  [1440/3542]  eta: 0:25:55  lr: 0.000014  min_lr: 0.000014  loss: 1.8486 (1.8606)  loss_scale: 8192.0000 (9260.7689)  weight_decay: 0.0500 (0.0500)  time: 0.7018  data: 0.1163  max mem: 13008
Epoch: [6]  [1450/3542]  eta: 0:25:48  lr: 0.000014  min_lr: 0.000014  loss: 1.8574 (1.8610)  loss_scale: 8192.0000 (9253.4032)  weight_decay: 0.0500 (0.0500)  time: 0.7138  data: 0.1287  max mem: 13008
Epoch: [6]  [1460/3542]  eta: 0:25:39  lr: 0.000014  min_lr: 0.000014  loss: 1.8574 (1.8611)  loss_scale: 8192.0000 (9246.1383)  weight_decay: 0.0500 (0.0500)  time: 0.6947  data: 0.1092  max mem: 13008
Epoch: [6]  [1470/3542]  eta: 0:25:32  lr: 0.000014  min_lr: 0.000014  loss: 1.8652 (1.8613)  loss_scale: 8192.0000 (9238.9721)  weight_decay: 0.0500 (0.0500)  time: 0.7016  data: 0.1158  max mem: 13008
Epoch: [6]  [1480/3542]  eta: 0:25:25  lr: 0.000014  min_lr: 0.000014  loss: 1.9287 (1.8613)  loss_scale: 8192.0000 (9231.9028)  weight_decay: 0.0500 (0.0500)  time: 0.7714  data: 0.1859  max mem: 13008
Epoch: [6]  [1490/3542]  eta: 0:25:16  lr: 0.000014  min_lr: 0.000014  loss: 1.9199 (1.8615)  loss_scale: 8192.0000 (9224.9282)  weight_decay: 0.0500 (0.0500)  time: 0.7173  data: 0.1322  max mem: 13008
Epoch: [6]  [1500/3542]  eta: 0:25:09  lr: 0.000014  min_lr: 0.000014  loss: 1.8633 (1.8613)  loss_scale: 8192.0000 (9218.0466)  weight_decay: 0.0500 (0.0500)  time: 0.6881  data: 0.1030  max mem: 13008
Epoch: [6]  [1510/3542]  eta: 0:25:02  lr: 0.000014  min_lr: 0.000014  loss: 1.8428 (1.8608)  loss_scale: 8192.0000 (9211.2561)  weight_decay: 0.0500 (0.0500)  time: 0.7672  data: 0.1826  max mem: 13008
Epoch: [6]  [1520/3542]  eta: 0:24:54  lr: 0.000014  min_lr: 0.000014  loss: 1.8242 (1.8603)  loss_scale: 8192.0000 (9204.5549)  weight_decay: 0.0500 (0.0500)  time: 0.7150  data: 0.1300  max mem: 13008
Epoch: [6]  [1530/3542]  eta: 0:24:46  lr: 0.000014  min_lr: 0.000014  loss: 1.8252 (1.8603)  loss_scale: 8192.0000 (9197.9412)  weight_decay: 0.0500 (0.0500)  time: 0.7001  data: 0.1132  max mem: 13008
Epoch: [6]  [1540/3542]  eta: 0:24:38  lr: 0.000014  min_lr: 0.000014  loss: 1.8740 (1.8607)  loss_scale: 8192.0000 (9191.4134)  weight_decay: 0.0500 (0.0500)  time: 0.7178  data: 0.1314  max mem: 13008
Epoch: [6]  [1550/3542]  eta: 0:24:32  lr: 0.000014  min_lr: 0.000014  loss: 1.8633 (1.8602)  loss_scale: 8192.0000 (9184.9697)  weight_decay: 0.0500 (0.0500)  time: 0.7481  data: 0.1624  max mem: 13008
Epoch: [6]  [1560/3542]  eta: 0:24:25  lr: 0.000014  min_lr: 0.000014  loss: 1.7520 (1.8600)  loss_scale: 8192.0000 (9178.6086)  weight_decay: 0.0500 (0.0500)  time: 0.7884  data: 0.2026  max mem: 13008
Epoch: [6]  [1570/3542]  eta: 0:24:17  lr: 0.000014  min_lr: 0.000014  loss: 1.7969 (1.8597)  loss_scale: 8192.0000 (9172.3285)  weight_decay: 0.0500 (0.0500)  time: 0.7437  data: 0.1583  max mem: 13008
Epoch: [6]  [1580/3542]  eta: 0:24:09  lr: 0.000014  min_lr: 0.000014  loss: 1.7969 (1.8595)  loss_scale: 8192.0000 (9166.1278)  weight_decay: 0.0500 (0.0500)  time: 0.7091  data: 0.1234  max mem: 13008
Epoch: [6]  [1590/3542]  eta: 0:24:03  lr: 0.000014  min_lr: 0.000014  loss: 1.8193 (1.8595)  loss_scale: 8192.0000 (9160.0050)  weight_decay: 0.0500 (0.0500)  time: 0.7732  data: 0.1877  max mem: 13008
Epoch: [6]  [1600/3542]  eta: 0:23:55  lr: 0.000014  min_lr: 0.000014  loss: 1.7959 (1.8594)  loss_scale: 8192.0000 (9153.9588)  weight_decay: 0.0500 (0.0500)  time: 0.7362  data: 0.1507  max mem: 13008
Epoch: [6]  [1610/3542]  eta: 0:23:47  lr: 0.000014  min_lr: 0.000014  loss: 1.7959 (1.8594)  loss_scale: 8192.0000 (9147.9876)  weight_decay: 0.0500 (0.0500)  time: 0.6857  data: 0.1004  max mem: 13008
Epoch: [6]  [1620/3542]  eta: 0:23:40  lr: 0.000014  min_lr: 0.000014  loss: 1.8613 (1.8595)  loss_scale: 8192.0000 (9142.0901)  weight_decay: 0.0500 (0.0500)  time: 0.7323  data: 0.1478  max mem: 13008
Epoch: [6]  [1630/3542]  eta: 0:23:31  lr: 0.000014  min_lr: 0.000014  loss: 1.8467 (1.8594)  loss_scale: 8192.0000 (9136.2649)  weight_decay: 0.0500 (0.0500)  time: 0.6944  data: 0.1090  max mem: 13008
Epoch: [6]  [1640/3542]  eta: 0:23:25  lr: 0.000014  min_lr: 0.000014  loss: 1.8350 (1.8595)  loss_scale: 8192.0000 (9130.5107)  weight_decay: 0.0500 (0.0500)  time: 0.7561  data: 0.1700  max mem: 13008
Epoch: [6]  [1650/3542]  eta: 0:23:17  lr: 0.000014  min_lr: 0.000014  loss: 1.8350 (1.8593)  loss_scale: 8192.0000 (9124.8262)  weight_decay: 0.0500 (0.0500)  time: 0.7474  data: 0.1623  max mem: 13008
Epoch: [6]  [1660/3542]  eta: 0:23:09  lr: 0.000014  min_lr: 0.000014  loss: 1.8252 (1.8595)  loss_scale: 8192.0000 (9119.2101)  weight_decay: 0.0500 (0.0500)  time: 0.6811  data: 0.0961  max mem: 13008
Epoch: [6]  [1670/3542]  eta: 0:23:00  lr: 0.000014  min_lr: 0.000014  loss: 1.8252 (1.8592)  loss_scale: 8192.0000 (9113.6613)  weight_decay: 0.0500 (0.0500)  time: 0.6669  data: 0.0808  max mem: 13008
Epoch: [6]  [1680/3542]  eta: 0:22:53  lr: 0.000014  min_lr: 0.000014  loss: 1.7920 (1.8587)  loss_scale: 8192.0000 (9108.1785)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.0913  max mem: 13008
Epoch: [6]  [1690/3542]  eta: 0:22:45  lr: 0.000014  min_lr: 0.000014  loss: 1.7393 (1.8584)  loss_scale: 8192.0000 (9102.7605)  weight_decay: 0.0500 (0.0500)  time: 0.7237  data: 0.1375  max mem: 13008
Epoch: [6]  [1700/3542]  eta: 0:22:37  lr: 0.000014  min_lr: 0.000014  loss: 1.8350 (1.8584)  loss_scale: 8192.0000 (9097.4062)  weight_decay: 0.0500 (0.0500)  time: 0.6948  data: 0.1085  max mem: 13008
Epoch: [6]  [1710/3542]  eta: 0:22:30  lr: 0.000014  min_lr: 0.000014  loss: 1.8350 (1.8582)  loss_scale: 8192.0000 (9092.1146)  weight_decay: 0.0500 (0.0500)  time: 0.6962  data: 0.1101  max mem: 13008
Epoch: [6]  [1720/3542]  eta: 0:22:21  lr: 0.000014  min_lr: 0.000014  loss: 1.8564 (1.8585)  loss_scale: 8192.0000 (9086.8844)  weight_decay: 0.0500 (0.0500)  time: 0.6783  data: 0.0923  max mem: 13008
Epoch: [6]  [1730/3542]  eta: 0:22:13  lr: 0.000014  min_lr: 0.000014  loss: 1.9072 (1.8588)  loss_scale: 8192.0000 (9081.7146)  weight_decay: 0.0500 (0.0500)  time: 0.6489  data: 0.0624  max mem: 13008
Epoch: [6]  [1740/3542]  eta: 0:22:05  lr: 0.000014  min_lr: 0.000014  loss: 1.8838 (1.8585)  loss_scale: 8192.0000 (9076.6043)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.0422  max mem: 13008
[2023-05-16 11:20:43,459] [INFO] [logging.py:60:log_dist] [Rank 0] step=23000, skipped=22, lr=[1.3873965976351314e-05, 1.3873965976351314e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 11:20:43,461] [INFO] [timer.py:157:stop] 0/23000, SamplesPerSec=55.54906481187568
Epoch: [6]  [1750/3542]  eta: 0:21:57  lr: 0.000014  min_lr: 0.000014  loss: 1.8750 (1.8589)  loss_scale: 8192.0000 (9071.5523)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.0518  max mem: 13008
Epoch: [6]  [1760/3542]  eta: 0:21:49  lr: 0.000014  min_lr: 0.000014  loss: 1.8750 (1.8589)  loss_scale: 8192.0000 (9066.5576)  weight_decay: 0.0500 (0.0500)  time: 0.6749  data: 0.0899  max mem: 13008
Epoch: [6]  [1770/3542]  eta: 0:21:42  lr: 0.000014  min_lr: 0.000014  loss: 1.8584 (1.8589)  loss_scale: 8192.0000 (9061.6194)  weight_decay: 0.0500 (0.0500)  time: 0.7229  data: 0.1373  max mem: 13008
Epoch: [6]  [1780/3542]  eta: 0:21:35  lr: 0.000014  min_lr: 0.000014  loss: 1.8486 (1.8589)  loss_scale: 8192.0000 (9056.7367)  weight_decay: 0.0500 (0.0500)  time: 0.7265  data: 0.1401  max mem: 13008
Epoch: [6]  [1790/3542]  eta: 0:21:26  lr: 0.000014  min_lr: 0.000014  loss: 1.8486 (1.8588)  loss_scale: 8192.0000 (9051.9084)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.0764  max mem: 13008
Epoch: [6]  [1800/3542]  eta: 0:21:19  lr: 0.000014  min_lr: 0.000014  loss: 1.8896 (1.8592)  loss_scale: 8192.0000 (9047.1338)  weight_decay: 0.0500 (0.0500)  time: 0.6674  data: 0.0808  max mem: 13008
Epoch: [6]  [1810/3542]  eta: 0:21:11  lr: 0.000014  min_lr: 0.000014  loss: 1.8330 (1.8590)  loss_scale: 8192.0000 (9042.4119)  weight_decay: 0.0500 (0.0500)  time: 0.6957  data: 0.1087  max mem: 13008
Epoch: [6]  [1820/3542]  eta: 0:21:03  lr: 0.000014  min_lr: 0.000014  loss: 1.7988 (1.8592)  loss_scale: 8192.0000 (9037.7419)  weight_decay: 0.0500 (0.0500)  time: 0.6857  data: 0.0988  max mem: 13008
Epoch: [6]  [1830/3542]  eta: 0:20:55  lr: 0.000014  min_lr: 0.000014  loss: 1.8076 (1.8592)  loss_scale: 8192.0000 (9033.1229)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.0797  max mem: 13008
Epoch: [6]  [1840/3542]  eta: 0:20:47  lr: 0.000014  min_lr: 0.000014  loss: 1.8389 (1.8592)  loss_scale: 8192.0000 (9028.5540)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0537  max mem: 13008
Epoch: [6]  [1850/3542]  eta: 0:20:39  lr: 0.000014  min_lr: 0.000014  loss: 1.8994 (1.8593)  loss_scale: 8192.0000 (9024.0346)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.0618  max mem: 13008
Epoch: [6]  [1860/3542]  eta: 0:20:31  lr: 0.000014  min_lr: 0.000014  loss: 1.8174 (1.8591)  loss_scale: 8192.0000 (9019.5637)  weight_decay: 0.0500 (0.0500)  time: 0.6888  data: 0.1018  max mem: 13008
Epoch: [6]  [1870/3542]  eta: 0:20:23  lr: 0.000014  min_lr: 0.000014  loss: 1.8486 (1.8594)  loss_scale: 8192.0000 (9015.1406)  weight_decay: 0.0500 (0.0500)  time: 0.6671  data: 0.0808  max mem: 13008
Epoch: [6]  [1880/3542]  eta: 0:20:16  lr: 0.000014  min_lr: 0.000014  loss: 1.8633 (1.8588)  loss_scale: 8192.0000 (9010.7645)  weight_decay: 0.0500 (0.0500)  time: 0.7090  data: 0.1236  max mem: 13008
Epoch: [6]  [1890/3542]  eta: 0:20:08  lr: 0.000014  min_lr: 0.000014  loss: 1.7783 (1.8588)  loss_scale: 8192.0000 (9006.4347)  weight_decay: 0.0500 (0.0500)  time: 0.7280  data: 0.1418  max mem: 13008
Epoch: [6]  [1900/3542]  eta: 0:20:01  lr: 0.000014  min_lr: 0.000014  loss: 1.8193 (1.8590)  loss_scale: 8192.0000 (9002.1504)  weight_decay: 0.0500 (0.0500)  time: 0.7347  data: 0.1484  max mem: 13008
Epoch: [6]  [1910/3542]  eta: 0:19:54  lr: 0.000014  min_lr: 0.000014  loss: 1.8174 (1.8586)  loss_scale: 8192.0000 (8997.9110)  weight_decay: 0.0500 (0.0500)  time: 0.7399  data: 0.1545  max mem: 13008
Epoch: [6]  [1920/3542]  eta: 0:19:46  lr: 0.000014  min_lr: 0.000014  loss: 1.7920 (1.8585)  loss_scale: 8192.0000 (8993.7158)  weight_decay: 0.0500 (0.0500)  time: 0.6711  data: 0.0848  max mem: 13008
Epoch: [6]  [1930/3542]  eta: 0:19:38  lr: 0.000014  min_lr: 0.000014  loss: 1.9062 (1.8589)  loss_scale: 8192.0000 (8989.5640)  weight_decay: 0.0500 (0.0500)  time: 0.6870  data: 0.1000  max mem: 13008
Epoch: [6]  [1940/3542]  eta: 0:19:31  lr: 0.000014  min_lr: 0.000014  loss: 1.9062 (1.8588)  loss_scale: 8192.0000 (8985.4549)  weight_decay: 0.0500 (0.0500)  time: 0.7212  data: 0.1347  max mem: 13008
Epoch: [6]  [1950/3542]  eta: 0:19:23  lr: 0.000014  min_lr: 0.000014  loss: 1.7930 (1.8586)  loss_scale: 8192.0000 (8981.3880)  weight_decay: 0.0500 (0.0500)  time: 0.6861  data: 0.1002  max mem: 13008
Epoch: [6]  [1960/3542]  eta: 0:19:15  lr: 0.000013  min_lr: 0.000013  loss: 1.8564 (1.8589)  loss_scale: 8192.0000 (8977.3626)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.0822  max mem: 13008
Epoch: [6]  [1970/3542]  eta: 0:19:08  lr: 0.000013  min_lr: 0.000013  loss: 1.9326 (1.8589)  loss_scale: 8192.0000 (8973.3780)  weight_decay: 0.0500 (0.0500)  time: 0.6893  data: 0.1031  max mem: 13008
Epoch: [6]  [1980/3542]  eta: 0:19:01  lr: 0.000013  min_lr: 0.000013  loss: 1.7959 (1.8583)  loss_scale: 8192.0000 (8969.4336)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.1423  max mem: 13008
Epoch: [6]  [1990/3542]  eta: 0:18:53  lr: 0.000013  min_lr: 0.000013  loss: 1.7998 (1.8583)  loss_scale: 8192.0000 (8965.5289)  weight_decay: 0.0500 (0.0500)  time: 0.7195  data: 0.1336  max mem: 13008
Epoch: [6]  [2000/3542]  eta: 0:18:45  lr: 0.000013  min_lr: 0.000013  loss: 1.8584 (1.8586)  loss_scale: 8192.0000 (8961.6632)  weight_decay: 0.0500 (0.0500)  time: 0.6817  data: 0.0954  max mem: 13008
Epoch: [6]  [2010/3542]  eta: 0:18:37  lr: 0.000013  min_lr: 0.000013  loss: 1.9531 (1.8590)  loss_scale: 8192.0000 (8957.8359)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.0734  max mem: 13008
Epoch: [6]  [2020/3542]  eta: 0:18:29  lr: 0.000013  min_lr: 0.000013  loss: 1.9102 (1.8590)  loss_scale: 8192.0000 (8954.0465)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0320  max mem: 13008
Epoch: [6]  [2030/3542]  eta: 0:18:21  lr: 0.000013  min_lr: 0.000013  loss: 1.8623 (1.8590)  loss_scale: 8192.0000 (8950.2944)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.0490  max mem: 13008
Epoch: [6]  [2040/3542]  eta: 0:18:14  lr: 0.000013  min_lr: 0.000013  loss: 1.8320 (1.8587)  loss_scale: 8192.0000 (8946.5791)  weight_decay: 0.0500 (0.0500)  time: 0.6961  data: 0.1096  max mem: 13008
Epoch: [6]  [2050/3542]  eta: 0:18:07  lr: 0.000013  min_lr: 0.000013  loss: 1.8320 (1.8589)  loss_scale: 8192.0000 (8942.9000)  weight_decay: 0.0500 (0.0500)  time: 0.7412  data: 0.1543  max mem: 13008
Epoch: [6]  [2060/3542]  eta: 0:17:59  lr: 0.000013  min_lr: 0.000013  loss: 1.8936 (1.8590)  loss_scale: 8192.0000 (8939.2567)  weight_decay: 0.0500 (0.0500)  time: 0.6695  data: 0.0824  max mem: 13008
Epoch: [6]  [2070/3542]  eta: 0:17:51  lr: 0.000013  min_lr: 0.000013  loss: 1.8955 (1.8593)  loss_scale: 8192.0000 (8935.6485)  weight_decay: 0.0500 (0.0500)  time: 0.6068  data: 0.0193  max mem: 13008
Epoch: [6]  [2080/3542]  eta: 0:17:43  lr: 0.000013  min_lr: 0.000013  loss: 1.8594 (1.8591)  loss_scale: 8192.0000 (8932.0750)  weight_decay: 0.0500 (0.0500)  time: 0.6767  data: 0.0898  max mem: 13008
Epoch: [6]  [2090/3542]  eta: 0:17:35  lr: 0.000013  min_lr: 0.000013  loss: 1.7803 (1.8590)  loss_scale: 8192.0000 (8928.5356)  weight_decay: 0.0500 (0.0500)  time: 0.6918  data: 0.1057  max mem: 13008
Epoch: [6]  [2100/3542]  eta: 0:17:28  lr: 0.000013  min_lr: 0.000013  loss: 1.7939 (1.8589)  loss_scale: 8192.0000 (8925.0300)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0793  max mem: 13008
Epoch: [6]  [2110/3542]  eta: 0:17:20  lr: 0.000013  min_lr: 0.000013  loss: 1.8350 (1.8592)  loss_scale: 8192.0000 (8921.5576)  weight_decay: 0.0500 (0.0500)  time: 0.6719  data: 0.0863  max mem: 13008
Epoch: [6]  [2120/3542]  eta: 0:17:12  lr: 0.000013  min_lr: 0.000013  loss: 1.8916 (1.8596)  loss_scale: 8192.0000 (8918.1179)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0720  max mem: 13008
Epoch: [6]  [2130/3542]  eta: 0:17:05  lr: 0.000013  min_lr: 0.000013  loss: 1.8564 (1.8597)  loss_scale: 8192.0000 (8914.7105)  weight_decay: 0.0500 (0.0500)  time: 0.6595  data: 0.0732  max mem: 13008
Epoch: [6]  [2140/3542]  eta: 0:16:57  lr: 0.000013  min_lr: 0.000013  loss: 1.8389 (1.8593)  loss_scale: 8192.0000 (8911.3349)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0555  max mem: 13008
Epoch: [6]  [2150/3542]  eta: 0:16:49  lr: 0.000013  min_lr: 0.000013  loss: 1.8076 (1.8590)  loss_scale: 8192.0000 (8907.9907)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0544  max mem: 13008
Epoch: [6]  [2160/3542]  eta: 0:16:42  lr: 0.000013  min_lr: 0.000013  loss: 1.8154 (1.8588)  loss_scale: 8192.0000 (8904.6775)  weight_decay: 0.0500 (0.0500)  time: 0.6741  data: 0.0877  max mem: 13008
Epoch: [6]  [2170/3542]  eta: 0:16:34  lr: 0.000013  min_lr: 0.000013  loss: 1.7910 (1.8583)  loss_scale: 8192.0000 (8901.3947)  weight_decay: 0.0500 (0.0500)  time: 0.6646  data: 0.0784  max mem: 13008
Epoch: [6]  [2180/3542]  eta: 0:16:26  lr: 0.000013  min_lr: 0.000013  loss: 1.8496 (1.8588)  loss_scale: 8192.0000 (8898.1421)  weight_decay: 0.0500 (0.0500)  time: 0.6582  data: 0.0706  max mem: 13008
[2023-05-16 11:25:40,001] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 11:25:40,002] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [6]  [2190/3542]  eta: 0:16:19  lr: 0.000013  min_lr: 0.000013  loss: 1.8496 (1.8584)  loss_scale: 8192.0000 (8921.0917)  weight_decay: 0.0500 (0.0500)  time: 0.6780  data: 0.0898  max mem: 13008
Epoch: [6]  [2200/3542]  eta: 0:16:11  lr: 0.000013  min_lr: 0.000013  loss: 1.8369 (1.8586)  loss_scale: 16384.0000 (8954.9986)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.0908  max mem: 13008
Epoch: [6]  [2210/3542]  eta: 0:16:04  lr: 0.000013  min_lr: 0.000013  loss: 1.8730 (1.8587)  loss_scale: 16384.0000 (8988.5988)  weight_decay: 0.0500 (0.0500)  time: 0.6574  data: 0.0716  max mem: 13008
Epoch: [6]  [2220/3542]  eta: 0:15:55  lr: 0.000013  min_lr: 0.000013  loss: 1.8838 (1.8590)  loss_scale: 16384.0000 (9021.8964)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0330  max mem: 13008
Epoch: [6]  [2230/3542]  eta: 0:15:48  lr: 0.000013  min_lr: 0.000013  loss: 1.8662 (1.8586)  loss_scale: 16384.0000 (9054.8956)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.0560  max mem: 13008
Epoch: [6]  [2240/3542]  eta: 0:15:41  lr: 0.000013  min_lr: 0.000013  loss: 1.7969 (1.8588)  loss_scale: 16384.0000 (9087.6002)  weight_decay: 0.0500 (0.0500)  time: 0.6866  data: 0.0989  max mem: 13008
Epoch: [6]  [2250/3542]  eta: 0:15:33  lr: 0.000013  min_lr: 0.000013  loss: 1.9160 (1.8592)  loss_scale: 16384.0000 (9120.0142)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0686  max mem: 13008
[2023-05-16 11:26:27,224] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 23508
[2023-05-16 11:26:27,224] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 11:26:27,224] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [6]  [2260/3542]  eta: 0:15:26  lr: 0.000013  min_lr: 0.000013  loss: 1.9160 (1.8593)  loss_scale: 16384.0000 (9134.0257)  weight_decay: 0.0500 (0.0500)  time: 0.6777  data: 0.0905  max mem: 13008
Epoch: [6]  [2270/3542]  eta: 0:15:19  lr: 0.000013  min_lr: 0.000013  loss: 1.7891 (1.8590)  loss_scale: 8192.0000 (9129.8776)  weight_decay: 0.0500 (0.0500)  time: 0.7410  data: 0.1539  max mem: 13008
Epoch: [6]  [2280/3542]  eta: 0:15:11  lr: 0.000013  min_lr: 0.000013  loss: 1.7891 (1.8593)  loss_scale: 8192.0000 (9125.7659)  weight_decay: 0.0500 (0.0500)  time: 0.7467  data: 0.1601  max mem: 13008
Epoch: [6]  [2290/3542]  eta: 0:15:04  lr: 0.000013  min_lr: 0.000013  loss: 1.8369 (1.8593)  loss_scale: 8192.0000 (9121.6901)  weight_decay: 0.0500 (0.0500)  time: 0.6739  data: 0.0880  max mem: 13008
Epoch: [6]  [2300/3542]  eta: 0:14:56  lr: 0.000013  min_lr: 0.000013  loss: 1.8389 (1.8592)  loss_scale: 8192.0000 (9117.6497)  weight_decay: 0.0500 (0.0500)  time: 0.6327  data: 0.0462  max mem: 13008
Epoch: [6]  [2310/3542]  eta: 0:14:48  lr: 0.000013  min_lr: 0.000013  loss: 1.8447 (1.8594)  loss_scale: 8192.0000 (9113.6443)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0527  max mem: 13008
Epoch: [6]  [2320/3542]  eta: 0:14:41  lr: 0.000013  min_lr: 0.000013  loss: 1.8994 (1.8598)  loss_scale: 8192.0000 (9109.6734)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.0529  max mem: 13008
Epoch: [6]  [2330/3542]  eta: 0:14:34  lr: 0.000013  min_lr: 0.000013  loss: 1.8682 (1.8598)  loss_scale: 8192.0000 (9105.7366)  weight_decay: 0.0500 (0.0500)  time: 0.6896  data: 0.1024  max mem: 13008
Epoch: [6]  [2340/3542]  eta: 0:14:26  lr: 0.000013  min_lr: 0.000013  loss: 1.9072 (1.8605)  loss_scale: 8192.0000 (9101.8334)  weight_decay: 0.0500 (0.0500)  time: 0.7065  data: 0.1196  max mem: 13008
Epoch: [6]  [2350/3542]  eta: 0:14:19  lr: 0.000013  min_lr: 0.000013  loss: 1.8652 (1.8602)  loss_scale: 8192.0000 (9097.9634)  weight_decay: 0.0500 (0.0500)  time: 0.6789  data: 0.0921  max mem: 13008
Epoch: [6]  [2360/3542]  eta: 0:14:11  lr: 0.000013  min_lr: 0.000013  loss: 1.8037 (1.8602)  loss_scale: 8192.0000 (9094.1262)  weight_decay: 0.0500 (0.0500)  time: 0.6772  data: 0.0915  max mem: 13008
Epoch: [6]  [2370/3542]  eta: 0:14:04  lr: 0.000013  min_lr: 0.000013  loss: 1.8506 (1.8603)  loss_scale: 8192.0000 (9090.3214)  weight_decay: 0.0500 (0.0500)  time: 0.6754  data: 0.0896  max mem: 13008
Epoch: [6]  [2380/3542]  eta: 0:13:56  lr: 0.000013  min_lr: 0.000013  loss: 1.9219 (1.8606)  loss_scale: 8192.0000 (9086.5485)  weight_decay: 0.0500 (0.0500)  time: 0.6821  data: 0.0956  max mem: 13008
Epoch: [6]  [2390/3542]  eta: 0:13:49  lr: 0.000013  min_lr: 0.000013  loss: 1.8311 (1.8602)  loss_scale: 8192.0000 (9082.8072)  weight_decay: 0.0500 (0.0500)  time: 0.6734  data: 0.0868  max mem: 13008
Epoch: [6]  [2400/3542]  eta: 0:13:41  lr: 0.000013  min_lr: 0.000013  loss: 1.8369 (1.8605)  loss_scale: 8192.0000 (9079.0970)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.0461  max mem: 13008
Epoch: [6]  [2410/3542]  eta: 0:13:33  lr: 0.000013  min_lr: 0.000013  loss: 1.8750 (1.8606)  loss_scale: 8192.0000 (9075.4177)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0183  max mem: 13008
Epoch: [6]  [2420/3542]  eta: 0:13:26  lr: 0.000013  min_lr: 0.000013  loss: 1.8496 (1.8606)  loss_scale: 8192.0000 (9071.7687)  weight_decay: 0.0500 (0.0500)  time: 0.6341  data: 0.0482  max mem: 13008
Epoch: [6]  [2430/3542]  eta: 0:13:19  lr: 0.000013  min_lr: 0.000013  loss: 1.8496 (1.8607)  loss_scale: 8192.0000 (9068.1497)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.0683  max mem: 13008
Epoch: [6]  [2440/3542]  eta: 0:13:11  lr: 0.000013  min_lr: 0.000013  loss: 1.8203 (1.8605)  loss_scale: 8192.0000 (9064.5604)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.0637  max mem: 13008
Epoch: [6]  [2450/3542]  eta: 0:13:04  lr: 0.000013  min_lr: 0.000013  loss: 1.8271 (1.8606)  loss_scale: 8192.0000 (9061.0004)  weight_decay: 0.0500 (0.0500)  time: 0.6917  data: 0.1062  max mem: 13008
Epoch: [6]  [2460/3542]  eta: 0:12:57  lr: 0.000013  min_lr: 0.000013  loss: 1.8662 (1.8607)  loss_scale: 8192.0000 (9057.4693)  weight_decay: 0.0500 (0.0500)  time: 0.6980  data: 0.1118  max mem: 13008
Epoch: [6]  [2470/3542]  eta: 0:12:49  lr: 0.000013  min_lr: 0.000013  loss: 1.8271 (1.8605)  loss_scale: 8192.0000 (9053.9668)  weight_decay: 0.0500 (0.0500)  time: 0.6751  data: 0.0896  max mem: 13008
Epoch: [6]  [2480/3542]  eta: 0:12:42  lr: 0.000013  min_lr: 0.000013  loss: 1.8271 (1.8609)  loss_scale: 8192.0000 (9050.4925)  weight_decay: 0.0500 (0.0500)  time: 0.6438  data: 0.0586  max mem: 13008
Epoch: [6]  [2490/3542]  eta: 0:12:34  lr: 0.000013  min_lr: 0.000013  loss: 1.8682 (1.8608)  loss_scale: 8192.0000 (9047.0462)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0234  max mem: 13008
Epoch: [6]  [2500/3542]  eta: 0:12:27  lr: 0.000013  min_lr: 0.000013  loss: 1.8477 (1.8609)  loss_scale: 8192.0000 (9043.6273)  weight_decay: 0.0500 (0.0500)  time: 0.6427  data: 0.0572  max mem: 13008
Epoch: [6]  [2510/3542]  eta: 0:12:19  lr: 0.000013  min_lr: 0.000013  loss: 1.8096 (1.8609)  loss_scale: 8192.0000 (9040.2358)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.0454  max mem: 13008
Epoch: [6]  [2520/3542]  eta: 0:12:11  lr: 0.000012  min_lr: 0.000012  loss: 1.8486 (1.8609)  loss_scale: 8192.0000 (9036.8711)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0189  max mem: 13008
Epoch: [6]  [2530/3542]  eta: 0:12:04  lr: 0.000012  min_lr: 0.000012  loss: 1.8486 (1.8607)  loss_scale: 8192.0000 (9033.5330)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0342  max mem: 13008
Epoch: [6]  [2540/3542]  eta: 0:11:57  lr: 0.000012  min_lr: 0.000012  loss: 1.8467 (1.8607)  loss_scale: 8192.0000 (9030.2212)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.0804  max mem: 13008
Epoch: [6]  [2550/3542]  eta: 0:11:49  lr: 0.000012  min_lr: 0.000012  loss: 1.8867 (1.8609)  loss_scale: 8192.0000 (9026.9353)  weight_decay: 0.0500 (0.0500)  time: 0.6856  data: 0.0983  max mem: 13008
Epoch: [6]  [2560/3542]  eta: 0:11:42  lr: 0.000012  min_lr: 0.000012  loss: 1.9043 (1.8611)  loss_scale: 8192.0000 (9023.6751)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.0397  max mem: 13008
Epoch: [6]  [2570/3542]  eta: 0:11:34  lr: 0.000012  min_lr: 0.000012  loss: 1.9121 (1.8614)  loss_scale: 8192.0000 (9020.4403)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.0527  max mem: 13008
Epoch: [6]  [2580/3542]  eta: 0:11:27  lr: 0.000012  min_lr: 0.000012  loss: 1.8809 (1.8614)  loss_scale: 8192.0000 (9017.2305)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.0685  max mem: 13008
Epoch: [6]  [2590/3542]  eta: 0:11:19  lr: 0.000012  min_lr: 0.000012  loss: 1.8486 (1.8616)  loss_scale: 8192.0000 (9014.0455)  weight_decay: 0.0500 (0.0500)  time: 0.6151  data: 0.0288  max mem: 13008
Epoch: [6]  [2600/3542]  eta: 0:11:12  lr: 0.000012  min_lr: 0.000012  loss: 1.9180 (1.8616)  loss_scale: 8192.0000 (9010.8850)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0126  max mem: 13008
Epoch: [6]  [2610/3542]  eta: 0:11:04  lr: 0.000012  min_lr: 0.000012  loss: 1.8223 (1.8617)  loss_scale: 8192.0000 (9007.7488)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0301  max mem: 13008
Epoch: [6]  [2620/3542]  eta: 0:10:57  lr: 0.000012  min_lr: 0.000012  loss: 1.8164 (1.8617)  loss_scale: 8192.0000 (9004.6364)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.0668  max mem: 13008
Epoch: [6]  [2630/3542]  eta: 0:10:50  lr: 0.000012  min_lr: 0.000012  loss: 1.8320 (1.8616)  loss_scale: 8192.0000 (9001.5477)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.0642  max mem: 13008
Epoch: [6]  [2640/3542]  eta: 0:10:42  lr: 0.000012  min_lr: 0.000012  loss: 1.8770 (1.8617)  loss_scale: 8192.0000 (8998.4824)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.0486  max mem: 13008
Epoch: [6]  [2650/3542]  eta: 0:10:35  lr: 0.000012  min_lr: 0.000012  loss: 1.8652 (1.8617)  loss_scale: 8192.0000 (8995.4402)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.0561  max mem: 13008
Epoch: [6]  [2660/3542]  eta: 0:10:28  lr: 0.000012  min_lr: 0.000012  loss: 1.8789 (1.8618)  loss_scale: 8192.0000 (8992.4209)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.0714  max mem: 13008
Epoch: [6]  [2670/3542]  eta: 0:10:20  lr: 0.000012  min_lr: 0.000012  loss: 1.7979 (1.8616)  loss_scale: 8192.0000 (8989.4242)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.0631  max mem: 13008
Epoch: [6]  [2680/3542]  eta: 0:10:13  lr: 0.000012  min_lr: 0.000012  loss: 1.7979 (1.8615)  loss_scale: 8192.0000 (8986.4498)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.0758  max mem: 13008
Epoch: [6]  [2690/3542]  eta: 0:10:06  lr: 0.000012  min_lr: 0.000012  loss: 1.8555 (1.8617)  loss_scale: 8192.0000 (8983.4976)  weight_decay: 0.0500 (0.0500)  time: 0.7025  data: 0.1167  max mem: 13008
Epoch: [6]  [2700/3542]  eta: 0:09:58  lr: 0.000012  min_lr: 0.000012  loss: 1.8623 (1.8620)  loss_scale: 8192.0000 (8980.5672)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.0741  max mem: 13008
Epoch: [6]  [2710/3542]  eta: 0:09:51  lr: 0.000012  min_lr: 0.000012  loss: 1.9111 (1.8620)  loss_scale: 8192.0000 (8977.6584)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.0528  max mem: 13008
Epoch: [6]  [2720/3542]  eta: 0:09:44  lr: 0.000012  min_lr: 0.000012  loss: 1.9121 (1.8622)  loss_scale: 8192.0000 (8974.7710)  weight_decay: 0.0500 (0.0500)  time: 0.6866  data: 0.1009  max mem: 13008
Epoch: [6]  [2730/3542]  eta: 0:09:37  lr: 0.000012  min_lr: 0.000012  loss: 1.9121 (1.8621)  loss_scale: 8192.0000 (8971.9048)  weight_decay: 0.0500 (0.0500)  time: 0.6820  data: 0.0964  max mem: 13008
Epoch: [6]  [2740/3542]  eta: 0:09:29  lr: 0.000012  min_lr: 0.000012  loss: 1.8545 (1.8622)  loss_scale: 8192.0000 (8969.0595)  weight_decay: 0.0500 (0.0500)  time: 0.6340  data: 0.0479  max mem: 13008
[2023-05-16 11:31:49,551] [INFO] [logging.py:60:log_dist] [Rank 0] step=24000, skipped=23, lr=[1.2101651434938163e-05, 1.2101651434938163e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 11:31:49,553] [INFO] [timer.py:157:stop] 0/24000, SamplesPerSec=55.546954679050096
Epoch: [6]  [2750/3542]  eta: 0:09:22  lr: 0.000012  min_lr: 0.000012  loss: 1.8740 (1.8625)  loss_scale: 8192.0000 (8966.2348)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0385  max mem: 13008
Epoch: [6]  [2760/3542]  eta: 0:09:15  lr: 0.000012  min_lr: 0.000012  loss: 1.8555 (1.8622)  loss_scale: 8192.0000 (8963.4306)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.0549  max mem: 13008
Epoch: [6]  [2770/3542]  eta: 0:09:08  lr: 0.000012  min_lr: 0.000012  loss: 1.8027 (1.8621)  loss_scale: 8192.0000 (8960.6467)  weight_decay: 0.0500 (0.0500)  time: 0.6430  data: 0.0577  max mem: 13008
Epoch: [6]  [2780/3542]  eta: 0:09:00  lr: 0.000012  min_lr: 0.000012  loss: 1.7451 (1.8619)  loss_scale: 8192.0000 (8957.8828)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.0651  max mem: 13008
Epoch: [6]  [2790/3542]  eta: 0:08:53  lr: 0.000012  min_lr: 0.000012  loss: 1.8066 (1.8620)  loss_scale: 8192.0000 (8955.1387)  weight_decay: 0.0500 (0.0500)  time: 0.6308  data: 0.0452  max mem: 13008
Epoch: [6]  [2800/3542]  eta: 0:08:46  lr: 0.000012  min_lr: 0.000012  loss: 1.8389 (1.8619)  loss_scale: 8192.0000 (8952.4141)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0421  max mem: 13008
Epoch: [6]  [2810/3542]  eta: 0:08:38  lr: 0.000012  min_lr: 0.000012  loss: 1.7852 (1.8616)  loss_scale: 8192.0000 (8949.7090)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.0676  max mem: 13008
Epoch: [6]  [2820/3542]  eta: 0:08:31  lr: 0.000012  min_lr: 0.000012  loss: 1.7373 (1.8616)  loss_scale: 8192.0000 (8947.0230)  weight_decay: 0.0500 (0.0500)  time: 0.6542  data: 0.0676  max mem: 13008
Epoch: [6]  [2830/3542]  eta: 0:08:24  lr: 0.000012  min_lr: 0.000012  loss: 1.7910 (1.8616)  loss_scale: 8192.0000 (8944.3561)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.0668  max mem: 13008
Epoch: [6]  [2840/3542]  eta: 0:08:17  lr: 0.000012  min_lr: 0.000012  loss: 1.7969 (1.8613)  loss_scale: 8192.0000 (8941.7078)  weight_decay: 0.0500 (0.0500)  time: 0.6478  data: 0.0612  max mem: 13008
Epoch: [6]  [2850/3542]  eta: 0:08:09  lr: 0.000012  min_lr: 0.000012  loss: 1.8193 (1.8612)  loss_scale: 8192.0000 (8939.0782)  weight_decay: 0.0500 (0.0500)  time: 0.6254  data: 0.0388  max mem: 13008
Epoch: [6]  [2860/3542]  eta: 0:08:02  lr: 0.000012  min_lr: 0.000012  loss: 1.8398 (1.8613)  loss_scale: 8192.0000 (8936.4670)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.0656  max mem: 13008
Epoch: [6]  [2870/3542]  eta: 0:07:55  lr: 0.000012  min_lr: 0.000012  loss: 1.8750 (1.8613)  loss_scale: 8192.0000 (8933.8739)  weight_decay: 0.0500 (0.0500)  time: 0.6665  data: 0.0808  max mem: 13008
Epoch: [6]  [2880/3542]  eta: 0:07:48  lr: 0.000012  min_lr: 0.000012  loss: 1.9121 (1.8615)  loss_scale: 8192.0000 (8931.2989)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0431  max mem: 13008
Epoch: [6]  [2890/3542]  eta: 0:07:41  lr: 0.000012  min_lr: 0.000012  loss: 1.9229 (1.8615)  loss_scale: 8192.0000 (8928.7416)  weight_decay: 0.0500 (0.0500)  time: 0.6272  data: 0.0414  max mem: 13008
Epoch: [6]  [2900/3542]  eta: 0:07:33  lr: 0.000012  min_lr: 0.000012  loss: 1.8428 (1.8616)  loss_scale: 8192.0000 (8926.2020)  weight_decay: 0.0500 (0.0500)  time: 0.6374  data: 0.0521  max mem: 13008
Epoch: [6]  [2910/3542]  eta: 0:07:26  lr: 0.000012  min_lr: 0.000012  loss: 1.8438 (1.8617)  loss_scale: 8192.0000 (8923.6798)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.0654  max mem: 13008
Epoch: [6]  [2920/3542]  eta: 0:07:19  lr: 0.000012  min_lr: 0.000012  loss: 1.8652 (1.8619)  loss_scale: 8192.0000 (8921.1749)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0540  max mem: 13008
Epoch: [6]  [2930/3542]  eta: 0:07:12  lr: 0.000012  min_lr: 0.000012  loss: 1.8691 (1.8620)  loss_scale: 8192.0000 (8918.6871)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.0342  max mem: 13008
Epoch: [6]  [2940/3542]  eta: 0:07:05  lr: 0.000012  min_lr: 0.000012  loss: 1.8564 (1.8619)  loss_scale: 8192.0000 (8916.2163)  weight_decay: 0.0500 (0.0500)  time: 0.6566  data: 0.0699  max mem: 13008
Epoch: [6]  [2950/3542]  eta: 0:06:58  lr: 0.000012  min_lr: 0.000012  loss: 1.8428 (1.8619)  loss_scale: 8192.0000 (8913.7621)  weight_decay: 0.0500 (0.0500)  time: 0.7025  data: 0.1158  max mem: 13008
Epoch: [6]  [2960/3542]  eta: 0:06:50  lr: 0.000012  min_lr: 0.000012  loss: 1.8730 (1.8620)  loss_scale: 8192.0000 (8911.3246)  weight_decay: 0.0500 (0.0500)  time: 0.6648  data: 0.0779  max mem: 13008
Epoch: [6]  [2970/3542]  eta: 0:06:43  lr: 0.000012  min_lr: 0.000012  loss: 1.8408 (1.8619)  loss_scale: 8192.0000 (8908.9034)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.0468  max mem: 13008
Epoch: [6]  [2980/3542]  eta: 0:06:36  lr: 0.000012  min_lr: 0.000012  loss: 1.8281 (1.8620)  loss_scale: 8192.0000 (8906.4985)  weight_decay: 0.0500 (0.0500)  time: 0.6839  data: 0.0976  max mem: 13008
Epoch: [6]  [2990/3542]  eta: 0:06:29  lr: 0.000012  min_lr: 0.000012  loss: 1.8408 (1.8620)  loss_scale: 8192.0000 (8904.1097)  weight_decay: 0.0500 (0.0500)  time: 0.7181  data: 0.1310  max mem: 13008
Epoch: [6]  [3000/3542]  eta: 0:06:22  lr: 0.000012  min_lr: 0.000012  loss: 1.8320 (1.8619)  loss_scale: 8192.0000 (8901.7368)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.0670  max mem: 13008
Epoch: [6]  [3010/3542]  eta: 0:06:15  lr: 0.000012  min_lr: 0.000012  loss: 1.8301 (1.8618)  loss_scale: 8192.0000 (8899.3796)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0204  max mem: 13008
Epoch: [6]  [3020/3542]  eta: 0:06:07  lr: 0.000012  min_lr: 0.000012  loss: 1.8447 (1.8620)  loss_scale: 8192.0000 (8897.0381)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.0585  max mem: 13008
Epoch: [6]  [3030/3542]  eta: 0:06:00  lr: 0.000012  min_lr: 0.000012  loss: 1.8535 (1.8620)  loss_scale: 8192.0000 (8894.7120)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.0675  max mem: 13008
Epoch: [6]  [3040/3542]  eta: 0:05:53  lr: 0.000012  min_lr: 0.000012  loss: 1.8545 (1.8619)  loss_scale: 8192.0000 (8892.4012)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0501  max mem: 13008
Epoch: [6]  [3050/3542]  eta: 0:05:46  lr: 0.000012  min_lr: 0.000012  loss: 1.8320 (1.8619)  loss_scale: 8192.0000 (8890.1055)  weight_decay: 0.0500 (0.0500)  time: 0.6395  data: 0.0543  max mem: 13008
Epoch: [6]  [3060/3542]  eta: 0:05:39  lr: 0.000012  min_lr: 0.000012  loss: 1.8232 (1.8618)  loss_scale: 8192.0000 (8887.8249)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.0391  max mem: 13008
Epoch: [6]  [3070/3542]  eta: 0:05:32  lr: 0.000012  min_lr: 0.000012  loss: 1.8232 (1.8616)  loss_scale: 8192.0000 (8885.5591)  weight_decay: 0.0500 (0.0500)  time: 0.5919  data: 0.0063  max mem: 13008
Epoch: [6]  [3080/3542]  eta: 0:05:24  lr: 0.000012  min_lr: 0.000012  loss: 1.8975 (1.8617)  loss_scale: 8192.0000 (8883.3080)  weight_decay: 0.0500 (0.0500)  time: 0.6101  data: 0.0241  max mem: 13008
Epoch: [6]  [3090/3542]  eta: 0:05:17  lr: 0.000012  min_lr: 0.000012  loss: 1.8711 (1.8618)  loss_scale: 8192.0000 (8881.0715)  weight_decay: 0.0500 (0.0500)  time: 0.6344  data: 0.0480  max mem: 13008
Epoch: [6]  [3100/3542]  eta: 0:05:10  lr: 0.000011  min_lr: 0.000011  loss: 1.8008 (1.8617)  loss_scale: 8192.0000 (8878.8494)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0253  max mem: 13008
Epoch: [6]  [3110/3542]  eta: 0:05:03  lr: 0.000011  min_lr: 0.000011  loss: 1.8115 (1.8617)  loss_scale: 8192.0000 (8876.6416)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0304  max mem: 13008
Epoch: [6]  [3120/3542]  eta: 0:04:56  lr: 0.000011  min_lr: 0.000011  loss: 1.8613 (1.8617)  loss_scale: 8192.0000 (8874.4479)  weight_decay: 0.0500 (0.0500)  time: 0.6533  data: 0.0673  max mem: 13008
Epoch: [6]  [3130/3542]  eta: 0:04:49  lr: 0.000011  min_lr: 0.000011  loss: 1.8428 (1.8616)  loss_scale: 8192.0000 (8872.2683)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.0415  max mem: 13008
Epoch: [6]  [3140/3542]  eta: 0:04:42  lr: 0.000011  min_lr: 0.000011  loss: 1.8438 (1.8617)  loss_scale: 8192.0000 (8870.1025)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.0317  max mem: 13008
Epoch: [6]  [3150/3542]  eta: 0:04:35  lr: 0.000011  min_lr: 0.000011  loss: 1.9033 (1.8619)  loss_scale: 8192.0000 (8867.9505)  weight_decay: 0.0500 (0.0500)  time: 0.6304  data: 0.0448  max mem: 13008
Epoch: [6]  [3160/3542]  eta: 0:04:27  lr: 0.000011  min_lr: 0.000011  loss: 1.8877 (1.8620)  loss_scale: 8192.0000 (8865.8121)  weight_decay: 0.0500 (0.0500)  time: 0.6251  data: 0.0394  max mem: 13008
Epoch: [6]  [3170/3542]  eta: 0:04:20  lr: 0.000011  min_lr: 0.000011  loss: 1.8750 (1.8622)  loss_scale: 8192.0000 (8863.6872)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.0608  max mem: 13008
Epoch: [6]  [3180/3542]  eta: 0:04:13  lr: 0.000011  min_lr: 0.000011  loss: 1.8340 (1.8621)  loss_scale: 8192.0000 (8861.5756)  weight_decay: 0.0500 (0.0500)  time: 0.6256  data: 0.0383  max mem: 13008
Epoch: [6]  [3190/3542]  eta: 0:04:06  lr: 0.000011  min_lr: 0.000011  loss: 1.8047 (1.8620)  loss_scale: 8192.0000 (8859.4773)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0071  max mem: 13008
Epoch: [6]  [3200/3542]  eta: 0:03:59  lr: 0.000011  min_lr: 0.000011  loss: 1.8252 (1.8620)  loss_scale: 8192.0000 (8857.3921)  weight_decay: 0.0500 (0.0500)  time: 0.6141  data: 0.0271  max mem: 13008
Epoch: [6]  [3210/3542]  eta: 0:03:52  lr: 0.000011  min_lr: 0.000011  loss: 1.8359 (1.8618)  loss_scale: 8192.0000 (8855.3198)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0250  max mem: 13008
Epoch: [6]  [3220/3542]  eta: 0:03:45  lr: 0.000011  min_lr: 0.000011  loss: 1.8438 (1.8620)  loss_scale: 8192.0000 (8853.2605)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0259  max mem: 13008
Epoch: [6]  [3230/3542]  eta: 0:03:38  lr: 0.000011  min_lr: 0.000011  loss: 1.9180 (1.8623)  loss_scale: 8192.0000 (8851.2139)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0213  max mem: 13008
Epoch: [6]  [3240/3542]  eta: 0:03:31  lr: 0.000011  min_lr: 0.000011  loss: 1.9111 (1.8622)  loss_scale: 8192.0000 (8849.1799)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0186  max mem: 13008
Epoch: [6]  [3250/3542]  eta: 0:03:24  lr: 0.000011  min_lr: 0.000011  loss: 1.8779 (1.8622)  loss_scale: 8192.0000 (8847.1584)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.0339  max mem: 13008
[2023-05-16 11:37:14,183] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 11:37:14,184] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [6]  [3260/3542]  eta: 0:03:17  lr: 0.000011  min_lr: 0.000011  loss: 1.8809 (1.8622)  loss_scale: 8192.0000 (8855.1978)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0242  max mem: 13008
Epoch: [6]  [3270/3542]  eta: 0:03:09  lr: 0.000011  min_lr: 0.000011  loss: 1.7998 (1.8621)  loss_scale: 16384.0000 (8878.2146)  weight_decay: 0.0500 (0.0500)  time: 0.5962  data: 0.0089  max mem: 13008
Epoch: [6]  [3280/3542]  eta: 0:03:02  lr: 0.000011  min_lr: 0.000011  loss: 1.8018 (1.8622)  loss_scale: 16384.0000 (8901.0911)  weight_decay: 0.0500 (0.0500)  time: 0.5963  data: 0.0090  max mem: 13008
[2023-05-16 11:37:32,163] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 24539
[2023-05-16 11:37:32,163] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 11:37:32,163] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [6]  [3290/3542]  eta: 0:02:55  lr: 0.000011  min_lr: 0.000011  loss: 1.8398 (1.8622)  loss_scale: 16384.0000 (8913.8718)  weight_decay: 0.0500 (0.0500)  time: 0.6440  data: 0.0571  max mem: 13008
Epoch: [6]  [3300/3542]  eta: 0:02:48  lr: 0.000011  min_lr: 0.000011  loss: 1.8174 (1.8620)  loss_scale: 8192.0000 (8911.6849)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.0515  max mem: 13008
Epoch: [6]  [3310/3542]  eta: 0:02:41  lr: 0.000011  min_lr: 0.000011  loss: 1.8174 (1.8619)  loss_scale: 8192.0000 (8909.5113)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0188  max mem: 13008
Epoch: [6]  [3320/3542]  eta: 0:02:34  lr: 0.000011  min_lr: 0.000011  loss: 1.8174 (1.8619)  loss_scale: 8192.0000 (8907.3508)  weight_decay: 0.0500 (0.0500)  time: 0.6362  data: 0.0491  max mem: 13008
Epoch: [6]  [3330/3542]  eta: 0:02:27  lr: 0.000011  min_lr: 0.000011  loss: 1.8037 (1.8617)  loss_scale: 8192.0000 (8905.2032)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0337  max mem: 13008
Epoch: [6]  [3340/3542]  eta: 0:02:20  lr: 0.000011  min_lr: 0.000011  loss: 1.8359 (1.8617)  loss_scale: 8192.0000 (8903.0685)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0194  max mem: 13008
Epoch: [6]  [3350/3542]  eta: 0:02:13  lr: 0.000011  min_lr: 0.000011  loss: 1.8564 (1.8616)  loss_scale: 8192.0000 (8900.9466)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.0401  max mem: 13008
Epoch: [6]  [3360/3542]  eta: 0:02:06  lr: 0.000011  min_lr: 0.000011  loss: 1.8701 (1.8618)  loss_scale: 8192.0000 (8898.8373)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.0435  max mem: 13008
Epoch: [6]  [3370/3542]  eta: 0:01:59  lr: 0.000011  min_lr: 0.000011  loss: 1.8652 (1.8616)  loss_scale: 8192.0000 (8896.7404)  weight_decay: 0.0500 (0.0500)  time: 0.6309  data: 0.0449  max mem: 13008
Epoch: [6]  [3380/3542]  eta: 0:01:52  lr: 0.000011  min_lr: 0.000011  loss: 1.8652 (1.8617)  loss_scale: 8192.0000 (8894.6560)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0405  max mem: 13008
Epoch: [6]  [3390/3542]  eta: 0:01:45  lr: 0.000011  min_lr: 0.000011  loss: 1.8809 (1.8618)  loss_scale: 8192.0000 (8892.5839)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0184  max mem: 13008
Epoch: [6]  [3400/3542]  eta: 0:01:38  lr: 0.000011  min_lr: 0.000011  loss: 1.8809 (1.8619)  loss_scale: 8192.0000 (8890.5240)  weight_decay: 0.0500 (0.0500)  time: 0.5948  data: 0.0087  max mem: 13008
Epoch: [6]  [3410/3542]  eta: 0:01:31  lr: 0.000011  min_lr: 0.000011  loss: 1.9287 (1.8623)  loss_scale: 8192.0000 (8888.4761)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0087  max mem: 13008
Epoch: [6]  [3420/3542]  eta: 0:01:24  lr: 0.000011  min_lr: 0.000011  loss: 1.9180 (1.8624)  loss_scale: 8192.0000 (8886.4402)  weight_decay: 0.0500 (0.0500)  time: 0.6054  data: 0.0197  max mem: 13008
Epoch: [6]  [3430/3542]  eta: 0:01:17  lr: 0.000011  min_lr: 0.000011  loss: 1.8887 (1.8625)  loss_scale: 8192.0000 (8884.4162)  weight_decay: 0.0500 (0.0500)  time: 0.6232  data: 0.0369  max mem: 13008
Epoch: [6]  [3440/3542]  eta: 0:01:10  lr: 0.000011  min_lr: 0.000011  loss: 1.8525 (1.8623)  loss_scale: 8192.0000 (8882.4040)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.0613  max mem: 13008
Epoch: [6]  [3450/3542]  eta: 0:01:03  lr: 0.000011  min_lr: 0.000011  loss: 1.8525 (1.8624)  loss_scale: 8192.0000 (8880.4034)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.0691  max mem: 13008
Epoch: [6]  [3460/3542]  eta: 0:00:56  lr: 0.000011  min_lr: 0.000011  loss: 1.7744 (1.8622)  loss_scale: 8192.0000 (8878.4143)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.0708  max mem: 13008
Epoch: [6]  [3470/3542]  eta: 0:00:49  lr: 0.000011  min_lr: 0.000011  loss: 1.8398 (1.8624)  loss_scale: 8192.0000 (8876.4368)  weight_decay: 0.0500 (0.0500)  time: 0.6756  data: 0.0893  max mem: 13008
Epoch: [6]  [3480/3542]  eta: 0:00:43  lr: 0.000011  min_lr: 0.000011  loss: 1.8643 (1.8624)  loss_scale: 8192.0000 (8874.4706)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.0596  max mem: 13008
Epoch: [6]  [3490/3542]  eta: 0:00:36  lr: 0.000011  min_lr: 0.000011  loss: 1.8223 (1.8622)  loss_scale: 8192.0000 (8872.5156)  weight_decay: 0.0500 (0.0500)  time: 0.6142  data: 0.0274  max mem: 13008
Epoch: [6]  [3500/3542]  eta: 0:00:29  lr: 0.000011  min_lr: 0.000011  loss: 1.8438 (1.8625)  loss_scale: 8192.0000 (8870.5718)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0381  max mem: 13008
Epoch: [6]  [3510/3542]  eta: 0:00:22  lr: 0.000011  min_lr: 0.000011  loss: 1.8867 (1.8625)  loss_scale: 8192.0000 (8868.6391)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0574  max mem: 13008
Epoch: [6]  [3520/3542]  eta: 0:00:15  lr: 0.000011  min_lr: 0.000011  loss: 1.8564 (1.8624)  loss_scale: 8192.0000 (8866.7174)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.0605  max mem: 13008
Epoch: [6]  [3530/3542]  eta: 0:00:08  lr: 0.000011  min_lr: 0.000011  loss: 1.7754 (1.8621)  loss_scale: 8192.0000 (8864.8066)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.0380  max mem: 13008
Epoch: [6]  [3540/3542]  eta: 0:00:01  lr: 0.000011  min_lr: 0.000011  loss: 1.8281 (1.8621)  loss_scale: 8192.0000 (8862.9065)  weight_decay: 0.0500 (0.0500)  time: 0.5930  data: 0.0082  max mem: 13008
Epoch: [6]  [3541/3542]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000011  loss: 1.8281 (1.8620)  loss_scale: 8192.0000 (8862.7171)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0082  max mem: 13008
Epoch: [6] Total time: 0:40:55 (0.6931 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000011  loss: 1.8281 (1.8620)  loss_scale: 8192.0000 (8862.7171)  weight_decay: 0.0500 (0.0500)
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 11:40:12,200] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-6/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:30:07    time: 11.5879  data: 7.7656  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:10:29    time: 4.3131  data: 0.7062  max mem: 13008
Test:  [ 20/156]  eta: 0:08:52    time: 3.5320  data: 0.0002  max mem: 13008
Test:  [ 30/156]  eta: 0:07:58    time: 3.5092  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 40/156]  eta: 0:07:11    time: 3.5094  data: 0.0002  max mem: 13008
Test:  [ 50/156]  eta: 0:06:32    time: 3.5657  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 60/156]  eta: 0:05:52    time: 3.5700  data: 0.0002  max mem: 13008
Test:  [ 70/156]  eta: 0:05:13    time: 3.4999  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:35    time: 3.4648  data: 0.0002  max mem: 13008
Test:  [ 90/156]  eta: 0:03:58    time: 3.5211  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:22    time: 3.5668  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:45    time: 3.5562  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:09    time: 3.6117  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:33    time: 3.5945  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:00:57    time: 3.5434  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:21    time: 3.5257  data: 0.0001  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.5176  data: 0.0001  max mem: 13008
Test: Total time: 0:09:20 (3.5951 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e6.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e6.json and result file is ./output_freeze/coco_captioning_result_val_e6.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e6.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1262877.60 tokens per second.
PTBTokenizer tokenized 61774 tokens at 463545.60 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 50744, 'reflen': 49135, 'guess': [50744, 45752, 40760, 35768], 'correct': [33538, 17085, 7569, 3061]}
ratio: 1.0327465147043648
Bleu_1: 0.661
Bleu_2: 0.497
Bleu_3: 0.358
Bleu_4: 0.250
computing METEOR score...
METEOR: 0.239
computing Rouge score...
ROUGE_L: 0.508
computing CIDEr score...
CIDEr: 0.812
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
Threads( StanfordCoreNLP ) [17.314 seconds]
SPICE evaluation took: 28.09 s
SPICE: 0.175
Performance of the network on the 5000 val images: 0.8%
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 11:50:23,468] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-best/mp_rank_00_model_states.pt
Max performance: 0.81%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [7]  [   0/3542]  eta: 21:35:29  lr: 0.000011  min_lr: 0.000011  loss: 1.7217 (1.7217)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 21.9450  data: 21.3020  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [7]  [  10/3542]  eta: 3:49:58  lr: 0.000011  min_lr: 0.000011  loss: 1.7646 (1.7869)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 3.9068  data: 3.3182  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [7]  [  20/3542]  eta: 3:02:11  lr: 0.000011  min_lr: 0.000011  loss: 1.7852 (1.7968)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 2.1617  data: 1.5801  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [7]  [  30/3542]  eta: 2:32:11  lr: 0.000011  min_lr: 0.000011  loss: 1.7715 (1.8075)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.8814  data: 1.2985  max mem: 13008
Epoch: [7]  [  40/3542]  eta: 2:12:16  lr: 0.000011  min_lr: 0.000011  loss: 1.7988 (1.8232)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3868  data: 0.8039  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [7]  [  50/3542]  eta: 2:01:35  lr: 0.000011  min_lr: 0.000011  loss: 1.8057 (1.8327)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2975  data: 0.7175  max mem: 13008
Epoch: [7]  [  60/3542]  eta: 1:53:46  lr: 0.000011  min_lr: 0.000011  loss: 1.7803 (1.8372)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3339  data: 0.7521  max mem: 13008
Epoch: [7]  [  70/3542]  eta: 1:46:10  lr: 0.000011  min_lr: 0.000011  loss: 1.7598 (1.8354)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1858  data: 0.6030  max mem: 13008
Epoch: [7]  [  80/3542]  eta: 1:40:17  lr: 0.000011  min_lr: 0.000011  loss: 1.7783 (1.8312)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0603  data: 0.4789  max mem: 13008
Epoch: [7]  [  90/3542]  eta: 1:35:25  lr: 0.000011  min_lr: 0.000011  loss: 1.8213 (1.8347)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0331  data: 0.4514  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [7]  [ 100/3542]  eta: 1:30:54  lr: 0.000011  min_lr: 0.000011  loss: 1.8350 (1.8338)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9628  data: 0.3809  max mem: 13008
Epoch: [7]  [ 110/3542]  eta: 1:26:22  lr: 0.000011  min_lr: 0.000011  loss: 1.8350 (1.8417)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8342  data: 0.2524  max mem: 13008
Epoch: [7]  [ 120/3542]  eta: 1:23:03  lr: 0.000011  min_lr: 0.000011  loss: 1.8672 (1.8459)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8075  data: 0.2232  max mem: 13008
Epoch: [7]  [ 130/3542]  eta: 1:19:37  lr: 0.000011  min_lr: 0.000011  loss: 1.9160 (1.8538)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7909  data: 0.2066  max mem: 13008
Epoch: [7]  [ 140/3542]  eta: 1:16:44  lr: 0.000011  min_lr: 0.000011  loss: 1.9707 (1.8587)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7326  data: 0.1507  max mem: 13008
Epoch: [7]  [ 150/3542]  eta: 1:14:35  lr: 0.000011  min_lr: 0.000011  loss: 1.8730 (1.8586)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7905  data: 0.2081  max mem: 13008
Epoch: [7]  [ 160/3542]  eta: 1:12:45  lr: 0.000010  min_lr: 0.000010  loss: 1.8418 (1.8602)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8473  data: 0.2647  max mem: 13008
Epoch: [7]  [ 170/3542]  eta: 1:10:42  lr: 0.000010  min_lr: 0.000010  loss: 1.8633 (1.8635)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7949  data: 0.2128  max mem: 13008
Epoch: [7]  [ 180/3542]  eta: 1:09:09  lr: 0.000010  min_lr: 0.000010  loss: 1.9404 (1.8658)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7790  data: 0.1963  max mem: 13008
Epoch: [7]  [ 190/3542]  eta: 1:07:02  lr: 0.000010  min_lr: 0.000010  loss: 1.8838 (1.8674)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7039  data: 0.1206  max mem: 13008
Epoch: [7]  [ 200/3542]  eta: 1:05:32  lr: 0.000010  min_lr: 0.000010  loss: 1.8320 (1.8657)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6556  data: 0.0726  max mem: 13008
[2023-05-16 11:54:24,765] [INFO] [logging.py:60:log_dist] [Rank 0] step=25000, skipped=24, lr=[1.041083733922181e-05, 1.041083733922181e-05], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 11:54:24,768] [INFO] [timer.py:157:stop] 0/25000, SamplesPerSec=55.547609713993864
Epoch: [7]  [ 210/3542]  eta: 1:03:50  lr: 0.000010  min_lr: 0.000010  loss: 1.8154 (1.8659)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.0836  max mem: 13008
Epoch: [7]  [ 220/3542]  eta: 1:02:27  lr: 0.000010  min_lr: 0.000010  loss: 1.8525 (1.8650)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.0577  max mem: 13008
Epoch: [7]  [ 230/3542]  eta: 1:01:14  lr: 0.000010  min_lr: 0.000010  loss: 1.8613 (1.8643)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6857  data: 0.1015  max mem: 13008
Epoch: [7]  [ 240/3542]  eta: 0:59:52  lr: 0.000010  min_lr: 0.000010  loss: 1.8613 (1.8639)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.0596  max mem: 13008
Epoch: [7]  [ 250/3542]  eta: 0:59:02  lr: 0.000010  min_lr: 0.000010  loss: 1.7686 (1.8625)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6905  data: 0.1062  max mem: 13008
Epoch: [7]  [ 260/3542]  eta: 0:58:00  lr: 0.000010  min_lr: 0.000010  loss: 1.8984 (1.8653)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7272  data: 0.1434  max mem: 13008
Epoch: [7]  [ 270/3542]  eta: 0:57:00  lr: 0.000010  min_lr: 0.000010  loss: 1.8506 (1.8629)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6587  data: 0.0744  max mem: 13008
Epoch: [7]  [ 280/3542]  eta: 0:55:58  lr: 0.000010  min_lr: 0.000010  loss: 1.8184 (1.8631)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.0429  max mem: 13008
Epoch: [7]  [ 290/3542]  eta: 0:55:03  lr: 0.000010  min_lr: 0.000010  loss: 1.8604 (1.8632)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0339  max mem: 13008
Epoch: [7]  [ 300/3542]  eta: 0:54:08  lr: 0.000010  min_lr: 0.000010  loss: 1.8457 (1.8642)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0290  max mem: 13008
Epoch: [7]  [ 310/3542]  eta: 0:53:25  lr: 0.000010  min_lr: 0.000010  loss: 1.8457 (1.8626)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.0562  max mem: 13008
Epoch: [7]  [ 320/3542]  eta: 0:52:44  lr: 0.000010  min_lr: 0.000010  loss: 1.8115 (1.8627)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6855  data: 0.1010  max mem: 13008
Epoch: [7]  [ 330/3542]  eta: 0:52:03  lr: 0.000010  min_lr: 0.000010  loss: 1.8525 (1.8647)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6718  data: 0.0869  max mem: 13008
Epoch: [7]  [ 340/3542]  eta: 0:51:30  lr: 0.000010  min_lr: 0.000010  loss: 1.8389 (1.8639)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6932  data: 0.1083  max mem: 13008
Epoch: [7]  [ 350/3542]  eta: 0:50:55  lr: 0.000010  min_lr: 0.000010  loss: 1.8320 (1.8635)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7026  data: 0.1177  max mem: 13008
Epoch: [7]  [ 360/3542]  eta: 0:50:20  lr: 0.000010  min_lr: 0.000010  loss: 1.8506 (1.8649)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6728  data: 0.0880  max mem: 13008
Epoch: [7]  [ 370/3542]  eta: 0:49:43  lr: 0.000010  min_lr: 0.000010  loss: 1.8906 (1.8657)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6523  data: 0.0679  max mem: 13008
Epoch: [7]  [ 380/3542]  eta: 0:49:07  lr: 0.000010  min_lr: 0.000010  loss: 1.8369 (1.8632)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0430  max mem: 13008
Epoch: [7]  [ 390/3542]  eta: 0:48:37  lr: 0.000010  min_lr: 0.000010  loss: 1.8652 (1.8657)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6437  data: 0.0592  max mem: 13008
Epoch: [7]  [ 400/3542]  eta: 0:48:03  lr: 0.000010  min_lr: 0.000010  loss: 1.8867 (1.8640)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6403  data: 0.0554  max mem: 13008
Epoch: [7]  [ 410/3542]  eta: 0:47:36  lr: 0.000010  min_lr: 0.000010  loss: 1.8057 (1.8620)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6515  data: 0.0655  max mem: 13008
Epoch: [7]  [ 420/3542]  eta: 0:47:06  lr: 0.000010  min_lr: 0.000010  loss: 1.8555 (1.8620)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.0753  max mem: 13008
Epoch: [7]  [ 430/3542]  eta: 0:46:37  lr: 0.000010  min_lr: 0.000010  loss: 1.8477 (1.8602)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0432  max mem: 13008
Epoch: [7]  [ 440/3542]  eta: 0:46:10  lr: 0.000010  min_lr: 0.000010  loss: 1.7783 (1.8589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0443  max mem: 13008
Epoch: [7]  [ 450/3542]  eta: 0:45:40  lr: 0.000010  min_lr: 0.000010  loss: 1.8027 (1.8597)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6105  data: 0.0243  max mem: 13008
Epoch: [7]  [ 460/3542]  eta: 0:45:13  lr: 0.000010  min_lr: 0.000010  loss: 1.8584 (1.8598)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6065  data: 0.0210  max mem: 13008
Epoch: [7]  [ 470/3542]  eta: 0:44:47  lr: 0.000010  min_lr: 0.000010  loss: 1.8516 (1.8591)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0346  max mem: 13008
Epoch: [7]  [ 480/3542]  eta: 0:44:22  lr: 0.000010  min_lr: 0.000010  loss: 1.8340 (1.8593)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6189  data: 0.0336  max mem: 13008
Epoch: [7]  [ 490/3542]  eta: 0:43:59  lr: 0.000010  min_lr: 0.000010  loss: 1.8613 (1.8592)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0455  max mem: 13008
Epoch: [7]  [ 500/3542]  eta: 0:43:36  lr: 0.000010  min_lr: 0.000010  loss: 1.7988 (1.8581)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.0429  max mem: 13008
Epoch: [7]  [ 510/3542]  eta: 0:43:13  lr: 0.000010  min_lr: 0.000010  loss: 1.7441 (1.8568)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0358  max mem: 13008
Epoch: [7]  [ 520/3542]  eta: 0:42:55  lr: 0.000010  min_lr: 0.000010  loss: 1.8066 (1.8572)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.0667  max mem: 13008
Epoch: [7]  [ 530/3542]  eta: 0:42:32  lr: 0.000010  min_lr: 0.000010  loss: 1.8115 (1.8560)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0558  max mem: 13008
Epoch: [7]  [ 540/3542]  eta: 0:42:09  lr: 0.000010  min_lr: 0.000010  loss: 1.8115 (1.8564)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0135  max mem: 13008
Epoch: [7]  [ 550/3542]  eta: 0:41:53  lr: 0.000010  min_lr: 0.000010  loss: 1.8408 (1.8562)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0557  max mem: 13008
Epoch: [7]  [ 560/3542]  eta: 0:41:33  lr: 0.000010  min_lr: 0.000010  loss: 1.8174 (1.8552)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6555  data: 0.0697  max mem: 13008
Epoch: [7]  [ 570/3542]  eta: 0:41:13  lr: 0.000010  min_lr: 0.000010  loss: 1.7852 (1.8548)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0345  max mem: 13008
Epoch: [7]  [ 580/3542]  eta: 0:40:54  lr: 0.000010  min_lr: 0.000010  loss: 1.8340 (1.8549)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6184  data: 0.0319  max mem: 13008
Epoch: [7]  [ 590/3542]  eta: 0:40:35  lr: 0.000010  min_lr: 0.000010  loss: 1.8203 (1.8543)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6241  data: 0.0372  max mem: 13008
Epoch: [7]  [ 600/3542]  eta: 0:40:21  lr: 0.000010  min_lr: 0.000010  loss: 1.7676 (1.8533)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.0749  max mem: 13008
Epoch: [7]  [ 610/3542]  eta: 0:40:06  lr: 0.000010  min_lr: 0.000010  loss: 1.8271 (1.8536)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6856  data: 0.1001  max mem: 13008
Epoch: [7]  [ 620/3542]  eta: 0:39:48  lr: 0.000010  min_lr: 0.000010  loss: 1.8271 (1.8523)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.0655  max mem: 13008
Epoch: [7]  [ 630/3542]  eta: 0:39:31  lr: 0.000010  min_lr: 0.000010  loss: 1.8809 (1.8533)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0385  max mem: 13008
Epoch: [7]  [ 640/3542]  eta: 0:39:16  lr: 0.000010  min_lr: 0.000010  loss: 1.8994 (1.8530)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6412  data: 0.0550  max mem: 13008
Epoch: [7]  [ 650/3542]  eta: 0:38:59  lr: 0.000010  min_lr: 0.000010  loss: 1.8516 (1.8536)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6419  data: 0.0552  max mem: 13008
Epoch: [7]  [ 660/3542]  eta: 0:38:44  lr: 0.000010  min_lr: 0.000010  loss: 1.8486 (1.8536)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.0509  max mem: 13008
Epoch: [7]  [ 670/3542]  eta: 0:38:31  lr: 0.000010  min_lr: 0.000010  loss: 1.8486 (1.8534)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.0816  max mem: 13008
Epoch: [7]  [ 680/3542]  eta: 0:38:14  lr: 0.000010  min_lr: 0.000010  loss: 1.8867 (1.8541)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.0567  max mem: 13008
Epoch: [7]  [ 690/3542]  eta: 0:38:00  lr: 0.000010  min_lr: 0.000010  loss: 1.9033 (1.8544)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6264  data: 0.0402  max mem: 13008
Epoch: [7]  [ 700/3542]  eta: 0:37:46  lr: 0.000010  min_lr: 0.000010  loss: 1.8281 (1.8543)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6559  data: 0.0703  max mem: 13008
Epoch: [7]  [ 710/3542]  eta: 0:37:33  lr: 0.000010  min_lr: 0.000010  loss: 1.7646 (1.8539)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6656  data: 0.0803  max mem: 13008
Epoch: [7]  [ 720/3542]  eta: 0:37:20  lr: 0.000010  min_lr: 0.000010  loss: 1.7969 (1.8533)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.0752  max mem: 13008
Epoch: [7]  [ 730/3542]  eta: 0:37:05  lr: 0.000010  min_lr: 0.000010  loss: 1.7910 (1.8521)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.0439  max mem: 13008
Epoch: [7]  [ 740/3542]  eta: 0:36:53  lr: 0.000010  min_lr: 0.000010  loss: 1.7910 (1.8510)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6562  data: 0.0711  max mem: 13008
[2023-05-16 12:00:15,144] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 12:00:15,145] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [7]  [ 750/3542]  eta: 0:36:41  lr: 0.000010  min_lr: 0.000010  loss: 1.7910 (1.8503)  loss_scale: 8192.0000 (8246.5406)  weight_decay: 0.0500 (0.0500)  time: 0.6916  data: 0.1069  max mem: 13008
Epoch: [7]  [ 760/3542]  eta: 0:36:27  lr: 0.000010  min_lr: 0.000010  loss: 1.8027 (1.8499)  loss_scale: 16384.0000 (8353.4717)  weight_decay: 0.0500 (0.0500)  time: 0.6396  data: 0.0542  max mem: 13008
Epoch: [7]  [ 770/3542]  eta: 0:36:13  lr: 0.000009  min_lr: 0.000009  loss: 1.8955 (1.8519)  loss_scale: 16384.0000 (8457.6291)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0331  max mem: 13008
Epoch: [7]  [ 780/3542]  eta: 0:36:05  lr: 0.000009  min_lr: 0.000009  loss: 1.8818 (1.8517)  loss_scale: 16384.0000 (8559.1191)  weight_decay: 0.0500 (0.0500)  time: 0.6986  data: 0.1133  max mem: 13008
Epoch: [7]  [ 790/3542]  eta: 0:35:53  lr: 0.000009  min_lr: 0.000009  loss: 1.8350 (1.8520)  loss_scale: 16384.0000 (8658.0430)  weight_decay: 0.0500 (0.0500)  time: 0.7217  data: 0.1362  max mem: 13008
Epoch: [7]  [ 800/3542]  eta: 0:35:43  lr: 0.000009  min_lr: 0.000009  loss: 1.8447 (1.8517)  loss_scale: 16384.0000 (8754.4969)  weight_decay: 0.0500 (0.0500)  time: 0.6988  data: 0.1140  max mem: 13008
Epoch: [7]  [ 810/3542]  eta: 0:35:29  lr: 0.000009  min_lr: 0.000009  loss: 1.7969 (1.8512)  loss_scale: 16384.0000 (8848.5721)  weight_decay: 0.0500 (0.0500)  time: 0.6500  data: 0.0652  max mem: 13008
Epoch: [7]  [ 820/3542]  eta: 0:35:17  lr: 0.000009  min_lr: 0.000009  loss: 1.8184 (1.8520)  loss_scale: 16384.0000 (8940.3557)  weight_decay: 0.0500 (0.0500)  time: 0.6168  data: 0.0320  max mem: 13008
Epoch: [7]  [ 830/3542]  eta: 0:35:04  lr: 0.000009  min_lr: 0.000009  loss: 1.8672 (1.8522)  loss_scale: 16384.0000 (9029.9302)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.0593  max mem: 13008
Epoch: [7]  [ 840/3542]  eta: 0:34:54  lr: 0.000009  min_lr: 0.000009  loss: 1.9092 (1.8531)  loss_scale: 16384.0000 (9117.3746)  weight_decay: 0.0500 (0.0500)  time: 0.6654  data: 0.0808  max mem: 13008
Epoch: [7]  [ 850/3542]  eta: 0:34:40  lr: 0.000009  min_lr: 0.000009  loss: 1.8779 (1.8530)  loss_scale: 16384.0000 (9202.7638)  weight_decay: 0.0500 (0.0500)  time: 0.6383  data: 0.0535  max mem: 13008
Epoch: [7]  [ 860/3542]  eta: 0:34:30  lr: 0.000009  min_lr: 0.000009  loss: 1.8652 (1.8536)  loss_scale: 16384.0000 (9286.1696)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.0515  max mem: 13008
Epoch: [7]  [ 870/3542]  eta: 0:34:21  lr: 0.000009  min_lr: 0.000009  loss: 1.8770 (1.8532)  loss_scale: 16384.0000 (9367.6602)  weight_decay: 0.0500 (0.0500)  time: 0.7144  data: 0.1297  max mem: 13008
Epoch: [7]  [ 880/3542]  eta: 0:34:10  lr: 0.000009  min_lr: 0.000009  loss: 1.7822 (1.8526)  loss_scale: 16384.0000 (9447.3008)  weight_decay: 0.0500 (0.0500)  time: 0.6929  data: 0.1085  max mem: 13008
Epoch: [7]  [ 890/3542]  eta: 0:33:59  lr: 0.000009  min_lr: 0.000009  loss: 1.8193 (1.8533)  loss_scale: 16384.0000 (9525.1538)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0783  max mem: 13008
Epoch: [7]  [ 900/3542]  eta: 0:33:49  lr: 0.000009  min_lr: 0.000009  loss: 1.8115 (1.8526)  loss_scale: 16384.0000 (9601.2786)  weight_decay: 0.0500 (0.0500)  time: 0.6846  data: 0.0999  max mem: 13008
Epoch: [7]  [ 910/3542]  eta: 0:33:38  lr: 0.000009  min_lr: 0.000009  loss: 1.8359 (1.8533)  loss_scale: 16384.0000 (9675.7322)  weight_decay: 0.0500 (0.0500)  time: 0.6737  data: 0.0899  max mem: 13008
Epoch: [7]  [ 920/3542]  eta: 0:33:31  lr: 0.000009  min_lr: 0.000009  loss: 1.9082 (1.8534)  loss_scale: 16384.0000 (9748.5689)  weight_decay: 0.0500 (0.0500)  time: 0.7138  data: 0.1300  max mem: 13008
Epoch: [7]  [ 930/3542]  eta: 0:33:21  lr: 0.000009  min_lr: 0.000009  loss: 1.8115 (1.8529)  loss_scale: 16384.0000 (9819.8410)  weight_decay: 0.0500 (0.0500)  time: 0.7283  data: 0.1440  max mem: 13008
Epoch: [7]  [ 940/3542]  eta: 0:33:11  lr: 0.000009  min_lr: 0.000009  loss: 1.8174 (1.8530)  loss_scale: 16384.0000 (9889.5983)  weight_decay: 0.0500 (0.0500)  time: 0.6874  data: 0.1020  max mem: 13008
Epoch: [7]  [ 950/3542]  eta: 0:33:03  lr: 0.000009  min_lr: 0.000009  loss: 1.8213 (1.8519)  loss_scale: 16384.0000 (9957.8885)  weight_decay: 0.0500 (0.0500)  time: 0.7132  data: 0.1280  max mem: 13008
Epoch: [7]  [ 960/3542]  eta: 0:32:55  lr: 0.000009  min_lr: 0.000009  loss: 1.8213 (1.8511)  loss_scale: 16384.0000 (10024.7575)  weight_decay: 0.0500 (0.0500)  time: 0.7543  data: 0.1696  max mem: 13008
Epoch: [7]  [ 970/3542]  eta: 0:32:49  lr: 0.000009  min_lr: 0.000009  loss: 1.7793 (1.8509)  loss_scale: 16384.0000 (10090.2492)  weight_decay: 0.0500 (0.0500)  time: 0.7918  data: 0.2077  max mem: 13008
Epoch: [7]  [ 980/3542]  eta: 0:32:43  lr: 0.000009  min_lr: 0.000009  loss: 1.7793 (1.8503)  loss_scale: 16384.0000 (10154.4057)  weight_decay: 0.0500 (0.0500)  time: 0.8248  data: 0.2415  max mem: 13008
Epoch: [7]  [ 990/3542]  eta: 0:32:37  lr: 0.000009  min_lr: 0.000009  loss: 1.8447 (1.8507)  loss_scale: 16384.0000 (10217.2674)  weight_decay: 0.0500 (0.0500)  time: 0.8447  data: 0.2595  max mem: 13008
Epoch: [7]  [1000/3542]  eta: 0:32:32  lr: 0.000009  min_lr: 0.000009  loss: 1.8447 (1.8508)  loss_scale: 16384.0000 (10278.8731)  weight_decay: 0.0500 (0.0500)  time: 0.8592  data: 0.2735  max mem: 13008
Epoch: [7]  [1010/3542]  eta: 0:32:27  lr: 0.000009  min_lr: 0.000009  loss: 1.8730 (1.8517)  loss_scale: 16384.0000 (10339.2601)  weight_decay: 0.0500 (0.0500)  time: 0.8709  data: 0.2873  max mem: 13008
Epoch: [7]  [1020/3542]  eta: 0:32:26  lr: 0.000009  min_lr: 0.000009  loss: 1.8428 (1.8516)  loss_scale: 16384.0000 (10398.4643)  weight_decay: 0.0500 (0.0500)  time: 0.9437  data: 0.3603  max mem: 13008
Epoch: [7]  [1030/3542]  eta: 0:32:21  lr: 0.000009  min_lr: 0.000009  loss: 1.8271 (1.8516)  loss_scale: 16384.0000 (10456.5199)  weight_decay: 0.0500 (0.0500)  time: 0.9529  data: 0.3697  max mem: 13008
Epoch: [7]  [1040/3542]  eta: 0:32:12  lr: 0.000009  min_lr: 0.000009  loss: 1.8262 (1.8519)  loss_scale: 16384.0000 (10513.4601)  weight_decay: 0.0500 (0.0500)  time: 0.8125  data: 0.2305  max mem: 13008
Epoch: [7]  [1050/3542]  eta: 0:32:03  lr: 0.000009  min_lr: 0.000009  loss: 1.8457 (1.8520)  loss_scale: 16384.0000 (10569.3168)  weight_decay: 0.0500 (0.0500)  time: 0.7298  data: 0.1473  max mem: 13008
Epoch: [7]  [1060/3542]  eta: 0:32:00  lr: 0.000009  min_lr: 0.000009  loss: 1.8193 (1.8516)  loss_scale: 16384.0000 (10624.1206)  weight_decay: 0.0500 (0.0500)  time: 0.8503  data: 0.2656  max mem: 13008
Epoch: [7]  [1070/3542]  eta: 0:31:59  lr: 0.000009  min_lr: 0.000009  loss: 1.8311 (1.8517)  loss_scale: 16384.0000 (10677.9010)  weight_decay: 0.0500 (0.0500)  time: 1.0211  data: 0.4371  max mem: 13008
Epoch: [7]  [1080/3542]  eta: 0:31:57  lr: 0.000009  min_lr: 0.000009  loss: 1.8213 (1.8520)  loss_scale: 16384.0000 (10730.6864)  weight_decay: 0.0500 (0.0500)  time: 1.0429  data: 0.4599  max mem: 13008
Epoch: [7]  [1090/3542]  eta: 0:31:49  lr: 0.000009  min_lr: 0.000009  loss: 1.7979 (1.8520)  loss_scale: 16384.0000 (10782.5041)  weight_decay: 0.0500 (0.0500)  time: 0.8829  data: 0.2994  max mem: 13008
Epoch: [7]  [1100/3542]  eta: 0:31:42  lr: 0.000009  min_lr: 0.000009  loss: 1.8184 (1.8520)  loss_scale: 16384.0000 (10833.3806)  weight_decay: 0.0500 (0.0500)  time: 0.7859  data: 0.2027  max mem: 13008
Epoch: [7]  [1110/3542]  eta: 0:31:33  lr: 0.000009  min_lr: 0.000009  loss: 1.7881 (1.8518)  loss_scale: 16384.0000 (10883.3411)  weight_decay: 0.0500 (0.0500)  time: 0.7849  data: 0.2013  max mem: 13008
Epoch: [7]  [1120/3542]  eta: 0:31:24  lr: 0.000009  min_lr: 0.000009  loss: 1.7881 (1.8514)  loss_scale: 16384.0000 (10932.4103)  weight_decay: 0.0500 (0.0500)  time: 0.7291  data: 0.1453  max mem: 13008
Epoch: [7]  [1130/3542]  eta: 0:31:16  lr: 0.000009  min_lr: 0.000009  loss: 1.8223 (1.8518)  loss_scale: 16384.0000 (10980.6118)  weight_decay: 0.0500 (0.0500)  time: 0.7452  data: 0.1616  max mem: 13008
Epoch: [7]  [1140/3542]  eta: 0:31:05  lr: 0.000009  min_lr: 0.000009  loss: 1.7812 (1.8513)  loss_scale: 16384.0000 (11027.9684)  weight_decay: 0.0500 (0.0500)  time: 0.7053  data: 0.1220  max mem: 13008
Epoch: [7]  [1150/3542]  eta: 0:30:57  lr: 0.000009  min_lr: 0.000009  loss: 1.8574 (1.8519)  loss_scale: 16384.0000 (11074.5022)  weight_decay: 0.0500 (0.0500)  time: 0.6961  data: 0.0748  max mem: 13008
Epoch: [7]  [1160/3542]  eta: 0:30:47  lr: 0.000009  min_lr: 0.000009  loss: 1.9102 (1.8519)  loss_scale: 16384.0000 (11120.2343)  weight_decay: 0.0500 (0.0500)  time: 0.6975  data: 0.0759  max mem: 13008
Epoch: [7]  [1170/3542]  eta: 0:30:38  lr: 0.000009  min_lr: 0.000009  loss: 1.8564 (1.8518)  loss_scale: 16384.0000 (11165.1853)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0888  max mem: 13008
Epoch: [7]  [1180/3542]  eta: 0:30:28  lr: 0.000009  min_lr: 0.000009  loss: 1.8604 (1.8527)  loss_scale: 16384.0000 (11209.3751)  weight_decay: 0.0500 (0.0500)  time: 0.7081  data: 0.1238  max mem: 13008
Epoch: [7]  [1190/3542]  eta: 0:30:18  lr: 0.000009  min_lr: 0.000009  loss: 1.8848 (1.8527)  loss_scale: 16384.0000 (11252.8228)  weight_decay: 0.0500 (0.0500)  time: 0.6653  data: 0.0805  max mem: 13008
Epoch: [7]  [1200/3542]  eta: 0:30:08  lr: 0.000009  min_lr: 0.000009  loss: 1.8174 (1.8528)  loss_scale: 16384.0000 (11295.5470)  weight_decay: 0.0500 (0.0500)  time: 0.6577  data: 0.0731  max mem: 13008
[2023-05-16 12:05:57,760] [INFO] [logging.py:60:log_dist] [Rank 0] step=26000, skipped=24, lr=[8.817931927241381e-06, 8.817931927241381e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 12:05:57,763] [INFO] [timer.py:157:stop] 0/26000, SamplesPerSec=55.54773905760031
Epoch: [7]  [1210/3542]  eta: 0:30:01  lr: 0.000009  min_lr: 0.000009  loss: 1.7656 (1.8522)  loss_scale: 16384.0000 (11337.5656)  weight_decay: 0.0500 (0.0500)  time: 0.7332  data: 0.1491  max mem: 13008
Epoch: [7]  [1220/3542]  eta: 0:29:50  lr: 0.000009  min_lr: 0.000009  loss: 1.7148 (1.8519)  loss_scale: 16384.0000 (11378.8960)  weight_decay: 0.0500 (0.0500)  time: 0.6834  data: 0.0990  max mem: 13008
Epoch: [7]  [1230/3542]  eta: 0:29:40  lr: 0.000009  min_lr: 0.000009  loss: 1.8281 (1.8519)  loss_scale: 16384.0000 (11419.5548)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.0507  max mem: 13008
Epoch: [7]  [1240/3542]  eta: 0:29:31  lr: 0.000009  min_lr: 0.000009  loss: 1.8301 (1.8518)  loss_scale: 16384.0000 (11459.5584)  weight_decay: 0.0500 (0.0500)  time: 0.6764  data: 0.0923  max mem: 13008
Epoch: [7]  [1250/3542]  eta: 0:29:21  lr: 0.000009  min_lr: 0.000009  loss: 1.8672 (1.8521)  loss_scale: 16384.0000 (11498.9225)  weight_decay: 0.0500 (0.0500)  time: 0.6782  data: 0.0935  max mem: 13008
Epoch: [7]  [1260/3542]  eta: 0:29:14  lr: 0.000009  min_lr: 0.000009  loss: 1.8535 (1.8520)  loss_scale: 16384.0000 (11537.6622)  weight_decay: 0.0500 (0.0500)  time: 0.7412  data: 0.1562  max mem: 13008
Epoch: [7]  [1270/3542]  eta: 0:29:05  lr: 0.000009  min_lr: 0.000009  loss: 1.8398 (1.8521)  loss_scale: 16384.0000 (11575.7923)  weight_decay: 0.0500 (0.0500)  time: 0.7410  data: 0.1566  max mem: 13008
Epoch: [7]  [1280/3542]  eta: 0:28:55  lr: 0.000009  min_lr: 0.000009  loss: 1.8730 (1.8523)  loss_scale: 16384.0000 (11613.3271)  weight_decay: 0.0500 (0.0500)  time: 0.6620  data: 0.0773  max mem: 13008
Epoch: [7]  [1290/3542]  eta: 0:28:47  lr: 0.000009  min_lr: 0.000009  loss: 1.7627 (1.8518)  loss_scale: 16384.0000 (11650.2804)  weight_decay: 0.0500 (0.0500)  time: 0.6763  data: 0.0910  max mem: 13008
Epoch: [7]  [1300/3542]  eta: 0:28:37  lr: 0.000009  min_lr: 0.000009  loss: 1.7627 (1.8518)  loss_scale: 16384.0000 (11686.6656)  weight_decay: 0.0500 (0.0500)  time: 0.6768  data: 0.0912  max mem: 13008
[2023-05-16 12:07:06,226] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 26101
[2023-05-16 12:07:06,227] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 12:07:06,227] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [7]  [1310/3542]  eta: 0:28:27  lr: 0.000009  min_lr: 0.000009  loss: 1.8320 (1.8517)  loss_scale: 16384.0000 (11697.5011)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.0476  max mem: 13008
Epoch: [7]  [1320/3542]  eta: 0:28:17  lr: 0.000009  min_lr: 0.000009  loss: 1.7988 (1.8510)  loss_scale: 8192.0000 (11670.9644)  weight_decay: 0.0500 (0.0500)  time: 0.6347  data: 0.0498  max mem: 13008
Epoch: [7]  [1330/3542]  eta: 0:28:07  lr: 0.000009  min_lr: 0.000009  loss: 1.7744 (1.8508)  loss_scale: 8192.0000 (11644.8264)  weight_decay: 0.0500 (0.0500)  time: 0.6275  data: 0.0429  max mem: 13008
Epoch: [7]  [1340/3542]  eta: 0:27:57  lr: 0.000009  min_lr: 0.000009  loss: 1.7803 (1.8506)  loss_scale: 8192.0000 (11619.0783)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.0484  max mem: 13008
Epoch: [7]  [1350/3542]  eta: 0:27:48  lr: 0.000009  min_lr: 0.000009  loss: 1.8340 (1.8508)  loss_scale: 8192.0000 (11593.7113)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0761  max mem: 13008
Epoch: [7]  [1360/3542]  eta: 0:27:39  lr: 0.000009  min_lr: 0.000009  loss: 1.8730 (1.8506)  loss_scale: 8192.0000 (11568.7171)  weight_decay: 0.0500 (0.0500)  time: 0.6551  data: 0.0698  max mem: 13008
Epoch: [7]  [1370/3542]  eta: 0:27:29  lr: 0.000009  min_lr: 0.000009  loss: 1.7930 (1.8501)  loss_scale: 8192.0000 (11544.0875)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.0476  max mem: 13008
Epoch: [7]  [1380/3542]  eta: 0:27:19  lr: 0.000009  min_lr: 0.000009  loss: 1.7764 (1.8498)  loss_scale: 8192.0000 (11519.8146)  weight_decay: 0.0500 (0.0500)  time: 0.6084  data: 0.0225  max mem: 13008
Epoch: [7]  [1390/3542]  eta: 0:27:09  lr: 0.000009  min_lr: 0.000009  loss: 1.7939 (1.8497)  loss_scale: 8192.0000 (11495.8907)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0282  max mem: 13008
Epoch: [7]  [1400/3542]  eta: 0:27:00  lr: 0.000009  min_lr: 0.000009  loss: 1.8242 (1.8497)  loss_scale: 8192.0000 (11472.3084)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0468  max mem: 13008
Epoch: [7]  [1410/3542]  eta: 0:26:50  lr: 0.000009  min_lr: 0.000009  loss: 1.8330 (1.8499)  loss_scale: 8192.0000 (11449.0602)  weight_decay: 0.0500 (0.0500)  time: 0.6350  data: 0.0506  max mem: 13008
Epoch: [7]  [1420/3542]  eta: 0:26:40  lr: 0.000008  min_lr: 0.000008  loss: 1.8330 (1.8498)  loss_scale: 8192.0000 (11426.1393)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.0369  max mem: 13008
Epoch: [7]  [1430/3542]  eta: 0:26:31  lr: 0.000008  min_lr: 0.000008  loss: 1.8379 (1.8496)  loss_scale: 8192.0000 (11403.5388)  weight_decay: 0.0500 (0.0500)  time: 0.5971  data: 0.0125  max mem: 13008
Epoch: [7]  [1440/3542]  eta: 0:26:21  lr: 0.000008  min_lr: 0.000008  loss: 1.7412 (1.8493)  loss_scale: 8192.0000 (11381.2519)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.0247  max mem: 13008
Epoch: [7]  [1450/3542]  eta: 0:26:12  lr: 0.000008  min_lr: 0.000008  loss: 1.7178 (1.8487)  loss_scale: 8192.0000 (11359.2722)  weight_decay: 0.0500 (0.0500)  time: 0.6380  data: 0.0521  max mem: 13008
Epoch: [7]  [1460/3542]  eta: 0:26:03  lr: 0.000008  min_lr: 0.000008  loss: 1.7549 (1.8491)  loss_scale: 8192.0000 (11337.5934)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.0693  max mem: 13008
Epoch: [7]  [1470/3542]  eta: 0:25:55  lr: 0.000008  min_lr: 0.000008  loss: 1.9189 (1.8494)  loss_scale: 8192.0000 (11316.2094)  weight_decay: 0.0500 (0.0500)  time: 0.6608  data: 0.0751  max mem: 13008
Epoch: [7]  [1480/3542]  eta: 0:25:45  lr: 0.000008  min_lr: 0.000008  loss: 1.9395 (1.8499)  loss_scale: 8192.0000 (11295.1141)  weight_decay: 0.0500 (0.0500)  time: 0.6250  data: 0.0393  max mem: 13008
Epoch: [7]  [1490/3542]  eta: 0:25:36  lr: 0.000008  min_lr: 0.000008  loss: 1.9766 (1.8508)  loss_scale: 8192.0000 (11274.3018)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0192  max mem: 13008
Epoch: [7]  [1500/3542]  eta: 0:25:26  lr: 0.000008  min_lr: 0.000008  loss: 1.8984 (1.8508)  loss_scale: 8192.0000 (11253.7668)  weight_decay: 0.0500 (0.0500)  time: 0.6100  data: 0.0237  max mem: 13008
Epoch: [7]  [1510/3542]  eta: 0:25:17  lr: 0.000008  min_lr: 0.000008  loss: 1.8506 (1.8506)  loss_scale: 8192.0000 (11233.5036)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0444  max mem: 13008
Epoch: [7]  [1520/3542]  eta: 0:25:09  lr: 0.000008  min_lr: 0.000008  loss: 1.8535 (1.8506)  loss_scale: 8192.0000 (11213.5069)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.0694  max mem: 13008
Epoch: [7]  [1530/3542]  eta: 0:24:59  lr: 0.000008  min_lr: 0.000008  loss: 1.9062 (1.8513)  loss_scale: 8192.0000 (11193.7714)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.0298  max mem: 13008
Epoch: [7]  [1540/3542]  eta: 0:24:50  lr: 0.000008  min_lr: 0.000008  loss: 1.8965 (1.8514)  loss_scale: 8192.0000 (11174.2920)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0146  max mem: 13008
Epoch: [7]  [1550/3542]  eta: 0:24:41  lr: 0.000008  min_lr: 0.000008  loss: 1.8145 (1.8510)  loss_scale: 8192.0000 (11155.0638)  weight_decay: 0.0500 (0.0500)  time: 0.6138  data: 0.0286  max mem: 13008
Epoch: [7]  [1560/3542]  eta: 0:24:32  lr: 0.000008  min_lr: 0.000008  loss: 1.8145 (1.8510)  loss_scale: 8192.0000 (11136.0820)  weight_decay: 0.0500 (0.0500)  time: 0.6118  data: 0.0269  max mem: 13008
Epoch: [7]  [1570/3542]  eta: 0:24:23  lr: 0.000008  min_lr: 0.000008  loss: 1.8770 (1.8515)  loss_scale: 8192.0000 (11117.3418)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.0365  max mem: 13008
Epoch: [7]  [1580/3542]  eta: 0:24:13  lr: 0.000008  min_lr: 0.000008  loss: 1.8643 (1.8513)  loss_scale: 8192.0000 (11098.8387)  weight_decay: 0.0500 (0.0500)  time: 0.6094  data: 0.0240  max mem: 13008
Epoch: [7]  [1590/3542]  eta: 0:24:04  lr: 0.000008  min_lr: 0.000008  loss: 1.8418 (1.8514)  loss_scale: 8192.0000 (11080.5682)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0005  max mem: 13008
Epoch: [7]  [1600/3542]  eta: 0:23:55  lr: 0.000008  min_lr: 0.000008  loss: 1.8330 (1.8512)  loss_scale: 8192.0000 (11062.5259)  weight_decay: 0.0500 (0.0500)  time: 0.5866  data: 0.0005  max mem: 13008
Epoch: [7]  [1610/3542]  eta: 0:23:46  lr: 0.000008  min_lr: 0.000008  loss: 1.7695 (1.8513)  loss_scale: 8192.0000 (11044.7076)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0145  max mem: 13008
Epoch: [7]  [1620/3542]  eta: 0:23:37  lr: 0.000008  min_lr: 0.000008  loss: 1.8340 (1.8517)  loss_scale: 8192.0000 (11027.1092)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.0401  max mem: 13008
Epoch: [7]  [1630/3542]  eta: 0:23:28  lr: 0.000008  min_lr: 0.000008  loss: 1.7568 (1.8509)  loss_scale: 8192.0000 (11009.7265)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0319  max mem: 13008
Epoch: [7]  [1640/3542]  eta: 0:23:19  lr: 0.000008  min_lr: 0.000008  loss: 1.7500 (1.8509)  loss_scale: 8192.0000 (10992.5558)  weight_decay: 0.0500 (0.0500)  time: 0.5994  data: 0.0131  max mem: 13008
Epoch: [7]  [1650/3542]  eta: 0:23:11  lr: 0.000008  min_lr: 0.000008  loss: 1.8145 (1.8517)  loss_scale: 8192.0000 (10975.5930)  weight_decay: 0.0500 (0.0500)  time: 0.6055  data: 0.0199  max mem: 13008
Epoch: [7]  [1660/3542]  eta: 0:23:02  lr: 0.000008  min_lr: 0.000008  loss: 1.8760 (1.8516)  loss_scale: 8192.0000 (10958.8344)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0334  max mem: 13008
Epoch: [7]  [1670/3542]  eta: 0:22:54  lr: 0.000008  min_lr: 0.000008  loss: 1.8555 (1.8517)  loss_scale: 8192.0000 (10942.2765)  weight_decay: 0.0500 (0.0500)  time: 0.6398  data: 0.0540  max mem: 13008
Epoch: [7]  [1680/3542]  eta: 0:22:45  lr: 0.000008  min_lr: 0.000008  loss: 1.8232 (1.8514)  loss_scale: 8192.0000 (10925.9155)  weight_decay: 0.0500 (0.0500)  time: 0.6247  data: 0.0394  max mem: 13008
Epoch: [7]  [1690/3542]  eta: 0:22:36  lr: 0.000008  min_lr: 0.000008  loss: 1.8379 (1.8513)  loss_scale: 8192.0000 (10909.7481)  weight_decay: 0.0500 (0.0500)  time: 0.6119  data: 0.0271  max mem: 13008
Epoch: [7]  [1700/3542]  eta: 0:22:29  lr: 0.000008  min_lr: 0.000008  loss: 1.8379 (1.8512)  loss_scale: 8192.0000 (10893.7707)  weight_decay: 0.0500 (0.0500)  time: 0.6638  data: 0.0790  max mem: 13008
Epoch: [7]  [1710/3542]  eta: 0:22:20  lr: 0.000008  min_lr: 0.000008  loss: 1.8066 (1.8511)  loss_scale: 8192.0000 (10877.9801)  weight_decay: 0.0500 (0.0500)  time: 0.6433  data: 0.0582  max mem: 13008
Epoch: [7]  [1720/3542]  eta: 0:22:11  lr: 0.000008  min_lr: 0.000008  loss: 1.8066 (1.8508)  loss_scale: 8192.0000 (10862.3730)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0003  max mem: 13008
Epoch: [7]  [1730/3542]  eta: 0:22:02  lr: 0.000008  min_lr: 0.000008  loss: 1.8291 (1.8507)  loss_scale: 8192.0000 (10846.9463)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0116  max mem: 13008
Epoch: [7]  [1740/3542]  eta: 0:21:54  lr: 0.000008  min_lr: 0.000008  loss: 1.8291 (1.8508)  loss_scale: 8192.0000 (10831.6967)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.0408  max mem: 13008
Epoch: [7]  [1750/3542]  eta: 0:21:47  lr: 0.000008  min_lr: 0.000008  loss: 1.8506 (1.8511)  loss_scale: 8192.0000 (10816.6214)  weight_decay: 0.0500 (0.0500)  time: 0.6771  data: 0.0906  max mem: 13008
Epoch: [7]  [1760/3542]  eta: 0:21:38  lr: 0.000008  min_lr: 0.000008  loss: 1.8389 (1.8509)  loss_scale: 8192.0000 (10801.7172)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.0819  max mem: 13008
Epoch: [7]  [1770/3542]  eta: 0:21:30  lr: 0.000008  min_lr: 0.000008  loss: 1.7822 (1.8507)  loss_scale: 8192.0000 (10786.9814)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0207  max mem: 13008
Epoch: [7]  [1780/3542]  eta: 0:21:21  lr: 0.000008  min_lr: 0.000008  loss: 1.8105 (1.8508)  loss_scale: 8192.0000 (10772.4110)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0005  max mem: 13008
Epoch: [7]  [1790/3542]  eta: 0:21:13  lr: 0.000008  min_lr: 0.000008  loss: 1.8486 (1.8511)  loss_scale: 8192.0000 (10758.0034)  weight_decay: 0.0500 (0.0500)  time: 0.5944  data: 0.0089  max mem: 13008
Epoch: [7]  [1800/3542]  eta: 0:21:04  lr: 0.000008  min_lr: 0.000008  loss: 1.8242 (1.8511)  loss_scale: 8192.0000 (10743.7557)  weight_decay: 0.0500 (0.0500)  time: 0.6224  data: 0.0372  max mem: 13008
Epoch: [7]  [1810/3542]  eta: 0:20:57  lr: 0.000008  min_lr: 0.000008  loss: 1.8867 (1.8511)  loss_scale: 8192.0000 (10729.6654)  weight_decay: 0.0500 (0.0500)  time: 0.6565  data: 0.0710  max mem: 13008
Epoch: [7]  [1820/3542]  eta: 0:20:48  lr: 0.000008  min_lr: 0.000008  loss: 1.8984 (1.8514)  loss_scale: 8192.0000 (10715.7298)  weight_decay: 0.0500 (0.0500)  time: 0.6494  data: 0.0640  max mem: 13008
Epoch: [7]  [1830/3542]  eta: 0:20:40  lr: 0.000008  min_lr: 0.000008  loss: 1.8604 (1.8510)  loss_scale: 8192.0000 (10701.9465)  weight_decay: 0.0500 (0.0500)  time: 0.6331  data: 0.0476  max mem: 13008
Epoch: [7]  [1840/3542]  eta: 0:20:32  lr: 0.000008  min_lr: 0.000008  loss: 1.7617 (1.8504)  loss_scale: 8192.0000 (10688.3129)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0566  max mem: 13008
Epoch: [7]  [1850/3542]  eta: 0:20:24  lr: 0.000008  min_lr: 0.000008  loss: 1.7725 (1.8505)  loss_scale: 8192.0000 (10674.8266)  weight_decay: 0.0500 (0.0500)  time: 0.6416  data: 0.0561  max mem: 13008
Epoch: [7]  [1860/3542]  eta: 0:20:16  lr: 0.000008  min_lr: 0.000008  loss: 1.8760 (1.8507)  loss_scale: 8192.0000 (10661.4852)  weight_decay: 0.0500 (0.0500)  time: 0.6273  data: 0.0424  max mem: 13008
Epoch: [7]  [1870/3542]  eta: 0:20:08  lr: 0.000008  min_lr: 0.000008  loss: 1.8135 (1.8504)  loss_scale: 8192.0000 (10648.2865)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.0440  max mem: 13008
Epoch: [7]  [1880/3542]  eta: 0:20:01  lr: 0.000008  min_lr: 0.000008  loss: 1.7959 (1.8500)  loss_scale: 8192.0000 (10635.2281)  weight_decay: 0.0500 (0.0500)  time: 0.6606  data: 0.0753  max mem: 13008
Epoch: [7]  [1890/3542]  eta: 0:19:53  lr: 0.000008  min_lr: 0.000008  loss: 1.8477 (1.8501)  loss_scale: 8192.0000 (10622.3078)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0769  max mem: 13008
Epoch: [7]  [1900/3542]  eta: 0:19:45  lr: 0.000008  min_lr: 0.000008  loss: 1.8604 (1.8499)  loss_scale: 8192.0000 (10609.5234)  weight_decay: 0.0500 (0.0500)  time: 0.6527  data: 0.0679  max mem: 13008
Epoch: [7]  [1910/3542]  eta: 0:19:37  lr: 0.000008  min_lr: 0.000008  loss: 1.8203 (1.8504)  loss_scale: 8192.0000 (10596.8728)  weight_decay: 0.0500 (0.0500)  time: 0.6345  data: 0.0489  max mem: 13008
Epoch: [7]  [1920/3542]  eta: 0:19:29  lr: 0.000008  min_lr: 0.000008  loss: 1.8076 (1.8501)  loss_scale: 8192.0000 (10584.3540)  weight_decay: 0.0500 (0.0500)  time: 0.6521  data: 0.0664  max mem: 13008
Epoch: [7]  [1930/3542]  eta: 0:19:21  lr: 0.000008  min_lr: 0.000008  loss: 1.7705 (1.8499)  loss_scale: 8192.0000 (10571.9648)  weight_decay: 0.0500 (0.0500)  time: 0.6451  data: 0.0602  max mem: 13008
Epoch: [7]  [1940/3542]  eta: 0:19:13  lr: 0.000008  min_lr: 0.000008  loss: 1.8867 (1.8501)  loss_scale: 8192.0000 (10559.7032)  weight_decay: 0.0500 (0.0500)  time: 0.5933  data: 0.0084  max mem: 13008
Epoch: [7]  [1950/3542]  eta: 0:19:05  lr: 0.000008  min_lr: 0.000008  loss: 1.8740 (1.8500)  loss_scale: 8192.0000 (10547.5674)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.0456  max mem: 13008
Epoch: [7]  [1960/3542]  eta: 0:18:57  lr: 0.000008  min_lr: 0.000008  loss: 1.8203 (1.8500)  loss_scale: 8192.0000 (10535.5553)  weight_decay: 0.0500 (0.0500)  time: 0.6266  data: 0.0410  max mem: 13008
Epoch: [7]  [1970/3542]  eta: 0:18:49  lr: 0.000008  min_lr: 0.000008  loss: 1.8096 (1.8495)  loss_scale: 8192.0000 (10523.6651)  weight_decay: 0.0500 (0.0500)  time: 0.6102  data: 0.0255  max mem: 13008
Epoch: [7]  [1980/3542]  eta: 0:18:41  lr: 0.000008  min_lr: 0.000008  loss: 1.7334 (1.8494)  loss_scale: 8192.0000 (10511.8950)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.0652  max mem: 13008
Epoch: [7]  [1990/3542]  eta: 0:18:34  lr: 0.000008  min_lr: 0.000008  loss: 1.7959 (1.8493)  loss_scale: 8192.0000 (10500.2431)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0801  max mem: 13008
Epoch: [7]  [2000/3542]  eta: 0:18:26  lr: 0.000008  min_lr: 0.000008  loss: 1.8311 (1.8495)  loss_scale: 8192.0000 (10488.7076)  weight_decay: 0.0500 (0.0500)  time: 0.6492  data: 0.0645  max mem: 13008
Epoch: [7]  [2010/3542]  eta: 0:18:18  lr: 0.000008  min_lr: 0.000008  loss: 1.8652 (1.8495)  loss_scale: 8192.0000 (10477.2869)  weight_decay: 0.0500 (0.0500)  time: 0.6204  data: 0.0349  max mem: 13008
Epoch: [7]  [2020/3542]  eta: 0:18:10  lr: 0.000008  min_lr: 0.000008  loss: 1.8740 (1.8498)  loss_scale: 8192.0000 (10465.9792)  weight_decay: 0.0500 (0.0500)  time: 0.6169  data: 0.0307  max mem: 13008
Epoch: [7]  [2030/3542]  eta: 0:18:02  lr: 0.000008  min_lr: 0.000008  loss: 1.8799 (1.8497)  loss_scale: 8192.0000 (10454.7829)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0342  max mem: 13008
Epoch: [7]  [2040/3542]  eta: 0:17:54  lr: 0.000008  min_lr: 0.000008  loss: 1.8555 (1.8500)  loss_scale: 8192.0000 (10443.6962)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0345  max mem: 13008
Epoch: [7]  [2050/3542]  eta: 0:17:47  lr: 0.000008  min_lr: 0.000008  loss: 1.8555 (1.8501)  loss_scale: 8192.0000 (10432.7177)  weight_decay: 0.0500 (0.0500)  time: 0.6278  data: 0.0411  max mem: 13008
Epoch: [7]  [2060/3542]  eta: 0:17:39  lr: 0.000008  min_lr: 0.000008  loss: 1.8613 (1.8500)  loss_scale: 8192.0000 (10421.8457)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0250  max mem: 13008
Epoch: [7]  [2070/3542]  eta: 0:17:31  lr: 0.000008  min_lr: 0.000008  loss: 1.8730 (1.8505)  loss_scale: 8192.0000 (10411.0787)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0298  max mem: 13008
Epoch: [7]  [2080/3542]  eta: 0:17:23  lr: 0.000008  min_lr: 0.000008  loss: 1.8311 (1.8502)  loss_scale: 8192.0000 (10400.4152)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0418  max mem: 13008
Epoch: [7]  [2090/3542]  eta: 0:17:15  lr: 0.000008  min_lr: 0.000008  loss: 1.7988 (1.8500)  loss_scale: 8192.0000 (10389.8537)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0233  max mem: 13008
Epoch: [7]  [2100/3542]  eta: 0:17:07  lr: 0.000007  min_lr: 0.000007  loss: 1.8887 (1.8505)  loss_scale: 8192.0000 (10379.3927)  weight_decay: 0.0500 (0.0500)  time: 0.5976  data: 0.0121  max mem: 13008
Epoch: [7]  [2110/3542]  eta: 0:16:59  lr: 0.000007  min_lr: 0.000007  loss: 1.8955 (1.8506)  loss_scale: 8192.0000 (10369.0308)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0233  max mem: 13008
Epoch: [7]  [2120/3542]  eta: 0:16:52  lr: 0.000007  min_lr: 0.000007  loss: 1.8457 (1.8504)  loss_scale: 8192.0000 (10358.7666)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0267  max mem: 13008
Epoch: [7]  [2130/3542]  eta: 0:16:44  lr: 0.000007  min_lr: 0.000007  loss: 1.8184 (1.8503)  loss_scale: 8192.0000 (10348.5988)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0239  max mem: 13008
Epoch: [7]  [2140/3542]  eta: 0:16:36  lr: 0.000007  min_lr: 0.000007  loss: 1.8145 (1.8500)  loss_scale: 8192.0000 (10338.5259)  weight_decay: 0.0500 (0.0500)  time: 0.6117  data: 0.0263  max mem: 13008
Epoch: [7]  [2150/3542]  eta: 0:16:28  lr: 0.000007  min_lr: 0.000007  loss: 1.7871 (1.8496)  loss_scale: 8192.0000 (10328.5467)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0158  max mem: 13008
Epoch: [7]  [2160/3542]  eta: 0:16:20  lr: 0.000007  min_lr: 0.000007  loss: 1.7646 (1.8493)  loss_scale: 8192.0000 (10318.6599)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0119  max mem: 13008
Epoch: [7]  [2170/3542]  eta: 0:16:13  lr: 0.000007  min_lr: 0.000007  loss: 1.7461 (1.8493)  loss_scale: 8192.0000 (10308.8641)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0273  max mem: 13008
Epoch: [7]  [2180/3542]  eta: 0:16:05  lr: 0.000007  min_lr: 0.000007  loss: 1.8555 (1.8497)  loss_scale: 8192.0000 (10299.1582)  weight_decay: 0.0500 (0.0500)  time: 0.6067  data: 0.0206  max mem: 13008
Epoch: [7]  [2190/3542]  eta: 0:15:57  lr: 0.000007  min_lr: 0.000007  loss: 1.8525 (1.8499)  loss_scale: 8192.0000 (10289.5408)  weight_decay: 0.0500 (0.0500)  time: 0.5921  data: 0.0066  max mem: 13008
Epoch: [7]  [2200/3542]  eta: 0:15:49  lr: 0.000007  min_lr: 0.000007  loss: 1.8477 (1.8499)  loss_scale: 8192.0000 (10280.0109)  weight_decay: 0.0500 (0.0500)  time: 0.6090  data: 0.0236  max mem: 13008
[2023-05-16 12:16:26,730] [INFO] [logging.py:60:log_dist] [Rank 0] step=27000, skipped=25, lr=[7.338393297696104e-06, 7.338393297696104e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 12:16:26,733] [INFO] [timer.py:157:stop] 0/27000, SamplesPerSec=55.54844347158577
Epoch: [7]  [2210/3542]  eta: 0:15:42  lr: 0.000007  min_lr: 0.000007  loss: 1.8643 (1.8500)  loss_scale: 8192.0000 (10270.5672)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0356  max mem: 13008
Epoch: [7]  [2220/3542]  eta: 0:15:34  lr: 0.000007  min_lr: 0.000007  loss: 1.8506 (1.8500)  loss_scale: 8192.0000 (10261.2085)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0242  max mem: 13008
Epoch: [7]  [2230/3542]  eta: 0:15:27  lr: 0.000007  min_lr: 0.000007  loss: 1.8457 (1.8501)  loss_scale: 8192.0000 (10251.9337)  weight_decay: 0.0500 (0.0500)  time: 0.6585  data: 0.0734  max mem: 13008
Epoch: [7]  [2240/3542]  eta: 0:15:20  lr: 0.000007  min_lr: 0.000007  loss: 1.8457 (1.8499)  loss_scale: 8192.0000 (10242.7416)  weight_decay: 0.0500 (0.0500)  time: 0.7058  data: 0.1211  max mem: 13008
Epoch: [7]  [2250/3542]  eta: 0:15:13  lr: 0.000007  min_lr: 0.000007  loss: 1.8145 (1.8502)  loss_scale: 8192.0000 (10233.6313)  weight_decay: 0.0500 (0.0500)  time: 0.7056  data: 0.1197  max mem: 13008
Epoch: [7]  [2260/3542]  eta: 0:15:06  lr: 0.000007  min_lr: 0.000007  loss: 1.8252 (1.8501)  loss_scale: 8192.0000 (10224.6015)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.1013  max mem: 13008
Epoch: [7]  [2270/3542]  eta: 0:14:58  lr: 0.000007  min_lr: 0.000007  loss: 1.8252 (1.8501)  loss_scale: 8192.0000 (10215.6513)  weight_decay: 0.0500 (0.0500)  time: 0.6479  data: 0.0622  max mem: 13008
Epoch: [7]  [2280/3542]  eta: 0:14:51  lr: 0.000007  min_lr: 0.000007  loss: 1.8730 (1.8506)  loss_scale: 8192.0000 (10206.7795)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.0908  max mem: 13008
Epoch: [7]  [2290/3542]  eta: 0:14:44  lr: 0.000007  min_lr: 0.000007  loss: 1.8740 (1.8505)  loss_scale: 8192.0000 (10197.9852)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.0780  max mem: 13008
Epoch: [7]  [2300/3542]  eta: 0:14:36  lr: 0.000007  min_lr: 0.000007  loss: 1.8926 (1.8506)  loss_scale: 8192.0000 (10189.2673)  weight_decay: 0.0500 (0.0500)  time: 0.6166  data: 0.0317  max mem: 13008
[2023-05-16 12:17:35,237] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 12:17:35,237] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [7]  [2310/3542]  eta: 0:14:29  lr: 0.000007  min_lr: 0.000007  loss: 1.8330 (1.8504)  loss_scale: 8192.0000 (10191.2592)  weight_decay: 0.0500 (0.0500)  time: 0.6617  data: 0.0767  max mem: 13008
Epoch: [7]  [2320/3542]  eta: 0:14:22  lr: 0.000007  min_lr: 0.000007  loss: 1.8086 (1.8502)  loss_scale: 16384.0000 (10217.9405)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.0854  max mem: 13008
Epoch: [7]  [2330/3542]  eta: 0:14:14  lr: 0.000007  min_lr: 0.000007  loss: 1.8105 (1.8501)  loss_scale: 16384.0000 (10244.3930)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.0344  max mem: 13008
Epoch: [7]  [2340/3542]  eta: 0:14:07  lr: 0.000007  min_lr: 0.000007  loss: 1.8662 (1.8502)  loss_scale: 16384.0000 (10270.6194)  weight_decay: 0.0500 (0.0500)  time: 0.6283  data: 0.0431  max mem: 13008
Epoch: [7]  [2350/3542]  eta: 0:13:59  lr: 0.000007  min_lr: 0.000007  loss: 1.8359 (1.8501)  loss_scale: 16384.0000 (10296.6227)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.0611  max mem: 13008
Epoch: [7]  [2360/3542]  eta: 0:13:52  lr: 0.000007  min_lr: 0.000007  loss: 1.7988 (1.8499)  loss_scale: 16384.0000 (10322.4058)  weight_decay: 0.0500 (0.0500)  time: 0.6475  data: 0.0636  max mem: 13008
Epoch: [7]  [2370/3542]  eta: 0:13:44  lr: 0.000007  min_lr: 0.000007  loss: 1.7988 (1.8498)  loss_scale: 16384.0000 (10347.9713)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.0427  max mem: 13008
Epoch: [7]  [2380/3542]  eta: 0:13:37  lr: 0.000007  min_lr: 0.000007  loss: 1.8145 (1.8497)  loss_scale: 16384.0000 (10373.3221)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0176  max mem: 13008
Epoch: [7]  [2390/3542]  eta: 0:13:29  lr: 0.000007  min_lr: 0.000007  loss: 1.8525 (1.8497)  loss_scale: 16384.0000 (10398.4609)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0254  max mem: 13008
Epoch: [7]  [2400/3542]  eta: 0:13:22  lr: 0.000007  min_lr: 0.000007  loss: 1.8525 (1.8498)  loss_scale: 16384.0000 (10423.3903)  weight_decay: 0.0500 (0.0500)  time: 0.6520  data: 0.0672  max mem: 13008
Epoch: [7]  [2410/3542]  eta: 0:13:15  lr: 0.000007  min_lr: 0.000007  loss: 1.8486 (1.8497)  loss_scale: 16384.0000 (10448.1128)  weight_decay: 0.0500 (0.0500)  time: 0.7158  data: 0.1272  max mem: 13008
Epoch: [7]  [2420/3542]  eta: 0:13:08  lr: 0.000007  min_lr: 0.000007  loss: 1.8486 (1.8498)  loss_scale: 16384.0000 (10472.6311)  weight_decay: 0.0500 (0.0500)  time: 0.6624  data: 0.0734  max mem: 13008
Epoch: [7]  [2430/3542]  eta: 0:13:00  lr: 0.000007  min_lr: 0.000007  loss: 1.8740 (1.8501)  loss_scale: 16384.0000 (10496.9478)  weight_decay: 0.0500 (0.0500)  time: 0.5864  data: 0.0003  max mem: 13008
Epoch: [7]  [2440/3542]  eta: 0:12:53  lr: 0.000007  min_lr: 0.000007  loss: 1.8564 (1.8501)  loss_scale: 16384.0000 (10521.0651)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0003  max mem: 13008
Epoch: [7]  [2450/3542]  eta: 0:12:45  lr: 0.000007  min_lr: 0.000007  loss: 1.9180 (1.8504)  loss_scale: 16384.0000 (10544.9857)  weight_decay: 0.0500 (0.0500)  time: 0.5856  data: 0.0003  max mem: 13008
Epoch: [7]  [2460/3542]  eta: 0:12:38  lr: 0.000007  min_lr: 0.000007  loss: 1.9512 (1.8507)  loss_scale: 16384.0000 (10568.7119)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0003  max mem: 13008
Epoch: [7]  [2470/3542]  eta: 0:12:30  lr: 0.000007  min_lr: 0.000007  loss: 1.8428 (1.8507)  loss_scale: 16384.0000 (10592.2461)  weight_decay: 0.0500 (0.0500)  time: 0.6045  data: 0.0188  max mem: 13008
Epoch: [7]  [2480/3542]  eta: 0:12:23  lr: 0.000007  min_lr: 0.000007  loss: 1.8125 (1.8506)  loss_scale: 16384.0000 (10615.5905)  weight_decay: 0.0500 (0.0500)  time: 0.6596  data: 0.0746  max mem: 13008
Epoch: [7]  [2490/3542]  eta: 0:12:16  lr: 0.000007  min_lr: 0.000007  loss: 1.8242 (1.8507)  loss_scale: 16384.0000 (10638.7475)  weight_decay: 0.0500 (0.0500)  time: 0.6781  data: 0.0930  max mem: 13008
Epoch: [7]  [2500/3542]  eta: 0:12:09  lr: 0.000007  min_lr: 0.000007  loss: 1.9199 (1.8509)  loss_scale: 16384.0000 (10661.7193)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.0566  max mem: 13008
Epoch: [7]  [2510/3542]  eta: 0:12:01  lr: 0.000007  min_lr: 0.000007  loss: 1.9072 (1.8509)  loss_scale: 16384.0000 (10684.5082)  weight_decay: 0.0500 (0.0500)  time: 0.6076  data: 0.0221  max mem: 13008
Epoch: [7]  [2520/3542]  eta: 0:11:54  lr: 0.000007  min_lr: 0.000007  loss: 1.8779 (1.8512)  loss_scale: 16384.0000 (10707.1162)  weight_decay: 0.0500 (0.0500)  time: 0.6072  data: 0.0223  max mem: 13008
Epoch: [7]  [2530/3542]  eta: 0:11:47  lr: 0.000007  min_lr: 0.000007  loss: 1.7842 (1.8508)  loss_scale: 16384.0000 (10729.5456)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0469  max mem: 13008
Epoch: [7]  [2540/3542]  eta: 0:11:40  lr: 0.000007  min_lr: 0.000007  loss: 1.7979 (1.8507)  loss_scale: 16384.0000 (10751.7985)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0301  max mem: 13008
Epoch: [7]  [2550/3542]  eta: 0:11:32  lr: 0.000007  min_lr: 0.000007  loss: 1.8623 (1.8508)  loss_scale: 16384.0000 (10773.8769)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.0482  max mem: 13008
Epoch: [7]  [2560/3542]  eta: 0:11:25  lr: 0.000007  min_lr: 0.000007  loss: 1.8994 (1.8508)  loss_scale: 16384.0000 (10795.7829)  weight_decay: 0.0500 (0.0500)  time: 0.6301  data: 0.0452  max mem: 13008
Epoch: [7]  [2570/3542]  eta: 0:11:18  lr: 0.000007  min_lr: 0.000007  loss: 1.8877 (1.8511)  loss_scale: 16384.0000 (10817.5185)  weight_decay: 0.0500 (0.0500)  time: 0.5932  data: 0.0082  max mem: 13008
Epoch: [7]  [2580/3542]  eta: 0:11:10  lr: 0.000007  min_lr: 0.000007  loss: 1.9111 (1.8511)  loss_scale: 16384.0000 (10839.0856)  weight_decay: 0.0500 (0.0500)  time: 0.5999  data: 0.0146  max mem: 13008
Epoch: [7]  [2590/3542]  eta: 0:11:03  lr: 0.000007  min_lr: 0.000007  loss: 1.8203 (1.8509)  loss_scale: 16384.0000 (10860.4863)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.0395  max mem: 13008
Epoch: [7]  [2600/3542]  eta: 0:10:56  lr: 0.000007  min_lr: 0.000007  loss: 1.8320 (1.8510)  loss_scale: 16384.0000 (10881.7224)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.0506  max mem: 13008
Epoch: [7]  [2610/3542]  eta: 0:10:49  lr: 0.000007  min_lr: 0.000007  loss: 1.8613 (1.8510)  loss_scale: 16384.0000 (10902.7959)  weight_decay: 0.0500 (0.0500)  time: 0.6222  data: 0.0369  max mem: 13008
Epoch: [7]  [2620/3542]  eta: 0:10:41  lr: 0.000007  min_lr: 0.000007  loss: 1.8486 (1.8510)  loss_scale: 16384.0000 (10923.7085)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0344  max mem: 13008
Epoch: [7]  [2630/3542]  eta: 0:10:34  lr: 0.000007  min_lr: 0.000007  loss: 1.8311 (1.8508)  loss_scale: 16384.0000 (10944.4622)  weight_decay: 0.0500 (0.0500)  time: 0.6205  data: 0.0359  max mem: 13008
Epoch: [7]  [2640/3542]  eta: 0:10:27  lr: 0.000007  min_lr: 0.000007  loss: 1.8242 (1.8508)  loss_scale: 16384.0000 (10965.0587)  weight_decay: 0.0500 (0.0500)  time: 0.6061  data: 0.0210  max mem: 13008
Epoch: [7]  [2650/3542]  eta: 0:10:20  lr: 0.000007  min_lr: 0.000007  loss: 1.8008 (1.8506)  loss_scale: 16384.0000 (10985.4998)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0092  max mem: 13008
Epoch: [7]  [2660/3542]  eta: 0:10:12  lr: 0.000007  min_lr: 0.000007  loss: 1.7832 (1.8506)  loss_scale: 16384.0000 (11005.7873)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0170  max mem: 13008
Epoch: [7]  [2670/3542]  eta: 0:10:05  lr: 0.000007  min_lr: 0.000007  loss: 1.8545 (1.8509)  loss_scale: 16384.0000 (11025.9229)  weight_decay: 0.0500 (0.0500)  time: 0.6050  data: 0.0189  max mem: 13008
Epoch: [7]  [2680/3542]  eta: 0:09:58  lr: 0.000007  min_lr: 0.000007  loss: 1.8760 (1.8509)  loss_scale: 16384.0000 (11045.9082)  weight_decay: 0.0500 (0.0500)  time: 0.5968  data: 0.0110  max mem: 13008
Epoch: [7]  [2690/3542]  eta: 0:09:51  lr: 0.000007  min_lr: 0.000007  loss: 1.8760 (1.8511)  loss_scale: 16384.0000 (11065.7451)  weight_decay: 0.0500 (0.0500)  time: 0.5862  data: 0.0004  max mem: 13008
Epoch: [7]  [2700/3542]  eta: 0:09:43  lr: 0.000007  min_lr: 0.000007  loss: 1.8193 (1.8508)  loss_scale: 16384.0000 (11085.4350)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0003  max mem: 13008
Epoch: [7]  [2710/3542]  eta: 0:09:36  lr: 0.000007  min_lr: 0.000007  loss: 1.7754 (1.8508)  loss_scale: 16384.0000 (11104.9797)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0301  max mem: 13008
Epoch: [7]  [2720/3542]  eta: 0:09:29  lr: 0.000007  min_lr: 0.000007  loss: 1.8555 (1.8506)  loss_scale: 16384.0000 (11124.3807)  weight_decay: 0.0500 (0.0500)  time: 0.6472  data: 0.0621  max mem: 13008
Epoch: [7]  [2730/3542]  eta: 0:09:22  lr: 0.000007  min_lr: 0.000007  loss: 1.7959 (1.8507)  loss_scale: 16384.0000 (11143.6397)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0546  max mem: 13008
Epoch: [7]  [2740/3542]  eta: 0:09:15  lr: 0.000007  min_lr: 0.000007  loss: 1.7969 (1.8510)  loss_scale: 16384.0000 (11162.7581)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0431  max mem: 13008
Epoch: [7]  [2750/3542]  eta: 0:09:08  lr: 0.000007  min_lr: 0.000007  loss: 1.8672 (1.8510)  loss_scale: 16384.0000 (11181.7375)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0211  max mem: 13008
Epoch: [7]  [2760/3542]  eta: 0:09:01  lr: 0.000007  min_lr: 0.000007  loss: 1.8545 (1.8511)  loss_scale: 16384.0000 (11200.5795)  weight_decay: 0.0500 (0.0500)  time: 0.6114  data: 0.0264  max mem: 13008
Epoch: [7]  [2770/3542]  eta: 0:08:54  lr: 0.000007  min_lr: 0.000007  loss: 1.8369 (1.8513)  loss_scale: 16384.0000 (11219.2855)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.0520  max mem: 13008
Epoch: [7]  [2780/3542]  eta: 0:08:46  lr: 0.000007  min_lr: 0.000007  loss: 1.9277 (1.8515)  loss_scale: 16384.0000 (11237.8569)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.0587  max mem: 13008
Epoch: [7]  [2790/3542]  eta: 0:08:39  lr: 0.000007  min_lr: 0.000007  loss: 1.8916 (1.8517)  loss_scale: 16384.0000 (11256.2952)  weight_decay: 0.0500 (0.0500)  time: 0.6558  data: 0.0705  max mem: 13008
Epoch: [7]  [2800/3542]  eta: 0:08:33  lr: 0.000007  min_lr: 0.000007  loss: 1.8613 (1.8514)  loss_scale: 16384.0000 (11274.6019)  weight_decay: 0.0500 (0.0500)  time: 0.6757  data: 0.0912  max mem: 13008
Epoch: [7]  [2810/3542]  eta: 0:08:26  lr: 0.000007  min_lr: 0.000007  loss: 1.8350 (1.8514)  loss_scale: 16384.0000 (11292.7784)  weight_decay: 0.0500 (0.0500)  time: 0.7075  data: 0.1226  max mem: 13008
Epoch: [7]  [2820/3542]  eta: 0:08:19  lr: 0.000006  min_lr: 0.000006  loss: 1.8125 (1.8514)  loss_scale: 16384.0000 (11310.8259)  weight_decay: 0.0500 (0.0500)  time: 0.7297  data: 0.1446  max mem: 13008
Epoch: [7]  [2830/3542]  eta: 0:08:12  lr: 0.000006  min_lr: 0.000006  loss: 1.8281 (1.8515)  loss_scale: 16384.0000 (11328.7460)  weight_decay: 0.0500 (0.0500)  time: 0.7706  data: 0.1862  max mem: 13008
[2023-05-16 12:23:12,812] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 27633
[2023-05-16 12:23:12,812] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 12:23:12,830] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [7]  [2840/3542]  eta: 0:08:06  lr: 0.000006  min_lr: 0.000006  loss: 1.8311 (1.8515)  loss_scale: 16384.0000 (11340.7730)  weight_decay: 0.0500 (0.0500)  time: 0.8239  data: 0.2398  max mem: 13008
Epoch: [7]  [2850/3542]  eta: 0:07:59  lr: 0.000006  min_lr: 0.000006  loss: 1.7930 (1.8512)  loss_scale: 8192.0000 (11329.7285)  weight_decay: 0.0500 (0.0500)  time: 0.8270  data: 0.2428  max mem: 13008
Epoch: [7]  [2860/3542]  eta: 0:07:52  lr: 0.000006  min_lr: 0.000006  loss: 1.7842 (1.8511)  loss_scale: 8192.0000 (11318.7613)  weight_decay: 0.0500 (0.0500)  time: 0.8057  data: 0.2218  max mem: 13008
Epoch: [7]  [2870/3542]  eta: 0:07:46  lr: 0.000006  min_lr: 0.000006  loss: 1.7842 (1.8509)  loss_scale: 8192.0000 (11307.8704)  weight_decay: 0.0500 (0.0500)  time: 0.8449  data: 0.2593  max mem: 13008
Epoch: [7]  [2880/3542]  eta: 0:07:39  lr: 0.000006  min_lr: 0.000006  loss: 1.7910 (1.8508)  loss_scale: 8192.0000 (11297.0552)  weight_decay: 0.0500 (0.0500)  time: 0.8549  data: 0.2696  max mem: 13008
Epoch: [7]  [2890/3542]  eta: 0:07:33  lr: 0.000006  min_lr: 0.000006  loss: 1.7930 (1.8506)  loss_scale: 8192.0000 (11286.3148)  weight_decay: 0.0500 (0.0500)  time: 0.8251  data: 0.2417  max mem: 13008
Epoch: [7]  [2900/3542]  eta: 0:07:26  lr: 0.000006  min_lr: 0.000006  loss: 1.8594 (1.8507)  loss_scale: 8192.0000 (11275.6484)  weight_decay: 0.0500 (0.0500)  time: 0.8491  data: 0.2650  max mem: 13008
Epoch: [7]  [2910/3542]  eta: 0:07:20  lr: 0.000006  min_lr: 0.000006  loss: 1.8984 (1.8508)  loss_scale: 8192.0000 (11265.0553)  weight_decay: 0.0500 (0.0500)  time: 0.8638  data: 0.2800  max mem: 13008
Epoch: [7]  [2920/3542]  eta: 0:07:12  lr: 0.000006  min_lr: 0.000006  loss: 1.8125 (1.8506)  loss_scale: 8192.0000 (11254.5347)  weight_decay: 0.0500 (0.0500)  time: 0.7523  data: 0.1691  max mem: 13008
Epoch: [7]  [2930/3542]  eta: 0:07:06  lr: 0.000006  min_lr: 0.000006  loss: 1.7422 (1.8503)  loss_scale: 8192.0000 (11244.0860)  weight_decay: 0.0500 (0.0500)  time: 0.7069  data: 0.1238  max mem: 13008
Epoch: [7]  [2940/3542]  eta: 0:06:59  lr: 0.000006  min_lr: 0.000006  loss: 1.8105 (1.8505)  loss_scale: 8192.0000 (11233.7083)  weight_decay: 0.0500 (0.0500)  time: 0.7626  data: 0.1797  max mem: 13008
Epoch: [7]  [2950/3542]  eta: 0:06:52  lr: 0.000006  min_lr: 0.000006  loss: 1.8311 (1.8504)  loss_scale: 8192.0000 (11223.4009)  weight_decay: 0.0500 (0.0500)  time: 0.6786  data: 0.0948  max mem: 13008
Epoch: [7]  [2960/3542]  eta: 0:06:45  lr: 0.000006  min_lr: 0.000006  loss: 1.8516 (1.8508)  loss_scale: 8192.0000 (11213.1631)  weight_decay: 0.0500 (0.0500)  time: 0.6469  data: 0.0618  max mem: 13008
Epoch: [7]  [2970/3542]  eta: 0:06:38  lr: 0.000006  min_lr: 0.000006  loss: 1.8789 (1.8510)  loss_scale: 8192.0000 (11202.9943)  weight_decay: 0.0500 (0.0500)  time: 0.6914  data: 0.1065  max mem: 13008
Epoch: [7]  [2980/3542]  eta: 0:06:31  lr: 0.000006  min_lr: 0.000006  loss: 1.8613 (1.8510)  loss_scale: 8192.0000 (11192.8937)  weight_decay: 0.0500 (0.0500)  time: 0.6816  data: 0.0972  max mem: 13008
Epoch: [7]  [2990/3542]  eta: 0:06:24  lr: 0.000006  min_lr: 0.000006  loss: 1.8291 (1.8510)  loss_scale: 8192.0000 (11182.8606)  weight_decay: 0.0500 (0.0500)  time: 0.6664  data: 0.0821  max mem: 13008
Epoch: [7]  [3000/3542]  eta: 0:06:17  lr: 0.000006  min_lr: 0.000006  loss: 1.8291 (1.8509)  loss_scale: 8192.0000 (11172.8944)  weight_decay: 0.0500 (0.0500)  time: 0.6697  data: 0.0856  max mem: 13008
Epoch: [7]  [3010/3542]  eta: 0:06:10  lr: 0.000006  min_lr: 0.000006  loss: 1.8486 (1.8509)  loss_scale: 8192.0000 (11162.9944)  weight_decay: 0.0500 (0.0500)  time: 0.6655  data: 0.0813  max mem: 13008
Epoch: [7]  [3020/3542]  eta: 0:06:03  lr: 0.000006  min_lr: 0.000006  loss: 1.8809 (1.8512)  loss_scale: 8192.0000 (11153.1599)  weight_decay: 0.0500 (0.0500)  time: 0.6235  data: 0.0390  max mem: 13008
Epoch: [7]  [3030/3542]  eta: 0:05:55  lr: 0.000006  min_lr: 0.000006  loss: 1.8721 (1.8512)  loss_scale: 8192.0000 (11143.3903)  weight_decay: 0.0500 (0.0500)  time: 0.6255  data: 0.0407  max mem: 13008
Epoch: [7]  [3040/3542]  eta: 0:05:48  lr: 0.000006  min_lr: 0.000006  loss: 1.8096 (1.8509)  loss_scale: 8192.0000 (11133.6850)  weight_decay: 0.0500 (0.0500)  time: 0.6354  data: 0.0513  max mem: 13008
Epoch: [7]  [3050/3542]  eta: 0:05:41  lr: 0.000006  min_lr: 0.000006  loss: 1.8096 (1.8509)  loss_scale: 8192.0000 (11124.0433)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0475  max mem: 13008
Epoch: [7]  [3060/3542]  eta: 0:05:34  lr: 0.000006  min_lr: 0.000006  loss: 1.8496 (1.8510)  loss_scale: 8192.0000 (11114.4646)  weight_decay: 0.0500 (0.0500)  time: 0.6598  data: 0.0748  max mem: 13008
Epoch: [7]  [3070/3542]  eta: 0:05:27  lr: 0.000006  min_lr: 0.000006  loss: 1.8604 (1.8511)  loss_scale: 8192.0000 (11104.9482)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.0514  max mem: 13008
Epoch: [7]  [3080/3542]  eta: 0:05:20  lr: 0.000006  min_lr: 0.000006  loss: 1.8340 (1.8510)  loss_scale: 8192.0000 (11095.4937)  weight_decay: 0.0500 (0.0500)  time: 0.6187  data: 0.0330  max mem: 13008
Epoch: [7]  [3090/3542]  eta: 0:05:13  lr: 0.000006  min_lr: 0.000006  loss: 1.8076 (1.8510)  loss_scale: 8192.0000 (11086.1003)  weight_decay: 0.0500 (0.0500)  time: 0.6238  data: 0.0383  max mem: 13008
Epoch: [7]  [3100/3542]  eta: 0:05:06  lr: 0.000006  min_lr: 0.000006  loss: 1.8281 (1.8511)  loss_scale: 8192.0000 (11076.7675)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.0446  max mem: 13008
Epoch: [7]  [3110/3542]  eta: 0:04:59  lr: 0.000006  min_lr: 0.000006  loss: 1.8242 (1.8511)  loss_scale: 8192.0000 (11067.4947)  weight_decay: 0.0500 (0.0500)  time: 0.6326  data: 0.0475  max mem: 13008
Epoch: [7]  [3120/3542]  eta: 0:04:52  lr: 0.000006  min_lr: 0.000006  loss: 1.8154 (1.8511)  loss_scale: 8192.0000 (11058.2813)  weight_decay: 0.0500 (0.0500)  time: 0.6200  data: 0.0350  max mem: 13008
Epoch: [7]  [3130/3542]  eta: 0:04:45  lr: 0.000006  min_lr: 0.000006  loss: 1.8330 (1.8512)  loss_scale: 8192.0000 (11049.1268)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.0554  max mem: 13008
Epoch: [7]  [3140/3542]  eta: 0:04:38  lr: 0.000006  min_lr: 0.000006  loss: 1.7822 (1.8509)  loss_scale: 8192.0000 (11040.0306)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.0395  max mem: 13008
Epoch: [7]  [3150/3542]  eta: 0:04:31  lr: 0.000006  min_lr: 0.000006  loss: 1.7764 (1.8509)  loss_scale: 8192.0000 (11030.9921)  weight_decay: 0.0500 (0.0500)  time: 0.6339  data: 0.0494  max mem: 13008
Epoch: [7]  [3160/3542]  eta: 0:04:24  lr: 0.000006  min_lr: 0.000006  loss: 1.8164 (1.8509)  loss_scale: 8192.0000 (11022.0108)  weight_decay: 0.0500 (0.0500)  time: 0.6319  data: 0.0471  max mem: 13008
Epoch: [7]  [3170/3542]  eta: 0:04:17  lr: 0.000006  min_lr: 0.000006  loss: 1.8164 (1.8510)  loss_scale: 8192.0000 (11013.0861)  weight_decay: 0.0500 (0.0500)  time: 0.6013  data: 0.0153  max mem: 13008
Epoch: [7]  [3180/3542]  eta: 0:04:10  lr: 0.000006  min_lr: 0.000006  loss: 1.7891 (1.8508)  loss_scale: 8192.0000 (11004.2175)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0344  max mem: 13008
Epoch: [7]  [3190/3542]  eta: 0:04:03  lr: 0.000006  min_lr: 0.000006  loss: 1.8799 (1.8510)  loss_scale: 8192.0000 (10995.4046)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0194  max mem: 13008
Epoch: [7]  [3200/3542]  eta: 0:03:56  lr: 0.000006  min_lr: 0.000006  loss: 1.9111 (1.8509)  loss_scale: 8192.0000 (10986.6467)  weight_decay: 0.0500 (0.0500)  time: 0.6108  data: 0.0232  max mem: 13008
[2023-05-16 12:27:22,626] [INFO] [logging.py:60:log_dist] [Rank 0] step=28000, skipped=26, lr=[5.986579399270715e-06, 5.986579399270715e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 12:27:22,629] [INFO] [timer.py:157:stop] 0/28000, SamplesPerSec=55.54978208262028
Epoch: [7]  [3210/3542]  eta: 0:03:49  lr: 0.000006  min_lr: 0.000006  loss: 1.8135 (1.8507)  loss_scale: 8192.0000 (10977.9433)  weight_decay: 0.0500 (0.0500)  time: 0.6239  data: 0.0373  max mem: 13008
Epoch: [7]  [3220/3542]  eta: 0:03:42  lr: 0.000006  min_lr: 0.000006  loss: 1.8242 (1.8509)  loss_scale: 8192.0000 (10969.2940)  weight_decay: 0.0500 (0.0500)  time: 0.5996  data: 0.0143  max mem: 13008
Epoch: [7]  [3230/3542]  eta: 0:03:35  lr: 0.000006  min_lr: 0.000006  loss: 1.8799 (1.8510)  loss_scale: 8192.0000 (10960.6982)  weight_decay: 0.0500 (0.0500)  time: 0.5852  data: 0.0003  max mem: 13008
Epoch: [7]  [3240/3542]  eta: 0:03:28  lr: 0.000006  min_lr: 0.000006  loss: 1.8076 (1.8508)  loss_scale: 8192.0000 (10952.1555)  weight_decay: 0.0500 (0.0500)  time: 0.6173  data: 0.0324  max mem: 13008
Epoch: [7]  [3250/3542]  eta: 0:03:21  lr: 0.000006  min_lr: 0.000006  loss: 1.9033 (1.8511)  loss_scale: 8192.0000 (10943.6653)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.0513  max mem: 13008
Epoch: [7]  [3260/3542]  eta: 0:03:14  lr: 0.000006  min_lr: 0.000006  loss: 1.9004 (1.8510)  loss_scale: 8192.0000 (10935.2272)  weight_decay: 0.0500 (0.0500)  time: 0.6248  data: 0.0409  max mem: 13008
Epoch: [7]  [3270/3542]  eta: 0:03:07  lr: 0.000006  min_lr: 0.000006  loss: 1.8906 (1.8513)  loss_scale: 8192.0000 (10926.8407)  weight_decay: 0.0500 (0.0500)  time: 0.6317  data: 0.0472  max mem: 13008
Epoch: [7]  [3280/3542]  eta: 0:03:00  lr: 0.000006  min_lr: 0.000006  loss: 1.8955 (1.8514)  loss_scale: 8192.0000 (10918.5053)  weight_decay: 0.0500 (0.0500)  time: 0.6400  data: 0.0537  max mem: 13008
Epoch: [7]  [3290/3542]  eta: 0:02:53  lr: 0.000006  min_lr: 0.000006  loss: 1.8057 (1.8512)  loss_scale: 8192.0000 (10910.2206)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0319  max mem: 13008
Epoch: [7]  [3300/3542]  eta: 0:02:46  lr: 0.000006  min_lr: 0.000006  loss: 1.7803 (1.8511)  loss_scale: 8192.0000 (10901.9861)  weight_decay: 0.0500 (0.0500)  time: 0.5925  data: 0.0069  max mem: 13008
Epoch: [7]  [3310/3542]  eta: 0:02:39  lr: 0.000006  min_lr: 0.000006  loss: 1.8486 (1.8513)  loss_scale: 8192.0000 (10893.8013)  weight_decay: 0.0500 (0.0500)  time: 0.6035  data: 0.0174  max mem: 13008
Epoch: [7]  [3320/3542]  eta: 0:02:32  lr: 0.000006  min_lr: 0.000006  loss: 1.8418 (1.8511)  loss_scale: 8192.0000 (10885.6658)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.0454  max mem: 13008
Epoch: [7]  [3330/3542]  eta: 0:02:26  lr: 0.000006  min_lr: 0.000006  loss: 1.7393 (1.8508)  loss_scale: 8192.0000 (10877.5791)  weight_decay: 0.0500 (0.0500)  time: 0.6300  data: 0.0447  max mem: 13008
Epoch: [7]  [3340/3542]  eta: 0:02:19  lr: 0.000006  min_lr: 0.000006  loss: 1.8066 (1.8508)  loss_scale: 8192.0000 (10869.5409)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0133  max mem: 13008
Epoch: [7]  [3350/3542]  eta: 0:02:12  lr: 0.000006  min_lr: 0.000006  loss: 1.8633 (1.8510)  loss_scale: 8192.0000 (10861.5506)  weight_decay: 0.0500 (0.0500)  time: 0.5949  data: 0.0097  max mem: 13008
Epoch: [7]  [3360/3542]  eta: 0:02:05  lr: 0.000006  min_lr: 0.000006  loss: 1.8633 (1.8509)  loss_scale: 8192.0000 (10853.6079)  weight_decay: 0.0500 (0.0500)  time: 0.6236  data: 0.0387  max mem: 13008
Epoch: [7]  [3370/3542]  eta: 0:01:58  lr: 0.000006  min_lr: 0.000006  loss: 1.8828 (1.8514)  loss_scale: 8192.0000 (10845.7123)  weight_decay: 0.0500 (0.0500)  time: 0.6175  data: 0.0326  max mem: 13008
Epoch: [7]  [3380/3542]  eta: 0:01:51  lr: 0.000006  min_lr: 0.000006  loss: 1.8730 (1.8512)  loss_scale: 8192.0000 (10837.8634)  weight_decay: 0.0500 (0.0500)  time: 0.6011  data: 0.0161  max mem: 13008
Epoch: [7]  [3390/3542]  eta: 0:01:44  lr: 0.000006  min_lr: 0.000006  loss: 1.8730 (1.8514)  loss_scale: 8192.0000 (10830.0607)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0207  max mem: 13008
Epoch: [7]  [3400/3542]  eta: 0:01:37  lr: 0.000006  min_lr: 0.000006  loss: 1.8848 (1.8516)  loss_scale: 8192.0000 (10822.3040)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.0359  max mem: 13008
Epoch: [7]  [3410/3542]  eta: 0:01:30  lr: 0.000006  min_lr: 0.000006  loss: 1.8945 (1.8518)  loss_scale: 8192.0000 (10814.5928)  weight_decay: 0.0500 (0.0500)  time: 0.6413  data: 0.0552  max mem: 13008
Epoch: [7]  [3420/3542]  eta: 0:01:23  lr: 0.000006  min_lr: 0.000006  loss: 1.7832 (1.8514)  loss_scale: 8192.0000 (10806.9266)  weight_decay: 0.0500 (0.0500)  time: 0.6182  data: 0.0318  max mem: 13008
Epoch: [7]  [3430/3542]  eta: 0:01:16  lr: 0.000006  min_lr: 0.000006  loss: 1.8291 (1.8514)  loss_scale: 8192.0000 (10799.3052)  weight_decay: 0.0500 (0.0500)  time: 0.6088  data: 0.0231  max mem: 13008
Epoch: [7]  [3440/3542]  eta: 0:01:09  lr: 0.000006  min_lr: 0.000006  loss: 1.8613 (1.8515)  loss_scale: 8192.0000 (10791.7280)  weight_decay: 0.0500 (0.0500)  time: 0.6087  data: 0.0233  max mem: 13008
Epoch: [7]  [3450/3542]  eta: 0:01:03  lr: 0.000006  min_lr: 0.000006  loss: 1.7686 (1.8513)  loss_scale: 8192.0000 (10784.1947)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0082  max mem: 13008
Epoch: [7]  [3460/3542]  eta: 0:00:56  lr: 0.000006  min_lr: 0.000006  loss: 1.7598 (1.8513)  loss_scale: 8192.0000 (10776.7050)  weight_decay: 0.0500 (0.0500)  time: 0.6029  data: 0.0173  max mem: 13008
Epoch: [7]  [3470/3542]  eta: 0:00:49  lr: 0.000006  min_lr: 0.000006  loss: 1.8760 (1.8514)  loss_scale: 8192.0000 (10769.2584)  weight_decay: 0.0500 (0.0500)  time: 0.6121  data: 0.0265  max mem: 13008
Epoch: [7]  [3480/3542]  eta: 0:00:42  lr: 0.000006  min_lr: 0.000006  loss: 1.8379 (1.8514)  loss_scale: 8192.0000 (10761.8546)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.0600  max mem: 13008
Epoch: [7]  [3490/3542]  eta: 0:00:35  lr: 0.000006  min_lr: 0.000006  loss: 1.8486 (1.8515)  loss_scale: 8192.0000 (10754.4933)  weight_decay: 0.0500 (0.0500)  time: 0.6625  data: 0.0769  max mem: 13008
Epoch: [7]  [3500/3542]  eta: 0:00:28  lr: 0.000006  min_lr: 0.000006  loss: 1.8262 (1.8513)  loss_scale: 8192.0000 (10747.1740)  weight_decay: 0.0500 (0.0500)  time: 0.6214  data: 0.0352  max mem: 13008
Epoch: [7]  [3510/3542]  eta: 0:00:21  lr: 0.000006  min_lr: 0.000006  loss: 1.8135 (1.8514)  loss_scale: 8192.0000 (10739.8963)  weight_decay: 0.0500 (0.0500)  time: 0.6216  data: 0.0344  max mem: 13008
Epoch: [7]  [3520/3542]  eta: 0:00:15  lr: 0.000006  min_lr: 0.000006  loss: 1.8564 (1.8514)  loss_scale: 8192.0000 (10732.6600)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.0496  max mem: 13008
Epoch: [7]  [3530/3542]  eta: 0:00:08  lr: 0.000006  min_lr: 0.000006  loss: 1.8008 (1.8512)  loss_scale: 8192.0000 (10725.4647)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.0354  max mem: 13008
Epoch: [7]  [3540/3542]  eta: 0:00:01  lr: 0.000006  min_lr: 0.000006  loss: 1.7891 (1.8513)  loss_scale: 8192.0000 (10718.3101)  weight_decay: 0.0500 (0.0500)  time: 0.5991  data: 0.0149  max mem: 13008
Epoch: [7]  [3541/3542]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000006  loss: 1.8008 (1.8513)  loss_scale: 8192.0000 (10717.5968)  weight_decay: 0.0500 (0.0500)  time: 0.5990  data: 0.0149  max mem: 13008
Epoch: [7] Total time: 0:40:25 (0.6847 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000006  loss: 1.8008 (1.8513)  loss_scale: 8192.0000 (10717.5968)  weight_decay: 0.0500 (0.0500)
[2023-05-16 12:30:50,278] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-7/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:32:39    time: 12.5599  data: 8.7834  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:10:36    time: 4.3620  data: 0.7987  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 20/156]  eta: 0:08:59    time: 3.5404  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 30/156]  eta: 0:08:05    time: 3.5678  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 40/156]  eta: 0:07:18    time: 3.5856  data: 0.0002  max mem: 13008
Test:  [ 50/156]  eta: 0:06:36    time: 3.5796  data: 0.0002  max mem: 13008
Test:  [ 60/156]  eta: 0:05:53    time: 3.4876  data: 0.0002  max mem: 13008
Test:  [ 70/156]  eta: 0:05:17    time: 3.5477  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:38    time: 3.5781  data: 0.0002  max mem: 13008
Test:  [ 90/156]  eta: 0:04:01    time: 3.5242  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:24    time: 3.6017  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:47    time: 3.6060  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:11    time: 3.6150  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:34    time: 3.6311  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:00:58    time: 3.5624  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:21    time: 3.5452  data: 0.0001  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.5628  data: 0.0001  max mem: 13008
Test: Total time: 0:09:26 (3.6306 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e7.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e7.json and result file is ./output_freeze/coco_captioning_result_val_e7.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e7.json
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1286924.16 tokens per second.
PTBTokenizer tokenized 62579 tokens at 488559.24 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 51331, 'reflen': 49384, 'guess': [51331, 46339, 41347, 36355], 'correct': [33817, 17458, 7896, 3265]}
ratio: 1.0394257249311307
Bleu_1: 0.659
Bleu_2: 0.498
Bleu_3: 0.362
Bleu_4: 0.255
computing METEOR score...
METEOR: 0.241
computing Rouge score...
ROUGE_L: 0.510
computing CIDEr score...
CIDEr: 0.830
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].
Threads( StanfordCoreNLP ) [16.93 seconds]
SPICE evaluation took: 27.16 s
SPICE: 0.178
Performance of the network on the 5000 val images: 0.8%
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 12:41:05,572] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-best/mp_rank_00_model_states.pt
Max performance: 0.83%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [8]  [   0/3542]  eta: 21:08:28  lr: 0.000006  min_lr: 0.000006  loss: 1.9531 (1.9531)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 21.4873  data: 20.8547  max mem: 13008
Epoch: [8]  [  10/3542]  eta: 3:53:15  lr: 0.000006  min_lr: 0.000006  loss: 1.8799 (1.9276)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 3.9625  data: 3.3744  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [8]  [  20/3542]  eta: 2:51:42  lr: 0.000006  min_lr: 0.000006  loss: 1.8799 (1.9116)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.9972  data: 1.4155  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [8]  [  30/3542]  eta: 2:23:40  lr: 0.000006  min_lr: 0.000006  loss: 1.8516 (1.8671)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6251  data: 1.0428  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [8]  [  40/3542]  eta: 2:06:36  lr: 0.000006  min_lr: 0.000006  loss: 1.7822 (1.8563)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.3754  data: 0.7930  max mem: 13008
Epoch: [8]  [  50/3542]  eta: 1:54:46  lr: 0.000006  min_lr: 0.000006  loss: 1.8066 (1.8443)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2243  data: 0.6442  max mem: 13008
Epoch: [8]  [  60/3542]  eta: 1:45:47  lr: 0.000005  min_lr: 0.000005  loss: 1.7520 (1.8333)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1130  data: 0.5330  max mem: 13008
Epoch: [8]  [  70/3542]  eta: 1:38:47  lr: 0.000005  min_lr: 0.000005  loss: 1.7314 (1.8253)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0319  data: 0.4486  max mem: 13008
Epoch: [8]  [  80/3542]  eta: 1:34:00  lr: 0.000005  min_lr: 0.000005  loss: 1.8428 (1.8352)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0381  data: 0.4546  max mem: 13008
Epoch: [8]  [  90/3542]  eta: 1:28:21  lr: 0.000005  min_lr: 0.000005  loss: 1.8154 (1.8275)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9275  data: 0.3465  max mem: 13008
Epoch: [8]  [ 100/3542]  eta: 1:24:07  lr: 0.000005  min_lr: 0.000005  loss: 1.7139 (1.8243)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8079  data: 0.2268  max mem: 13008
Epoch: [8]  [ 110/3542]  eta: 1:20:31  lr: 0.000005  min_lr: 0.000005  loss: 1.8291 (1.8274)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8250  data: 0.2447  max mem: 13008
Epoch: [8]  [ 120/3542]  eta: 1:17:36  lr: 0.000005  min_lr: 0.000005  loss: 1.8291 (1.8275)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8271  data: 0.2466  max mem: 13008
Epoch: [8]  [ 130/3542]  eta: 1:15:16  lr: 0.000005  min_lr: 0.000005  loss: 1.8672 (1.8358)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8570  data: 0.2748  max mem: 13008
Epoch: [8]  [ 140/3542]  eta: 1:12:26  lr: 0.000005  min_lr: 0.000005  loss: 1.8916 (1.8386)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7739  data: 0.1907  max mem: 13008
Epoch: [8]  [ 150/3542]  eta: 1:10:15  lr: 0.000005  min_lr: 0.000005  loss: 1.8896 (1.8400)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7124  data: 0.1291  max mem: 13008
Epoch: [8]  [ 160/3542]  eta: 1:08:12  lr: 0.000005  min_lr: 0.000005  loss: 1.8662 (1.8430)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7331  data: 0.1494  max mem: 13008
Epoch: [8]  [ 170/3542]  eta: 1:06:15  lr: 0.000005  min_lr: 0.000005  loss: 1.8926 (1.8458)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6980  data: 0.1146  max mem: 13008
Epoch: [8]  [ 180/3542]  eta: 1:04:42  lr: 0.000005  min_lr: 0.000005  loss: 1.9170 (1.8497)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7098  data: 0.1268  max mem: 13008
Epoch: [8]  [ 190/3542]  eta: 1:03:10  lr: 0.000005  min_lr: 0.000005  loss: 1.8613 (1.8485)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7182  data: 0.1353  max mem: 13008
Epoch: [8]  [ 200/3542]  eta: 1:01:43  lr: 0.000005  min_lr: 0.000005  loss: 1.7900 (1.8429)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6872  data: 0.1047  max mem: 13008
Epoch: [8]  [ 210/3542]  eta: 1:00:22  lr: 0.000005  min_lr: 0.000005  loss: 1.7686 (1.8418)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6710  data: 0.0880  max mem: 13008
Epoch: [8]  [ 220/3542]  eta: 0:59:12  lr: 0.000005  min_lr: 0.000005  loss: 1.8418 (1.8409)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6782  data: 0.0942  max mem: 13008
Epoch: [8]  [ 230/3542]  eta: 0:57:56  lr: 0.000005  min_lr: 0.000005  loss: 1.8379 (1.8398)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.0704  max mem: 13008
Epoch: [8]  [ 240/3542]  eta: 0:56:55  lr: 0.000005  min_lr: 0.000005  loss: 1.8184 (1.8407)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.0639  max mem: 13008
Epoch: [8]  [ 250/3542]  eta: 0:55:55  lr: 0.000005  min_lr: 0.000005  loss: 1.8184 (1.8403)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6672  data: 0.0821  max mem: 13008
Epoch: [8]  [ 260/3542]  eta: 0:55:01  lr: 0.000005  min_lr: 0.000005  loss: 1.8145 (1.8398)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6627  data: 0.0784  max mem: 13008
Epoch: [8]  [ 270/3542]  eta: 0:54:25  lr: 0.000005  min_lr: 0.000005  loss: 1.7549 (1.8374)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7297  data: 0.1454  max mem: 13008
Epoch: [8]  [ 280/3542]  eta: 0:53:42  lr: 0.000005  min_lr: 0.000005  loss: 1.8057 (1.8374)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7536  data: 0.1700  max mem: 13008
Epoch: [8]  [ 290/3542]  eta: 0:53:05  lr: 0.000005  min_lr: 0.000005  loss: 1.8672 (1.8401)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7315  data: 0.1483  max mem: 13008
[2023-05-16 12:45:58,217] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 12:45:58,217] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [8]  [ 300/3542]  eta: 0:52:51  lr: 0.000005  min_lr: 0.000005  loss: 1.8115 (1.8396)  loss_scale: 8192.0000 (8273.6478)  weight_decay: 0.0500 (0.0500)  time: 0.8432  data: 0.2601  max mem: 13008
Epoch: [8]  [ 310/3542]  eta: 0:52:46  lr: 0.000005  min_lr: 0.000005  loss: 1.8213 (1.8412)  loss_scale: 16384.0000 (8534.4309)  weight_decay: 0.0500 (0.0500)  time: 0.9822  data: 0.3968  max mem: 13008
Epoch: [8]  [ 320/3542]  eta: 0:53:09  lr: 0.000005  min_lr: 0.000005  loss: 1.8555 (1.8423)  loss_scale: 16384.0000 (8778.9657)  weight_decay: 0.0500 (0.0500)  time: 1.1625  data: 0.5771  max mem: 13008
Epoch: [8]  [ 330/3542]  eta: 0:53:19  lr: 0.000005  min_lr: 0.000005  loss: 1.8418 (1.8430)  loss_scale: 16384.0000 (9008.7251)  weight_decay: 0.0500 (0.0500)  time: 1.2484  data: 0.6665  max mem: 13008
Epoch: [8]  [ 340/3542]  eta: 0:53:17  lr: 0.000005  min_lr: 0.000005  loss: 1.8486 (1.8439)  loss_scale: 16384.0000 (9225.0088)  weight_decay: 0.0500 (0.0500)  time: 1.1414  data: 0.5600  max mem: 13008
Epoch: [8]  [ 350/3542]  eta: 0:53:16  lr: 0.000005  min_lr: 0.000005  loss: 1.8379 (1.8444)  loss_scale: 16384.0000 (9428.9687)  weight_decay: 0.0500 (0.0500)  time: 1.0892  data: 0.5065  max mem: 13008
Epoch: [8]  [ 360/3542]  eta: 0:52:55  lr: 0.000005  min_lr: 0.000005  loss: 1.8301 (1.8465)  loss_scale: 16384.0000 (9621.6288)  weight_decay: 0.0500 (0.0500)  time: 0.9875  data: 0.4047  max mem: 13008
Epoch: [8]  [ 370/3542]  eta: 0:53:01  lr: 0.000005  min_lr: 0.000005  loss: 1.8242 (1.8460)  loss_scale: 16384.0000 (9803.9030)  weight_decay: 0.0500 (0.0500)  time: 1.0349  data: 0.4533  max mem: 13008
Epoch: [8]  [ 380/3542]  eta: 0:53:11  lr: 0.000005  min_lr: 0.000005  loss: 1.8584 (1.8478)  loss_scale: 16384.0000 (9976.6089)  weight_decay: 0.0500 (0.0500)  time: 1.2108  data: 0.6290  max mem: 13008
Epoch: [8]  [ 390/3542]  eta: 0:53:04  lr: 0.000005  min_lr: 0.000005  loss: 1.8789 (1.8487)  loss_scale: 16384.0000 (10140.4808)  weight_decay: 0.0500 (0.0500)  time: 1.1420  data: 0.5604  max mem: 13008
Epoch: [8]  [ 400/3542]  eta: 0:53:16  lr: 0.000005  min_lr: 0.000005  loss: 1.8691 (1.8488)  loss_scale: 16384.0000 (10296.1796)  weight_decay: 0.0500 (0.0500)  time: 1.1731  data: 0.5899  max mem: 13008
Epoch: [8]  [ 410/3542]  eta: 0:53:28  lr: 0.000005  min_lr: 0.000005  loss: 1.8232 (1.8492)  loss_scale: 16384.0000 (10444.3017)  weight_decay: 0.0500 (0.0500)  time: 1.3057  data: 0.7226  max mem: 13008
Epoch: [8]  [ 420/3542]  eta: 0:53:36  lr: 0.000005  min_lr: 0.000005  loss: 1.8115 (1.8490)  loss_scale: 16384.0000 (10585.3872)  weight_decay: 0.0500 (0.0500)  time: 1.2856  data: 0.7047  max mem: 13008
Epoch: [8]  [ 430/3542]  eta: 0:53:18  lr: 0.000005  min_lr: 0.000005  loss: 1.8467 (1.8498)  loss_scale: 16384.0000 (10719.9258)  weight_decay: 0.0500 (0.0500)  time: 1.0947  data: 0.5135  max mem: 13008
Epoch: [8]  [ 440/3542]  eta: 0:53:32  lr: 0.000005  min_lr: 0.000005  loss: 1.8604 (1.8507)  loss_scale: 16384.0000 (10848.3628)  weight_decay: 0.0500 (0.0500)  time: 1.1501  data: 0.5669  max mem: 13008
Epoch: [8]  [ 450/3542]  eta: 0:53:13  lr: 0.000005  min_lr: 0.000005  loss: 1.7979 (1.8494)  loss_scale: 16384.0000 (10971.1042)  weight_decay: 0.0500 (0.0500)  time: 1.1393  data: 0.5568  max mem: 13008
Epoch: [8]  [ 460/3542]  eta: 0:53:27  lr: 0.000005  min_lr: 0.000005  loss: 1.7803 (1.8482)  loss_scale: 16384.0000 (11088.5206)  weight_decay: 0.0500 (0.0500)  time: 1.1525  data: 0.5715  max mem: 13008
Epoch: [8]  [ 470/3542]  eta: 0:53:34  lr: 0.000005  min_lr: 0.000005  loss: 1.8096 (1.8489)  loss_scale: 16384.0000 (11200.9512)  weight_decay: 0.0500 (0.0500)  time: 1.3526  data: 0.7717  max mem: 13008
Epoch: [8]  [ 480/3542]  eta: 0:53:46  lr: 0.000005  min_lr: 0.000005  loss: 1.9346 (1.8500)  loss_scale: 16384.0000 (11308.7069)  weight_decay: 0.0500 (0.0500)  time: 1.3537  data: 0.7706  max mem: 13008
Epoch: [8]  [ 490/3542]  eta: 0:53:52  lr: 0.000005  min_lr: 0.000005  loss: 1.8105 (1.8490)  loss_scale: 16384.0000 (11412.0733)  weight_decay: 0.0500 (0.0500)  time: 1.3575  data: 0.7745  max mem: 13008
Epoch: [8]  [ 500/3542]  eta: 0:53:47  lr: 0.000005  min_lr: 0.000005  loss: 1.7822 (1.8488)  loss_scale: 16384.0000 (11511.3134)  weight_decay: 0.0500 (0.0500)  time: 1.2362  data: 0.6558  max mem: 13008
Epoch: [8]  [ 510/3542]  eta: 0:53:44  lr: 0.000005  min_lr: 0.000005  loss: 1.7979 (1.8476)  loss_scale: 16384.0000 (11606.6693)  weight_decay: 0.0500 (0.0500)  time: 1.1706  data: 0.5898  max mem: 13008
Epoch: [8]  [ 520/3542]  eta: 0:53:28  lr: 0.000005  min_lr: 0.000005  loss: 1.7959 (1.8451)  loss_scale: 16384.0000 (11698.3647)  weight_decay: 0.0500 (0.0500)  time: 1.0813  data: 0.4983  max mem: 13008
Epoch: [8]  [ 530/3542]  eta: 0:53:13  lr: 0.000005  min_lr: 0.000005  loss: 1.7881 (1.8441)  loss_scale: 16384.0000 (11786.6064)  weight_decay: 0.0500 (0.0500)  time: 0.9778  data: 0.3946  max mem: 13008
Epoch: [8]  [ 540/3542]  eta: 0:52:52  lr: 0.000005  min_lr: 0.000005  loss: 1.7910 (1.8438)  loss_scale: 16384.0000 (11871.5860)  weight_decay: 0.0500 (0.0500)  time: 0.9295  data: 0.3479  max mem: 13008
Epoch: [8]  [ 550/3542]  eta: 0:52:37  lr: 0.000005  min_lr: 0.000005  loss: 1.7891 (1.8437)  loss_scale: 16384.0000 (11953.4809)  weight_decay: 0.0500 (0.0500)  time: 0.9270  data: 0.3104  max mem: 13008
Epoch: [8]  [ 560/3542]  eta: 0:52:24  lr: 0.000005  min_lr: 0.000005  loss: 1.7900 (1.8429)  loss_scale: 16384.0000 (12032.4563)  weight_decay: 0.0500 (0.0500)  time: 0.9921  data: 0.3757  max mem: 13008
Epoch: [8]  [ 570/3542]  eta: 0:52:11  lr: 0.000005  min_lr: 0.000005  loss: 1.7920 (1.8425)  loss_scale: 16384.0000 (12108.6655)  weight_decay: 0.0500 (0.0500)  time: 1.0107  data: 0.4265  max mem: 13008
Epoch: [8]  [ 580/3542]  eta: 0:51:59  lr: 0.000005  min_lr: 0.000005  loss: 1.8281 (1.8431)  loss_scale: 16384.0000 (12182.2513)  weight_decay: 0.0500 (0.0500)  time: 1.0099  data: 0.4252  max mem: 13008
Epoch: [8]  [ 590/3542]  eta: 0:51:47  lr: 0.000005  min_lr: 0.000005  loss: 1.8574 (1.8443)  loss_scale: 16384.0000 (12253.3469)  weight_decay: 0.0500 (0.0500)  time: 1.0183  data: 0.4368  max mem: 13008
Epoch: [8]  [ 600/3542]  eta: 0:51:33  lr: 0.000005  min_lr: 0.000005  loss: 1.8125 (1.8438)  loss_scale: 16384.0000 (12322.0765)  weight_decay: 0.0500 (0.0500)  time: 1.0079  data: 0.4270  max mem: 13008
Epoch: [8]  [ 610/3542]  eta: 0:51:27  lr: 0.000005  min_lr: 0.000005  loss: 1.8018 (1.8433)  loss_scale: 16384.0000 (12388.5565)  weight_decay: 0.0500 (0.0500)  time: 1.0694  data: 0.4875  max mem: 13008
Epoch: [8]  [ 620/3542]  eta: 0:51:10  lr: 0.000005  min_lr: 0.000005  loss: 1.8457 (1.8436)  loss_scale: 16384.0000 (12452.8953)  weight_decay: 0.0500 (0.0500)  time: 1.0259  data: 0.4418  max mem: 13008
Epoch: [8]  [ 630/3542]  eta: 0:50:58  lr: 0.000005  min_lr: 0.000005  loss: 1.8125 (1.8427)  loss_scale: 16384.0000 (12515.1949)  weight_decay: 0.0500 (0.0500)  time: 0.9665  data: 0.3828  max mem: 13008
Epoch: [8]  [ 640/3542]  eta: 0:50:41  lr: 0.000005  min_lr: 0.000005  loss: 1.7822 (1.8429)  loss_scale: 16384.0000 (12575.5507)  weight_decay: 0.0500 (0.0500)  time: 0.9670  data: 0.3857  max mem: 13008
Epoch: [8]  [ 650/3542]  eta: 0:50:27  lr: 0.000005  min_lr: 0.000005  loss: 1.8271 (1.8428)  loss_scale: 16384.0000 (12634.0522)  weight_decay: 0.0500 (0.0500)  time: 0.9395  data: 0.3581  max mem: 13008
Epoch: [8]  [ 660/3542]  eta: 0:50:10  lr: 0.000005  min_lr: 0.000005  loss: 1.8379 (1.8440)  loss_scale: 16384.0000 (12690.7837)  weight_decay: 0.0500 (0.0500)  time: 0.9330  data: 0.3513  max mem: 13008
[2023-05-16 12:52:39,461] [INFO] [logging.py:60:log_dist] [Rank 0] step=29000, skipped=26, lr=[4.775608696192697e-06, 4.775608696192697e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 12:52:39,464] [INFO] [timer.py:157:stop] 0/29000, SamplesPerSec=55.553750951953205
Epoch: [8]  [ 670/3542]  eta: 0:50:00  lr: 0.000005  min_lr: 0.000005  loss: 1.8096 (1.8443)  loss_scale: 16384.0000 (12745.8241)  weight_decay: 0.0500 (0.0500)  time: 0.9725  data: 0.3884  max mem: 13008
Epoch: [8]  [ 680/3542]  eta: 0:49:44  lr: 0.000005  min_lr: 0.000005  loss: 1.8096 (1.8440)  loss_scale: 16384.0000 (12799.2482)  weight_decay: 0.0500 (0.0500)  time: 0.9826  data: 0.3983  max mem: 13008
Epoch: [8]  [ 690/3542]  eta: 0:49:33  lr: 0.000005  min_lr: 0.000005  loss: 1.8945 (1.8456)  loss_scale: 16384.0000 (12851.1259)  weight_decay: 0.0500 (0.0500)  time: 0.9682  data: 0.3861  max mem: 13008
[2023-05-16 12:53:12,291] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 29034
[2023-05-16 12:53:12,291] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 12:53:12,291] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [8]  [ 700/3542]  eta: 0:49:08  lr: 0.000005  min_lr: 0.000005  loss: 1.9219 (1.8465)  loss_scale: 16384.0000 (12866.4650)  weight_decay: 0.0500 (0.0500)  time: 0.8593  data: 0.2776  max mem: 13008
Epoch: [8]  [ 710/3542]  eta: 0:48:51  lr: 0.000005  min_lr: 0.000005  loss: 1.8096 (1.8453)  loss_scale: 8192.0000 (12800.7201)  weight_decay: 0.0500 (0.0500)  time: 0.7853  data: 0.2034  max mem: 13008
Epoch: [8]  [ 720/3542]  eta: 0:48:40  lr: 0.000005  min_lr: 0.000005  loss: 1.7998 (1.8453)  loss_scale: 8192.0000 (12736.7989)  weight_decay: 0.0500 (0.0500)  time: 0.9393  data: 0.3551  max mem: 13008
Epoch: [8]  [ 730/3542]  eta: 0:48:26  lr: 0.000005  min_lr: 0.000005  loss: 1.8828 (1.8455)  loss_scale: 8192.0000 (12674.6265)  weight_decay: 0.0500 (0.0500)  time: 0.9776  data: 0.3938  max mem: 13008
Epoch: [8]  [ 740/3542]  eta: 0:48:12  lr: 0.000005  min_lr: 0.000005  loss: 1.8525 (1.8456)  loss_scale: 8192.0000 (12614.1323)  weight_decay: 0.0500 (0.0500)  time: 0.9420  data: 0.3600  max mem: 13008
Epoch: [8]  [ 750/3542]  eta: 0:47:53  lr: 0.000005  min_lr: 0.000005  loss: 1.8818 (1.8470)  loss_scale: 8192.0000 (12555.2490)  weight_decay: 0.0500 (0.0500)  time: 0.8678  data: 0.2855  max mem: 13008
Epoch: [8]  [ 760/3542]  eta: 0:47:37  lr: 0.000005  min_lr: 0.000005  loss: 1.8838 (1.8467)  loss_scale: 8192.0000 (12497.9133)  weight_decay: 0.0500 (0.0500)  time: 0.8395  data: 0.2572  max mem: 13008
Epoch: [8]  [ 770/3542]  eta: 0:47:22  lr: 0.000005  min_lr: 0.000005  loss: 1.8721 (1.8469)  loss_scale: 8192.0000 (12442.0649)  weight_decay: 0.0500 (0.0500)  time: 0.8832  data: 0.2988  max mem: 13008
Epoch: [8]  [ 780/3542]  eta: 0:47:06  lr: 0.000005  min_lr: 0.000005  loss: 1.8721 (1.8476)  loss_scale: 8192.0000 (12387.6466)  weight_decay: 0.0500 (0.0500)  time: 0.8787  data: 0.2947  max mem: 13008
Epoch: [8]  [ 790/3542]  eta: 0:46:58  lr: 0.000005  min_lr: 0.000005  loss: 1.9199 (1.8490)  loss_scale: 8192.0000 (12334.6043)  weight_decay: 0.0500 (0.0500)  time: 0.9772  data: 0.3957  max mem: 13008
Epoch: [8]  [ 800/3542]  eta: 0:46:44  lr: 0.000005  min_lr: 0.000005  loss: 1.8945 (1.8492)  loss_scale: 8192.0000 (12282.8864)  weight_decay: 0.0500 (0.0500)  time: 0.9989  data: 0.4178  max mem: 13008
Epoch: [8]  [ 810/3542]  eta: 0:46:28  lr: 0.000005  min_lr: 0.000005  loss: 1.7920 (1.8483)  loss_scale: 8192.0000 (12232.4439)  weight_decay: 0.0500 (0.0500)  time: 0.8770  data: 0.2957  max mem: 13008
Epoch: [8]  [ 820/3542]  eta: 0:46:15  lr: 0.000005  min_lr: 0.000005  loss: 1.7920 (1.8485)  loss_scale: 8192.0000 (12183.2302)  weight_decay: 0.0500 (0.0500)  time: 0.8990  data: 0.3158  max mem: 13008
Epoch: [8]  [ 830/3542]  eta: 0:46:01  lr: 0.000005  min_lr: 0.000005  loss: 1.8330 (1.8478)  loss_scale: 8192.0000 (12135.2010)  weight_decay: 0.0500 (0.0500)  time: 0.9202  data: 0.3366  max mem: 13008
Epoch: [8]  [ 840/3542]  eta: 0:45:45  lr: 0.000005  min_lr: 0.000005  loss: 1.7529 (1.8479)  loss_scale: 8192.0000 (12088.3139)  weight_decay: 0.0500 (0.0500)  time: 0.8598  data: 0.2779  max mem: 13008
Epoch: [8]  [ 850/3542]  eta: 0:45:32  lr: 0.000005  min_lr: 0.000005  loss: 1.8154 (1.8478)  loss_scale: 8192.0000 (12042.5288)  weight_decay: 0.0500 (0.0500)  time: 0.8895  data: 0.3078  max mem: 13008
Epoch: [8]  [ 860/3542]  eta: 0:45:17  lr: 0.000005  min_lr: 0.000005  loss: 1.8789 (1.8483)  loss_scale: 8192.0000 (11997.8072)  weight_decay: 0.0500 (0.0500)  time: 0.9016  data: 0.3200  max mem: 13008
Epoch: [8]  [ 870/3542]  eta: 0:44:59  lr: 0.000005  min_lr: 0.000005  loss: 1.8311 (1.8484)  loss_scale: 8192.0000 (11954.1125)  weight_decay: 0.0500 (0.0500)  time: 0.8012  data: 0.2192  max mem: 13008
Epoch: [8]  [ 880/3542]  eta: 0:44:43  lr: 0.000005  min_lr: 0.000005  loss: 1.8311 (1.8485)  loss_scale: 8192.0000 (11911.4098)  weight_decay: 0.0500 (0.0500)  time: 0.7859  data: 0.2033  max mem: 13008
Epoch: [8]  [ 890/3542]  eta: 0:44:33  lr: 0.000005  min_lr: 0.000005  loss: 1.7734 (1.8484)  loss_scale: 8192.0000 (11869.6655)  weight_decay: 0.0500 (0.0500)  time: 0.9062  data: 0.3215  max mem: 13008
Epoch: [8]  [ 900/3542]  eta: 0:44:13  lr: 0.000005  min_lr: 0.000005  loss: 1.8340 (1.8489)  loss_scale: 8192.0000 (11828.8479)  weight_decay: 0.0500 (0.0500)  time: 0.8430  data: 0.2580  max mem: 13008
Epoch: [8]  [ 910/3542]  eta: 0:44:00  lr: 0.000004  min_lr: 0.000004  loss: 1.8994 (1.8494)  loss_scale: 8192.0000 (11788.9265)  weight_decay: 0.0500 (0.0500)  time: 0.7994  data: 0.2165  max mem: 13008
Epoch: [8]  [ 920/3542]  eta: 0:43:50  lr: 0.000004  min_lr: 0.000004  loss: 1.8467 (1.8489)  loss_scale: 8192.0000 (11749.8719)  weight_decay: 0.0500 (0.0500)  time: 0.9390  data: 0.3562  max mem: 13008
Epoch: [8]  [ 930/3542]  eta: 0:43:38  lr: 0.000004  min_lr: 0.000004  loss: 1.7803 (1.8488)  loss_scale: 8192.0000 (11711.6563)  weight_decay: 0.0500 (0.0500)  time: 0.9563  data: 0.3738  max mem: 13008
Epoch: [8]  [ 940/3542]  eta: 0:43:19  lr: 0.000004  min_lr: 0.000004  loss: 1.7930 (1.8488)  loss_scale: 8192.0000 (11674.2529)  weight_decay: 0.0500 (0.0500)  time: 0.8142  data: 0.2311  max mem: 13008
Epoch: [8]  [ 950/3542]  eta: 0:43:08  lr: 0.000004  min_lr: 0.000004  loss: 1.8223 (1.8483)  loss_scale: 8192.0000 (11637.6362)  weight_decay: 0.0500 (0.0500)  time: 0.8265  data: 0.2414  max mem: 13008
Epoch: [8]  [ 960/3542]  eta: 0:42:53  lr: 0.000004  min_lr: 0.000004  loss: 1.7705 (1.8475)  loss_scale: 8192.0000 (11601.7815)  weight_decay: 0.0500 (0.0500)  time: 0.8831  data: 0.2989  max mem: 13008
Epoch: [8]  [ 970/3542]  eta: 0:42:37  lr: 0.000004  min_lr: 0.000004  loss: 1.7783 (1.8469)  loss_scale: 8192.0000 (11566.6653)  weight_decay: 0.0500 (0.0500)  time: 0.7860  data: 0.2031  max mem: 13008
Epoch: [8]  [ 980/3542]  eta: 0:42:23  lr: 0.000004  min_lr: 0.000004  loss: 1.8311 (1.8477)  loss_scale: 8192.0000 (11532.2650)  weight_decay: 0.0500 (0.0500)  time: 0.8024  data: 0.2197  max mem: 13008
Epoch: [8]  [ 990/3542]  eta: 0:42:08  lr: 0.000004  min_lr: 0.000004  loss: 1.9834 (1.8488)  loss_scale: 8192.0000 (11498.5590)  weight_decay: 0.0500 (0.0500)  time: 0.8210  data: 0.2386  max mem: 13008
Epoch: [8]  [1000/3542]  eta: 0:41:57  lr: 0.000004  min_lr: 0.000004  loss: 1.9619 (1.8494)  loss_scale: 8192.0000 (11465.5265)  weight_decay: 0.0500 (0.0500)  time: 0.8768  data: 0.2944  max mem: 13008
Epoch: [8]  [1010/3542]  eta: 0:41:42  lr: 0.000004  min_lr: 0.000004  loss: 1.8369 (1.8493)  loss_scale: 8192.0000 (11433.1474)  weight_decay: 0.0500 (0.0500)  time: 0.8734  data: 0.2916  max mem: 13008
Epoch: [8]  [1020/3542]  eta: 0:41:29  lr: 0.000004  min_lr: 0.000004  loss: 1.8223 (1.8491)  loss_scale: 8192.0000 (11401.4025)  weight_decay: 0.0500 (0.0500)  time: 0.8213  data: 0.2397  max mem: 13008
Epoch: [8]  [1030/3542]  eta: 0:41:17  lr: 0.000004  min_lr: 0.000004  loss: 1.8223 (1.8488)  loss_scale: 8192.0000 (11370.2735)  weight_decay: 0.0500 (0.0500)  time: 0.8664  data: 0.2843  max mem: 13008
Epoch: [8]  [1040/3542]  eta: 0:41:06  lr: 0.000004  min_lr: 0.000004  loss: 1.7627 (1.8488)  loss_scale: 8192.0000 (11339.7426)  weight_decay: 0.0500 (0.0500)  time: 0.9280  data: 0.3433  max mem: 13008
Epoch: [8]  [1050/3542]  eta: 0:40:54  lr: 0.000004  min_lr: 0.000004  loss: 1.7627 (1.8486)  loss_scale: 8192.0000 (11309.7926)  weight_decay: 0.0500 (0.0500)  time: 0.9319  data: 0.3477  max mem: 13008
Epoch: [8]  [1060/3542]  eta: 0:40:38  lr: 0.000004  min_lr: 0.000004  loss: 1.8047 (1.8480)  loss_scale: 8192.0000 (11280.4072)  weight_decay: 0.0500 (0.0500)  time: 0.8117  data: 0.2292  max mem: 13008
Epoch: [8]  [1070/3542]  eta: 0:40:28  lr: 0.000004  min_lr: 0.000004  loss: 1.8340 (1.8479)  loss_scale: 8192.0000 (11251.5705)  weight_decay: 0.0500 (0.0500)  time: 0.8476  data: 0.2644  max mem: 13008
Epoch: [8]  [1080/3542]  eta: 0:40:15  lr: 0.000004  min_lr: 0.000004  loss: 1.8672 (1.8483)  loss_scale: 8192.0000 (11223.2673)  weight_decay: 0.0500 (0.0500)  time: 0.9037  data: 0.3208  max mem: 13008
Epoch: [8]  [1090/3542]  eta: 0:40:01  lr: 0.000004  min_lr: 0.000004  loss: 1.8672 (1.8483)  loss_scale: 8192.0000 (11195.4830)  weight_decay: 0.0500 (0.0500)  time: 0.8053  data: 0.2229  max mem: 13008
Epoch: [8]  [1100/3542]  eta: 0:39:51  lr: 0.000004  min_lr: 0.000004  loss: 1.8359 (1.8481)  loss_scale: 8192.0000 (11168.2035)  weight_decay: 0.0500 (0.0500)  time: 0.8820  data: 0.2992  max mem: 13008
Epoch: [8]  [1110/3542]  eta: 0:39:39  lr: 0.000004  min_lr: 0.000004  loss: 1.8740 (1.8482)  loss_scale: 8192.0000 (11141.4149)  weight_decay: 0.0500 (0.0500)  time: 0.9371  data: 0.3549  max mem: 13008
Epoch: [8]  [1120/3542]  eta: 0:39:29  lr: 0.000004  min_lr: 0.000004  loss: 1.8408 (1.8483)  loss_scale: 8192.0000 (11115.1044)  weight_decay: 0.0500 (0.0500)  time: 0.9272  data: 0.3451  max mem: 13008
Epoch: [8]  [1130/3542]  eta: 0:39:19  lr: 0.000004  min_lr: 0.000004  loss: 1.8018 (1.8484)  loss_scale: 8192.0000 (11089.2591)  weight_decay: 0.0500 (0.0500)  time: 0.9697  data: 0.3869  max mem: 13008
Epoch: [8]  [1140/3542]  eta: 0:39:09  lr: 0.000004  min_lr: 0.000004  loss: 1.8555 (1.8483)  loss_scale: 8192.0000 (11063.8668)  weight_decay: 0.0500 (0.0500)  time: 0.9702  data: 0.3879  max mem: 13008
Epoch: [8]  [1150/3542]  eta: 0:38:59  lr: 0.000004  min_lr: 0.000004  loss: 1.8555 (1.8483)  loss_scale: 8192.0000 (11038.9157)  weight_decay: 0.0500 (0.0500)  time: 0.9652  data: 0.3838  max mem: 13008
Epoch: [8]  [1160/3542]  eta: 0:38:48  lr: 0.000004  min_lr: 0.000004  loss: 1.8379 (1.8487)  loss_scale: 8192.0000 (11014.3945)  weight_decay: 0.0500 (0.0500)  time: 0.9361  data: 0.3547  max mem: 13008
Epoch: [8]  [1170/3542]  eta: 0:38:36  lr: 0.000004  min_lr: 0.000004  loss: 1.8379 (1.8486)  loss_scale: 8192.0000 (10990.2921)  weight_decay: 0.0500 (0.0500)  time: 0.8893  data: 0.3054  max mem: 13008
Epoch: [8]  [1180/3542]  eta: 0:38:24  lr: 0.000004  min_lr: 0.000004  loss: 1.8047 (1.8480)  loss_scale: 8192.0000 (10966.5978)  weight_decay: 0.0500 (0.0500)  time: 0.8613  data: 0.2777  max mem: 13008
Epoch: [8]  [1190/3542]  eta: 0:38:11  lr: 0.000004  min_lr: 0.000004  loss: 1.7754 (1.8477)  loss_scale: 8192.0000 (10943.3014)  weight_decay: 0.0500 (0.0500)  time: 0.8262  data: 0.2441  max mem: 13008
Epoch: [8]  [1200/3542]  eta: 0:37:58  lr: 0.000004  min_lr: 0.000004  loss: 1.8535 (1.8482)  loss_scale: 8192.0000 (10920.3930)  weight_decay: 0.0500 (0.0500)  time: 0.8094  data: 0.2269  max mem: 13008
Epoch: [8]  [1210/3542]  eta: 0:37:46  lr: 0.000004  min_lr: 0.000004  loss: 1.8809 (1.8481)  loss_scale: 8192.0000 (10897.8629)  weight_decay: 0.0500 (0.0500)  time: 0.8319  data: 0.2499  max mem: 13008
Epoch: [8]  [1220/3542]  eta: 0:37:31  lr: 0.000004  min_lr: 0.000004  loss: 1.8281 (1.8483)  loss_scale: 8192.0000 (10875.7019)  weight_decay: 0.0500 (0.0500)  time: 0.7879  data: 0.2060  max mem: 13008
Epoch: [8]  [1230/3542]  eta: 0:37:16  lr: 0.000004  min_lr: 0.000004  loss: 1.8359 (1.8485)  loss_scale: 8192.0000 (10853.9009)  weight_decay: 0.0500 (0.0500)  time: 0.7025  data: 0.1195  max mem: 13008
Epoch: [8]  [1240/3542]  eta: 0:37:02  lr: 0.000004  min_lr: 0.000004  loss: 1.8359 (1.8480)  loss_scale: 8192.0000 (10832.4512)  weight_decay: 0.0500 (0.0500)  time: 0.7035  data: 0.1194  max mem: 13008
Epoch: [8]  [1250/3542]  eta: 0:36:48  lr: 0.000004  min_lr: 0.000004  loss: 1.8174 (1.8478)  loss_scale: 8192.0000 (10811.3445)  weight_decay: 0.0500 (0.0500)  time: 0.7417  data: 0.1573  max mem: 13008
Epoch: [8]  [1260/3542]  eta: 0:36:32  lr: 0.000004  min_lr: 0.000004  loss: 1.8037 (1.8475)  loss_scale: 8192.0000 (10790.5726)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0822  max mem: 13008
Epoch: [8]  [1270/3542]  eta: 0:36:18  lr: 0.000004  min_lr: 0.000004  loss: 1.8447 (1.8472)  loss_scale: 8192.0000 (10770.1275)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0752  max mem: 13008
Epoch: [8]  [1280/3542]  eta: 0:36:04  lr: 0.000004  min_lr: 0.000004  loss: 1.7959 (1.8467)  loss_scale: 8192.0000 (10750.0016)  weight_decay: 0.0500 (0.0500)  time: 0.7073  data: 0.1235  max mem: 13008
Epoch: [8]  [1290/3542]  eta: 0:35:49  lr: 0.000004  min_lr: 0.000004  loss: 1.8350 (1.8469)  loss_scale: 8192.0000 (10730.1875)  weight_decay: 0.0500 (0.0500)  time: 0.6729  data: 0.0888  max mem: 13008
Epoch: [8]  [1300/3542]  eta: 0:35:33  lr: 0.000004  min_lr: 0.000004  loss: 1.8125 (1.8467)  loss_scale: 8192.0000 (10710.6779)  weight_decay: 0.0500 (0.0500)  time: 0.6219  data: 0.0369  max mem: 13008
Epoch: [8]  [1310/3542]  eta: 0:35:18  lr: 0.000004  min_lr: 0.000004  loss: 1.8242 (1.8470)  loss_scale: 8192.0000 (10691.4661)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.0306  max mem: 13008
Epoch: [8]  [1320/3542]  eta: 0:35:03  lr: 0.000004  min_lr: 0.000004  loss: 1.8438 (1.8470)  loss_scale: 8192.0000 (10672.5450)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.0507  max mem: 13008
Epoch: [8]  [1330/3542]  eta: 0:34:49  lr: 0.000004  min_lr: 0.000004  loss: 1.8838 (1.8476)  loss_scale: 8192.0000 (10653.9083)  weight_decay: 0.0500 (0.0500)  time: 0.6410  data: 0.0559  max mem: 13008
Epoch: [8]  [1340/3542]  eta: 0:34:34  lr: 0.000004  min_lr: 0.000004  loss: 1.8838 (1.8474)  loss_scale: 8192.0000 (10635.5496)  weight_decay: 0.0500 (0.0500)  time: 0.6209  data: 0.0358  max mem: 13008
Epoch: [8]  [1350/3542]  eta: 0:34:19  lr: 0.000004  min_lr: 0.000004  loss: 1.8096 (1.8472)  loss_scale: 8192.0000 (10617.4626)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0003  max mem: 13008
Epoch: [8]  [1360/3542]  eta: 0:34:04  lr: 0.000004  min_lr: 0.000004  loss: 1.8271 (1.8473)  loss_scale: 8192.0000 (10599.6414)  weight_decay: 0.0500 (0.0500)  time: 0.6082  data: 0.0226  max mem: 13008
Epoch: [8]  [1370/3542]  eta: 0:33:50  lr: 0.000004  min_lr: 0.000004  loss: 1.8271 (1.8472)  loss_scale: 8192.0000 (10582.0802)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0402  max mem: 13008
Epoch: [8]  [1380/3542]  eta: 0:33:36  lr: 0.000004  min_lr: 0.000004  loss: 1.8496 (1.8475)  loss_scale: 8192.0000 (10564.7734)  weight_decay: 0.0500 (0.0500)  time: 0.6188  data: 0.0344  max mem: 13008
Epoch: [8]  [1390/3542]  eta: 0:33:21  lr: 0.000004  min_lr: 0.000004  loss: 1.8838 (1.8478)  loss_scale: 8192.0000 (10547.7153)  weight_decay: 0.0500 (0.0500)  time: 0.6171  data: 0.0324  max mem: 13008
Epoch: [8]  [1400/3542]  eta: 0:33:08  lr: 0.000004  min_lr: 0.000004  loss: 1.8232 (1.8477)  loss_scale: 8192.0000 (10530.9008)  weight_decay: 0.0500 (0.0500)  time: 0.6322  data: 0.0466  max mem: 13008
Epoch: [8]  [1410/3542]  eta: 0:32:54  lr: 0.000004  min_lr: 0.000004  loss: 1.8232 (1.8478)  loss_scale: 8192.0000 (10514.3246)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.0604  max mem: 13008
Epoch: [8]  [1420/3542]  eta: 0:32:40  lr: 0.000004  min_lr: 0.000004  loss: 1.7832 (1.8473)  loss_scale: 8192.0000 (10497.9817)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0333  max mem: 13008
Epoch: [8]  [1430/3542]  eta: 0:32:27  lr: 0.000004  min_lr: 0.000004  loss: 1.7764 (1.8473)  loss_scale: 8192.0000 (10481.8672)  weight_decay: 0.0500 (0.0500)  time: 0.6262  data: 0.0414  max mem: 13008
Epoch: [8]  [1440/3542]  eta: 0:32:13  lr: 0.000004  min_lr: 0.000004  loss: 1.8330 (1.8475)  loss_scale: 8192.0000 (10465.9764)  weight_decay: 0.0500 (0.0500)  time: 0.6418  data: 0.0568  max mem: 13008
Epoch: [8]  [1450/3542]  eta: 0:31:59  lr: 0.000004  min_lr: 0.000004  loss: 1.8818 (1.8479)  loss_scale: 8192.0000 (10450.3046)  weight_decay: 0.0500 (0.0500)  time: 0.6047  data: 0.0193  max mem: 13008
Epoch: [8]  [1460/3542]  eta: 0:31:45  lr: 0.000004  min_lr: 0.000004  loss: 1.7783 (1.8475)  loss_scale: 8192.0000 (10434.8474)  weight_decay: 0.0500 (0.0500)  time: 0.5880  data: 0.0026  max mem: 13008
Epoch: [8]  [1470/3542]  eta: 0:31:32  lr: 0.000004  min_lr: 0.000004  loss: 1.7646 (1.8478)  loss_scale: 8192.0000 (10419.6003)  weight_decay: 0.0500 (0.0500)  time: 0.6060  data: 0.0206  max mem: 13008
Epoch: [8]  [1480/3542]  eta: 0:31:19  lr: 0.000004  min_lr: 0.000004  loss: 1.7891 (1.8476)  loss_scale: 8192.0000 (10404.5591)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.0474  max mem: 13008
Epoch: [8]  [1490/3542]  eta: 0:31:06  lr: 0.000004  min_lr: 0.000004  loss: 1.8398 (1.8480)  loss_scale: 8192.0000 (10389.7197)  weight_decay: 0.0500 (0.0500)  time: 0.6145  data: 0.0293  max mem: 13008
Epoch: [8]  [1500/3542]  eta: 0:30:52  lr: 0.000004  min_lr: 0.000004  loss: 1.8975 (1.8481)  loss_scale: 8192.0000 (10375.0779)  weight_decay: 0.0500 (0.0500)  time: 0.5940  data: 0.0088  max mem: 13008
Epoch: [8]  [1510/3542]  eta: 0:30:40  lr: 0.000004  min_lr: 0.000004  loss: 1.8701 (1.8481)  loss_scale: 8192.0000 (10360.6300)  weight_decay: 0.0500 (0.0500)  time: 0.6176  data: 0.0323  max mem: 13008
Epoch: [8]  [1520/3542]  eta: 0:30:27  lr: 0.000004  min_lr: 0.000004  loss: 1.8291 (1.8483)  loss_scale: 8192.0000 (10346.3721)  weight_decay: 0.0500 (0.0500)  time: 0.6260  data: 0.0402  max mem: 13008
Epoch: [8]  [1530/3542]  eta: 0:30:14  lr: 0.000004  min_lr: 0.000004  loss: 1.8828 (1.8487)  loss_scale: 8192.0000 (10332.3005)  weight_decay: 0.0500 (0.0500)  time: 0.6064  data: 0.0204  max mem: 13008
Epoch: [8]  [1540/3542]  eta: 0:30:01  lr: 0.000004  min_lr: 0.000004  loss: 1.8994 (1.8488)  loss_scale: 8192.0000 (10318.4114)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0185  max mem: 13008
Epoch: [8]  [1550/3542]  eta: 0:29:48  lr: 0.000004  min_lr: 0.000004  loss: 1.8809 (1.8489)  loss_scale: 8192.0000 (10304.7015)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0214  max mem: 13008
Epoch: [8]  [1560/3542]  eta: 0:29:35  lr: 0.000004  min_lr: 0.000004  loss: 1.8350 (1.8488)  loss_scale: 8192.0000 (10291.1672)  weight_decay: 0.0500 (0.0500)  time: 0.5984  data: 0.0122  max mem: 13008
Epoch: [8]  [1570/3542]  eta: 0:29:23  lr: 0.000004  min_lr: 0.000004  loss: 1.8350 (1.8489)  loss_scale: 8192.0000 (10277.8052)  weight_decay: 0.0500 (0.0500)  time: 0.5995  data: 0.0139  max mem: 13008
Epoch: [8]  [1580/3542]  eta: 0:29:10  lr: 0.000004  min_lr: 0.000004  loss: 1.8652 (1.8490)  loss_scale: 8192.0000 (10264.6123)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.0369  max mem: 13008
Epoch: [8]  [1590/3542]  eta: 0:28:58  lr: 0.000004  min_lr: 0.000004  loss: 1.8652 (1.8491)  loss_scale: 8192.0000 (10251.5852)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0286  max mem: 13008
Epoch: [8]  [1600/3542]  eta: 0:28:45  lr: 0.000004  min_lr: 0.000004  loss: 1.8633 (1.8492)  loss_scale: 8192.0000 (10238.7208)  weight_decay: 0.0500 (0.0500)  time: 0.5854  data: 0.0003  max mem: 13008
Epoch: [8]  [1610/3542]  eta: 0:28:33  lr: 0.000004  min_lr: 0.000004  loss: 1.7959 (1.8491)  loss_scale: 8192.0000 (10226.0161)  weight_decay: 0.0500 (0.0500)  time: 0.5863  data: 0.0003  max mem: 13008
Epoch: [8]  [1620/3542]  eta: 0:28:21  lr: 0.000004  min_lr: 0.000004  loss: 1.8135 (1.8489)  loss_scale: 8192.0000 (10213.4682)  weight_decay: 0.0500 (0.0500)  time: 0.6030  data: 0.0164  max mem: 13008
Epoch: [8]  [1630/3542]  eta: 0:28:09  lr: 0.000004  min_lr: 0.000004  loss: 1.8135 (1.8488)  loss_scale: 8192.0000 (10201.0742)  weight_decay: 0.0500 (0.0500)  time: 0.6202  data: 0.0344  max mem: 13008
Epoch: [8]  [1640/3542]  eta: 0:27:56  lr: 0.000004  min_lr: 0.000004  loss: 1.8115 (1.8487)  loss_scale: 8192.0000 (10188.8312)  weight_decay: 0.0500 (0.0500)  time: 0.6044  data: 0.0183  max mem: 13008
Epoch: [8]  [1650/3542]  eta: 0:27:44  lr: 0.000004  min_lr: 0.000004  loss: 1.8701 (1.8492)  loss_scale: 8192.0000 (10176.7365)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0014  max mem: 13008
Epoch: [8]  [1660/3542]  eta: 0:27:32  lr: 0.000004  min_lr: 0.000004  loss: 1.8750 (1.8491)  loss_scale: 8192.0000 (10164.7875)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0029  max mem: 13008
[2023-05-16 13:05:27,723] [INFO] [logging.py:60:log_dist] [Rank 0] step=30000, skipped=27, lr=[3.717232862170515e-06, 3.717232862170515e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 13:05:27,726] [INFO] [timer.py:157:stop] 0/30000, SamplesPerSec=55.55870404473201
Epoch: [8]  [1670/3542]  eta: 0:27:20  lr: 0.000004  min_lr: 0.000004  loss: 1.7979 (1.8487)  loss_scale: 8192.0000 (10152.9814)  weight_decay: 0.0500 (0.0500)  time: 0.6004  data: 0.0144  max mem: 13008
Epoch: [8]  [1680/3542]  eta: 0:27:09  lr: 0.000004  min_lr: 0.000004  loss: 1.7949 (1.8484)  loss_scale: 8192.0000 (10141.3159)  weight_decay: 0.0500 (0.0500)  time: 0.6329  data: 0.0468  max mem: 13008
Epoch: [8]  [1690/3542]  eta: 0:26:58  lr: 0.000004  min_lr: 0.000004  loss: 1.8301 (1.8484)  loss_scale: 8192.0000 (10129.7883)  weight_decay: 0.0500 (0.0500)  time: 0.6359  data: 0.0507  max mem: 13008
[2023-05-16 13:05:50,070] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 13:05:50,070] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [8]  [1700/3542]  eta: 0:26:46  lr: 0.000004  min_lr: 0.000004  loss: 1.8350 (1.8483)  loss_scale: 8192.0000 (10128.0282)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0196  max mem: 13008
Epoch: [8]  [1710/3542]  eta: 0:26:34  lr: 0.000004  min_lr: 0.000004  loss: 1.7930 (1.8482)  loss_scale: 16384.0000 (10164.5915)  weight_decay: 0.0500 (0.0500)  time: 0.5954  data: 0.0097  max mem: 13008
Epoch: [8]  [1720/3542]  eta: 0:26:23  lr: 0.000004  min_lr: 0.000004  loss: 1.8037 (1.8486)  loss_scale: 16384.0000 (10200.7298)  weight_decay: 0.0500 (0.0500)  time: 0.6057  data: 0.0200  max mem: 13008
Epoch: [8]  [1730/3542]  eta: 0:26:11  lr: 0.000004  min_lr: 0.000004  loss: 1.8623 (1.8485)  loss_scale: 16384.0000 (10236.4506)  weight_decay: 0.0500 (0.0500)  time: 0.6062  data: 0.0203  max mem: 13008
Epoch: [8]  [1740/3542]  eta: 0:26:00  lr: 0.000004  min_lr: 0.000004  loss: 1.8301 (1.8486)  loss_scale: 16384.0000 (10271.7611)  weight_decay: 0.0500 (0.0500)  time: 0.5936  data: 0.0071  max mem: 13008
Epoch: [8]  [1750/3542]  eta: 0:25:48  lr: 0.000004  min_lr: 0.000004  loss: 1.8301 (1.8489)  loss_scale: 16384.0000 (10306.6682)  weight_decay: 0.0500 (0.0500)  time: 0.5901  data: 0.0043  max mem: 13008
Epoch: [8]  [1760/3542]  eta: 0:25:37  lr: 0.000004  min_lr: 0.000004  loss: 1.8037 (1.8487)  loss_scale: 16384.0000 (10341.1789)  weight_decay: 0.0500 (0.0500)  time: 0.5898  data: 0.0043  max mem: 13008
Epoch: [8]  [1770/3542]  eta: 0:25:26  lr: 0.000004  min_lr: 0.000004  loss: 1.8145 (1.8487)  loss_scale: 16384.0000 (10375.2998)  weight_decay: 0.0500 (0.0500)  time: 0.6038  data: 0.0185  max mem: 13008
Epoch: [8]  [1780/3542]  eta: 0:25:14  lr: 0.000004  min_lr: 0.000004  loss: 1.8613 (1.8493)  loss_scale: 16384.0000 (10409.0376)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0232  max mem: 13008
Epoch: [8]  [1790/3542]  eta: 0:25:03  lr: 0.000004  min_lr: 0.000004  loss: 1.8350 (1.8486)  loss_scale: 16384.0000 (10442.3987)  weight_decay: 0.0500 (0.0500)  time: 0.5926  data: 0.0079  max mem: 13008
Epoch: [8]  [1800/3542]  eta: 0:24:52  lr: 0.000004  min_lr: 0.000004  loss: 1.7168 (1.8481)  loss_scale: 16384.0000 (10475.3892)  weight_decay: 0.0500 (0.0500)  time: 0.5889  data: 0.0032  max mem: 13008
Epoch: [8]  [1810/3542]  eta: 0:24:41  lr: 0.000004  min_lr: 0.000004  loss: 1.7822 (1.8484)  loss_scale: 16384.0000 (10508.0155)  weight_decay: 0.0500 (0.0500)  time: 0.5907  data: 0.0051  max mem: 13008
Epoch: [8]  [1820/3542]  eta: 0:24:30  lr: 0.000004  min_lr: 0.000004  loss: 1.8906 (1.8484)  loss_scale: 16384.0000 (10540.2834)  weight_decay: 0.0500 (0.0500)  time: 0.6237  data: 0.0386  max mem: 13008
Epoch: [8]  [1830/3542]  eta: 0:24:20  lr: 0.000004  min_lr: 0.000004  loss: 1.8975 (1.8486)  loss_scale: 16384.0000 (10572.1988)  weight_decay: 0.0500 (0.0500)  time: 0.6270  data: 0.0418  max mem: 13008
Epoch: [8]  [1840/3542]  eta: 0:24:09  lr: 0.000004  min_lr: 0.000004  loss: 1.8213 (1.8485)  loss_scale: 16384.0000 (10603.7675)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0082  max mem: 13008
Epoch: [8]  [1850/3542]  eta: 0:23:58  lr: 0.000004  min_lr: 0.000004  loss: 1.7930 (1.8488)  loss_scale: 16384.0000 (10634.9951)  weight_decay: 0.0500 (0.0500)  time: 0.6046  data: 0.0193  max mem: 13008
Epoch: [8]  [1860/3542]  eta: 0:23:47  lr: 0.000004  min_lr: 0.000004  loss: 1.7754 (1.8483)  loss_scale: 16384.0000 (10665.8872)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0357  max mem: 13008
Epoch: [8]  [1870/3542]  eta: 0:23:37  lr: 0.000004  min_lr: 0.000004  loss: 1.8105 (1.8481)  loss_scale: 16384.0000 (10696.4490)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.0432  max mem: 13008
Epoch: [8]  [1880/3542]  eta: 0:23:26  lr: 0.000004  min_lr: 0.000004  loss: 1.7803 (1.8477)  loss_scale: 16384.0000 (10726.6858)  weight_decay: 0.0500 (0.0500)  time: 0.6162  data: 0.0302  max mem: 13008
Epoch: [8]  [1890/3542]  eta: 0:23:16  lr: 0.000003  min_lr: 0.000003  loss: 1.7646 (1.8474)  loss_scale: 16384.0000 (10756.6029)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0069  max mem: 13008
Epoch: [8]  [1900/3542]  eta: 0:23:05  lr: 0.000003  min_lr: 0.000003  loss: 1.8438 (1.8476)  loss_scale: 16384.0000 (10786.2052)  weight_decay: 0.0500 (0.0500)  time: 0.5946  data: 0.0092  max mem: 13008
Epoch: [8]  [1910/3542]  eta: 0:22:55  lr: 0.000003  min_lr: 0.000003  loss: 1.8877 (1.8480)  loss_scale: 16384.0000 (10815.4976)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0241  max mem: 13008
Epoch: [8]  [1920/3542]  eta: 0:22:44  lr: 0.000003  min_lr: 0.000003  loss: 1.8779 (1.8481)  loss_scale: 16384.0000 (10844.4852)  weight_decay: 0.0500 (0.0500)  time: 0.6185  data: 0.0325  max mem: 13008
Epoch: [8]  [1930/3542]  eta: 0:22:34  lr: 0.000003  min_lr: 0.000003  loss: 1.8535 (1.8484)  loss_scale: 16384.0000 (10873.1724)  weight_decay: 0.0500 (0.0500)  time: 0.6157  data: 0.0294  max mem: 13008
Epoch: [8]  [1940/3542]  eta: 0:22:24  lr: 0.000003  min_lr: 0.000003  loss: 1.8604 (1.8487)  loss_scale: 16384.0000 (10901.5641)  weight_decay: 0.0500 (0.0500)  time: 0.6007  data: 0.0154  max mem: 13008
Epoch: [8]  [1950/3542]  eta: 0:22:13  lr: 0.000003  min_lr: 0.000003  loss: 1.8457 (1.8485)  loss_scale: 16384.0000 (10929.6648)  weight_decay: 0.0500 (0.0500)  time: 0.5935  data: 0.0083  max mem: 13008
Epoch: [8]  [1960/3542]  eta: 0:22:03  lr: 0.000003  min_lr: 0.000003  loss: 1.8848 (1.8487)  loss_scale: 16384.0000 (10957.4788)  weight_decay: 0.0500 (0.0500)  time: 0.5931  data: 0.0081  max mem: 13008
Epoch: [8]  [1970/3542]  eta: 0:21:53  lr: 0.000003  min_lr: 0.000003  loss: 1.8604 (1.8484)  loss_scale: 16384.0000 (10985.0107)  weight_decay: 0.0500 (0.0500)  time: 0.6040  data: 0.0191  max mem: 13008
Epoch: [8]  [1980/3542]  eta: 0:21:43  lr: 0.000003  min_lr: 0.000003  loss: 1.8350 (1.8484)  loss_scale: 16384.0000 (11012.2645)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.0426  max mem: 13008
Epoch: [8]  [1990/3542]  eta: 0:21:33  lr: 0.000003  min_lr: 0.000003  loss: 1.8613 (1.8484)  loss_scale: 16384.0000 (11039.2446)  weight_decay: 0.0500 (0.0500)  time: 0.6153  data: 0.0292  max mem: 13008
Epoch: [8]  [2000/3542]  eta: 0:21:23  lr: 0.000003  min_lr: 0.000003  loss: 1.8613 (1.8486)  loss_scale: 16384.0000 (11065.9550)  weight_decay: 0.0500 (0.0500)  time: 0.6129  data: 0.0273  max mem: 13008
Epoch: [8]  [2010/3542]  eta: 0:21:13  lr: 0.000003  min_lr: 0.000003  loss: 1.9043 (1.8489)  loss_scale: 16384.0000 (11092.3998)  weight_decay: 0.0500 (0.0500)  time: 0.6324  data: 0.0466  max mem: 13008
Epoch: [8]  [2020/3542]  eta: 0:21:03  lr: 0.000003  min_lr: 0.000003  loss: 1.8936 (1.8489)  loss_scale: 16384.0000 (11118.5829)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.0601  max mem: 13008
Epoch: [8]  [2030/3542]  eta: 0:20:53  lr: 0.000003  min_lr: 0.000003  loss: 1.8936 (1.8493)  loss_scale: 16384.0000 (11144.5081)  weight_decay: 0.0500 (0.0500)  time: 0.6280  data: 0.0429  max mem: 13008
Epoch: [8]  [2040/3542]  eta: 0:20:43  lr: 0.000003  min_lr: 0.000003  loss: 1.8916 (1.8493)  loss_scale: 16384.0000 (11170.1793)  weight_decay: 0.0500 (0.0500)  time: 0.6052  data: 0.0203  max mem: 13008
Epoch: [8]  [2050/3542]  eta: 0:20:34  lr: 0.000003  min_lr: 0.000003  loss: 1.8350 (1.8493)  loss_scale: 16384.0000 (11195.6002)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0319  max mem: 13008
[2023-05-16 13:09:27,139] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 30391
[2023-05-16 13:09:27,139] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 13:09:27,139] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [8]  [2060/3542]  eta: 0:20:24  lr: 0.000003  min_lr: 0.000003  loss: 1.9160 (1.8496)  loss_scale: 16384.0000 (11196.9258)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0389  max mem: 13008
Epoch: [8]  [2070/3542]  eta: 0:20:14  lr: 0.000003  min_lr: 0.000003  loss: 1.8857 (1.8496)  loss_scale: 8192.0000 (11182.4162)  weight_decay: 0.0500 (0.0500)  time: 0.6253  data: 0.0395  max mem: 13008
Epoch: [8]  [2080/3542]  eta: 0:20:05  lr: 0.000003  min_lr: 0.000003  loss: 1.8408 (1.8496)  loss_scale: 8192.0000 (11168.0461)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.0683  max mem: 13008
Epoch: [8]  [2090/3542]  eta: 0:19:55  lr: 0.000003  min_lr: 0.000003  loss: 1.8223 (1.8493)  loss_scale: 8192.0000 (11153.8135)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.0598  max mem: 13008
Epoch: [8]  [2100/3542]  eta: 0:19:45  lr: 0.000003  min_lr: 0.000003  loss: 1.8213 (1.8496)  loss_scale: 8192.0000 (11139.7163)  weight_decay: 0.0500 (0.0500)  time: 0.5982  data: 0.0131  max mem: 13008
Epoch: [8]  [2110/3542]  eta: 0:19:35  lr: 0.000003  min_lr: 0.000003  loss: 1.7764 (1.8495)  loss_scale: 8192.0000 (11125.7527)  weight_decay: 0.0500 (0.0500)  time: 0.5884  data: 0.0020  max mem: 13008
Epoch: [8]  [2120/3542]  eta: 0:19:26  lr: 0.000003  min_lr: 0.000003  loss: 1.7803 (1.8497)  loss_scale: 8192.0000 (11111.9208)  weight_decay: 0.0500 (0.0500)  time: 0.5870  data: 0.0003  max mem: 13008
Epoch: [8]  [2130/3542]  eta: 0:19:16  lr: 0.000003  min_lr: 0.000003  loss: 1.9248 (1.8502)  loss_scale: 8192.0000 (11098.2187)  weight_decay: 0.0500 (0.0500)  time: 0.5876  data: 0.0019  max mem: 13008
Epoch: [8]  [2140/3542]  eta: 0:19:07  lr: 0.000003  min_lr: 0.000003  loss: 1.8320 (1.8498)  loss_scale: 8192.0000 (11084.6446)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0318  max mem: 13008
Epoch: [8]  [2150/3542]  eta: 0:18:57  lr: 0.000003  min_lr: 0.000003  loss: 1.7793 (1.8499)  loss_scale: 8192.0000 (11071.1967)  weight_decay: 0.0500 (0.0500)  time: 0.6211  data: 0.0360  max mem: 13008
Epoch: [8]  [2160/3542]  eta: 0:18:47  lr: 0.000003  min_lr: 0.000003  loss: 1.7969 (1.8496)  loss_scale: 8192.0000 (11057.8732)  weight_decay: 0.0500 (0.0500)  time: 0.6028  data: 0.0179  max mem: 13008
Epoch: [8]  [2170/3542]  eta: 0:18:38  lr: 0.000003  min_lr: 0.000003  loss: 1.8154 (1.8496)  loss_scale: 8192.0000 (11044.6725)  weight_decay: 0.0500 (0.0500)  time: 0.6422  data: 0.0578  max mem: 13008
Epoch: [8]  [2180/3542]  eta: 0:18:29  lr: 0.000003  min_lr: 0.000003  loss: 1.8154 (1.8493)  loss_scale: 8192.0000 (11031.5928)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.0755  max mem: 13008
Epoch: [8]  [2190/3542]  eta: 0:18:20  lr: 0.000003  min_lr: 0.000003  loss: 1.8311 (1.8496)  loss_scale: 8192.0000 (11018.6326)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0574  max mem: 13008
Epoch: [8]  [2200/3542]  eta: 0:18:10  lr: 0.000003  min_lr: 0.000003  loss: 1.7832 (1.8494)  loss_scale: 8192.0000 (11005.7901)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0279  max mem: 13008
Epoch: [8]  [2210/3542]  eta: 0:18:01  lr: 0.000003  min_lr: 0.000003  loss: 1.7832 (1.8495)  loss_scale: 8192.0000 (10993.0638)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0033  max mem: 13008
Epoch: [8]  [2220/3542]  eta: 0:17:52  lr: 0.000003  min_lr: 0.000003  loss: 1.8789 (1.8494)  loss_scale: 8192.0000 (10980.4520)  weight_decay: 0.0500 (0.0500)  time: 0.6051  data: 0.0194  max mem: 13008
Epoch: [8]  [2230/3542]  eta: 0:17:42  lr: 0.000003  min_lr: 0.000003  loss: 1.8564 (1.8494)  loss_scale: 8192.0000 (10967.9534)  weight_decay: 0.0500 (0.0500)  time: 0.6137  data: 0.0277  max mem: 13008
Epoch: [8]  [2240/3542]  eta: 0:17:33  lr: 0.000003  min_lr: 0.000003  loss: 1.8555 (1.8496)  loss_scale: 8192.0000 (10955.5663)  weight_decay: 0.0500 (0.0500)  time: 0.6252  data: 0.0394  max mem: 13008
Epoch: [8]  [2250/3542]  eta: 0:17:24  lr: 0.000003  min_lr: 0.000003  loss: 1.8633 (1.8500)  loss_scale: 8192.0000 (10943.2892)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.0447  max mem: 13008
Epoch: [8]  [2260/3542]  eta: 0:17:15  lr: 0.000003  min_lr: 0.000003  loss: 1.9150 (1.8502)  loss_scale: 8192.0000 (10931.1207)  weight_decay: 0.0500 (0.0500)  time: 0.6023  data: 0.0169  max mem: 13008
Epoch: [8]  [2270/3542]  eta: 0:17:06  lr: 0.000003  min_lr: 0.000003  loss: 1.9023 (1.8503)  loss_scale: 8192.0000 (10919.0594)  weight_decay: 0.0500 (0.0500)  time: 0.6091  data: 0.0244  max mem: 13008
Epoch: [8]  [2280/3542]  eta: 0:16:57  lr: 0.000003  min_lr: 0.000003  loss: 1.8135 (1.8501)  loss_scale: 8192.0000 (10907.1039)  weight_decay: 0.0500 (0.0500)  time: 0.6147  data: 0.0298  max mem: 13008
Epoch: [8]  [2290/3542]  eta: 0:16:48  lr: 0.000003  min_lr: 0.000003  loss: 1.7568 (1.8502)  loss_scale: 8192.0000 (10895.2527)  weight_decay: 0.0500 (0.0500)  time: 0.6183  data: 0.0322  max mem: 13008
Epoch: [8]  [2300/3542]  eta: 0:16:38  lr: 0.000003  min_lr: 0.000003  loss: 1.7568 (1.8499)  loss_scale: 8192.0000 (10883.5046)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0268  max mem: 13008
Epoch: [8]  [2310/3542]  eta: 0:16:30  lr: 0.000003  min_lr: 0.000003  loss: 1.7881 (1.8499)  loss_scale: 8192.0000 (10871.8581)  weight_decay: 0.0500 (0.0500)  time: 0.6449  data: 0.0581  max mem: 13008
Epoch: [8]  [2320/3542]  eta: 0:16:21  lr: 0.000003  min_lr: 0.000003  loss: 1.7910 (1.8497)  loss_scale: 8192.0000 (10860.3119)  weight_decay: 0.0500 (0.0500)  time: 0.6555  data: 0.0693  max mem: 13008
Epoch: [8]  [2330/3542]  eta: 0:16:12  lr: 0.000003  min_lr: 0.000003  loss: 1.8135 (1.8499)  loss_scale: 8192.0000 (10848.8649)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.0421  max mem: 13008
Epoch: [8]  [2340/3542]  eta: 0:16:03  lr: 0.000003  min_lr: 0.000003  loss: 1.8252 (1.8500)  loss_scale: 8192.0000 (10837.5156)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.0644  max mem: 13008
Epoch: [8]  [2350/3542]  eta: 0:15:54  lr: 0.000003  min_lr: 0.000003  loss: 1.8066 (1.8497)  loss_scale: 8192.0000 (10826.2629)  weight_decay: 0.0500 (0.0500)  time: 0.6442  data: 0.0590  max mem: 13008
Epoch: [8]  [2360/3542]  eta: 0:15:45  lr: 0.000003  min_lr: 0.000003  loss: 1.8037 (1.8496)  loss_scale: 8192.0000 (10815.1055)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.0333  max mem: 13008
Epoch: [8]  [2370/3542]  eta: 0:15:36  lr: 0.000003  min_lr: 0.000003  loss: 1.8301 (1.8496)  loss_scale: 8192.0000 (10804.0422)  weight_decay: 0.0500 (0.0500)  time: 0.5929  data: 0.0082  max mem: 13008
Epoch: [8]  [2380/3542]  eta: 0:15:27  lr: 0.000003  min_lr: 0.000003  loss: 1.8223 (1.8495)  loss_scale: 8192.0000 (10793.0718)  weight_decay: 0.0500 (0.0500)  time: 0.5928  data: 0.0076  max mem: 13008
Epoch: [8]  [2390/3542]  eta: 0:15:18  lr: 0.000003  min_lr: 0.000003  loss: 1.8174 (1.8496)  loss_scale: 8192.0000 (10782.1932)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0157  max mem: 13008
Epoch: [8]  [2400/3542]  eta: 0:15:09  lr: 0.000003  min_lr: 0.000003  loss: 1.8193 (1.8494)  loss_scale: 8192.0000 (10771.4052)  weight_decay: 0.0500 (0.0500)  time: 0.6115  data: 0.0269  max mem: 13008
Epoch: [8]  [2410/3542]  eta: 0:15:01  lr: 0.000003  min_lr: 0.000003  loss: 1.8213 (1.8495)  loss_scale: 8192.0000 (10760.7068)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0259  max mem: 13008
Epoch: [8]  [2420/3542]  eta: 0:14:52  lr: 0.000003  min_lr: 0.000003  loss: 1.8506 (1.8494)  loss_scale: 8192.0000 (10750.0967)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0233  max mem: 13008
Epoch: [8]  [2430/3542]  eta: 0:14:43  lr: 0.000003  min_lr: 0.000003  loss: 1.8486 (1.8494)  loss_scale: 8192.0000 (10739.5738)  weight_decay: 0.0500 (0.0500)  time: 0.6022  data: 0.0162  max mem: 13008
Epoch: [8]  [2440/3542]  eta: 0:14:34  lr: 0.000003  min_lr: 0.000003  loss: 1.9463 (1.8499)  loss_scale: 8192.0000 (10729.1372)  weight_decay: 0.0500 (0.0500)  time: 0.5943  data: 0.0083  max mem: 13008
Epoch: [8]  [2450/3542]  eta: 0:14:25  lr: 0.000003  min_lr: 0.000003  loss: 1.9492 (1.8504)  loss_scale: 8192.0000 (10718.7858)  weight_decay: 0.0500 (0.0500)  time: 0.5941  data: 0.0084  max mem: 13008
Epoch: [8]  [2460/3542]  eta: 0:14:16  lr: 0.000003  min_lr: 0.000003  loss: 1.8838 (1.8503)  loss_scale: 8192.0000 (10708.5185)  weight_decay: 0.0500 (0.0500)  time: 0.5869  data: 0.0010  max mem: 13008
Epoch: [8]  [2470/3542]  eta: 0:14:08  lr: 0.000003  min_lr: 0.000003  loss: 1.8604 (1.8504)  loss_scale: 8192.0000 (10698.3343)  weight_decay: 0.0500 (0.0500)  time: 0.5882  data: 0.0028  max mem: 13008
Epoch: [8]  [2480/3542]  eta: 0:13:59  lr: 0.000003  min_lr: 0.000003  loss: 1.8604 (1.8503)  loss_scale: 8192.0000 (10688.2322)  weight_decay: 0.0500 (0.0500)  time: 0.6116  data: 0.0261  max mem: 13008
Epoch: [8]  [2490/3542]  eta: 0:13:50  lr: 0.000003  min_lr: 0.000003  loss: 1.8291 (1.8504)  loss_scale: 8192.0000 (10678.2112)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0243  max mem: 13008
Epoch: [8]  [2500/3542]  eta: 0:13:41  lr: 0.000003  min_lr: 0.000003  loss: 1.8076 (1.8500)  loss_scale: 8192.0000 (10668.2703)  weight_decay: 0.0500 (0.0500)  time: 0.5858  data: 0.0003  max mem: 13008
Epoch: [8]  [2510/3542]  eta: 0:13:33  lr: 0.000003  min_lr: 0.000003  loss: 1.8447 (1.8504)  loss_scale: 8192.0000 (10658.4086)  weight_decay: 0.0500 (0.0500)  time: 0.5877  data: 0.0011  max mem: 13008
Epoch: [8]  [2520/3542]  eta: 0:13:24  lr: 0.000003  min_lr: 0.000003  loss: 1.8223 (1.8501)  loss_scale: 8192.0000 (10648.6251)  weight_decay: 0.0500 (0.0500)  time: 0.6146  data: 0.0290  max mem: 13008
Epoch: [8]  [2530/3542]  eta: 0:13:16  lr: 0.000003  min_lr: 0.000003  loss: 1.8223 (1.8504)  loss_scale: 8192.0000 (10638.9190)  weight_decay: 0.0500 (0.0500)  time: 0.6320  data: 0.0473  max mem: 13008
Epoch: [8]  [2540/3542]  eta: 0:13:07  lr: 0.000003  min_lr: 0.000003  loss: 1.9199 (1.8507)  loss_scale: 8192.0000 (10629.2893)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0219  max mem: 13008
Epoch: [8]  [2550/3542]  eta: 0:12:59  lr: 0.000003  min_lr: 0.000003  loss: 1.8066 (1.8506)  loss_scale: 8192.0000 (10619.7350)  weight_decay: 0.0500 (0.0500)  time: 0.6019  data: 0.0160  max mem: 13008
Epoch: [8]  [2560/3542]  eta: 0:12:50  lr: 0.000003  min_lr: 0.000003  loss: 1.8232 (1.8507)  loss_scale: 8192.0000 (10610.2554)  weight_decay: 0.0500 (0.0500)  time: 0.6201  data: 0.0345  max mem: 13008
Epoch: [8]  [2570/3542]  eta: 0:12:41  lr: 0.000003  min_lr: 0.000003  loss: 1.8545 (1.8507)  loss_scale: 8192.0000 (10600.8495)  weight_decay: 0.0500 (0.0500)  time: 0.6073  data: 0.0220  max mem: 13008
Epoch: [8]  [2580/3542]  eta: 0:12:33  lr: 0.000003  min_lr: 0.000003  loss: 1.8066 (1.8505)  loss_scale: 8192.0000 (10591.5165)  weight_decay: 0.0500 (0.0500)  time: 0.5894  data: 0.0045  max mem: 13008
Epoch: [8]  [2590/3542]  eta: 0:12:24  lr: 0.000003  min_lr: 0.000003  loss: 1.8096 (1.8507)  loss_scale: 8192.0000 (10582.2555)  weight_decay: 0.0500 (0.0500)  time: 0.5977  data: 0.0122  max mem: 13008
Epoch: [8]  [2600/3542]  eta: 0:12:16  lr: 0.000003  min_lr: 0.000003  loss: 1.8516 (1.8506)  loss_scale: 8192.0000 (10573.0657)  weight_decay: 0.0500 (0.0500)  time: 0.6165  data: 0.0315  max mem: 13008
Epoch: [8]  [2610/3542]  eta: 0:12:08  lr: 0.000003  min_lr: 0.000003  loss: 1.8516 (1.8508)  loss_scale: 8192.0000 (10563.9464)  weight_decay: 0.0500 (0.0500)  time: 0.6361  data: 0.0514  max mem: 13008
Epoch: [8]  [2620/3542]  eta: 0:11:59  lr: 0.000003  min_lr: 0.000003  loss: 1.8584 (1.8507)  loss_scale: 8192.0000 (10554.8966)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0302  max mem: 13008
Epoch: [8]  [2630/3542]  eta: 0:11:51  lr: 0.000003  min_lr: 0.000003  loss: 1.8242 (1.8506)  loss_scale: 8192.0000 (10545.9156)  weight_decay: 0.0500 (0.0500)  time: 0.6001  data: 0.0145  max mem: 13008
Epoch: [8]  [2640/3542]  eta: 0:11:43  lr: 0.000003  min_lr: 0.000003  loss: 1.7998 (1.8507)  loss_scale: 8192.0000 (10537.0027)  weight_decay: 0.0500 (0.0500)  time: 0.6148  data: 0.0296  max mem: 13008
Epoch: [8]  [2650/3542]  eta: 0:11:34  lr: 0.000003  min_lr: 0.000003  loss: 1.7334 (1.8504)  loss_scale: 8192.0000 (10528.1569)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.0471  max mem: 13008
Epoch: [8]  [2660/3542]  eta: 0:11:26  lr: 0.000003  min_lr: 0.000003  loss: 1.8086 (1.8508)  loss_scale: 8192.0000 (10519.3777)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.0555  max mem: 13008
[2023-05-16 13:15:40,467] [INFO] [logging.py:60:log_dist] [Rank 0] step=31000, skipped=28, lr=[2.8217227381343086e-06, 2.8217227381343086e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 13:15:40,470] [INFO] [timer.py:157:stop] 0/31000, SamplesPerSec=55.55872688482278
Epoch: [8]  [2670/3542]  eta: 0:11:18  lr: 0.000003  min_lr: 0.000003  loss: 1.8447 (1.8507)  loss_scale: 8192.0000 (10510.6642)  weight_decay: 0.0500 (0.0500)  time: 0.6392  data: 0.0549  max mem: 13008
Epoch: [8]  [2680/3542]  eta: 0:11:09  lr: 0.000003  min_lr: 0.000003  loss: 1.8193 (1.8508)  loss_scale: 8192.0000 (10502.0157)  weight_decay: 0.0500 (0.0500)  time: 0.6150  data: 0.0300  max mem: 13008
Epoch: [8]  [2690/3542]  eta: 0:11:01  lr: 0.000003  min_lr: 0.000003  loss: 1.8535 (1.8509)  loss_scale: 8192.0000 (10493.4314)  weight_decay: 0.0500 (0.0500)  time: 0.5973  data: 0.0114  max mem: 13008
Epoch: [8]  [2700/3542]  eta: 0:10:53  lr: 0.000003  min_lr: 0.000003  loss: 1.9092 (1.8512)  loss_scale: 8192.0000 (10484.9108)  weight_decay: 0.0500 (0.0500)  time: 0.6590  data: 0.0727  max mem: 13008
Epoch: [8]  [2710/3542]  eta: 0:10:45  lr: 0.000003  min_lr: 0.000003  loss: 1.8984 (1.8513)  loss_scale: 8192.0000 (10476.4530)  weight_decay: 0.0500 (0.0500)  time: 0.6726  data: 0.0871  max mem: 13008
Epoch: [8]  [2720/3542]  eta: 0:10:37  lr: 0.000003  min_lr: 0.000003  loss: 1.8848 (1.8515)  loss_scale: 8192.0000 (10468.0573)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0576  max mem: 13008
Epoch: [8]  [2730/3542]  eta: 0:10:29  lr: 0.000003  min_lr: 0.000003  loss: 1.9150 (1.8517)  loss_scale: 8192.0000 (10459.7232)  weight_decay: 0.0500 (0.0500)  time: 0.6601  data: 0.0755  max mem: 13008
Epoch: [8]  [2740/3542]  eta: 0:10:20  lr: 0.000003  min_lr: 0.000003  loss: 1.8711 (1.8519)  loss_scale: 8192.0000 (10451.4498)  weight_decay: 0.0500 (0.0500)  time: 0.6286  data: 0.0438  max mem: 13008
Epoch: [8]  [2750/3542]  eta: 0:10:12  lr: 0.000003  min_lr: 0.000003  loss: 1.8574 (1.8517)  loss_scale: 8192.0000 (10443.2366)  weight_decay: 0.0500 (0.0500)  time: 0.6056  data: 0.0199  max mem: 13008
Epoch: [8]  [2760/3542]  eta: 0:10:04  lr: 0.000003  min_lr: 0.000003  loss: 1.8027 (1.8517)  loss_scale: 8192.0000 (10435.0829)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.0600  max mem: 13008
Epoch: [8]  [2770/3542]  eta: 0:09:56  lr: 0.000003  min_lr: 0.000003  loss: 1.8252 (1.8518)  loss_scale: 8192.0000 (10426.9881)  weight_decay: 0.0500 (0.0500)  time: 0.6491  data: 0.0630  max mem: 13008
Epoch: [8]  [2780/3542]  eta: 0:09:48  lr: 0.000003  min_lr: 0.000003  loss: 1.7793 (1.8514)  loss_scale: 8192.0000 (10418.9515)  weight_decay: 0.0500 (0.0500)  time: 0.6552  data: 0.0681  max mem: 13008
Epoch: [8]  [2790/3542]  eta: 0:09:40  lr: 0.000003  min_lr: 0.000003  loss: 1.7852 (1.8513)  loss_scale: 8192.0000 (10410.9724)  weight_decay: 0.0500 (0.0500)  time: 0.6876  data: 0.1011  max mem: 13008
Epoch: [8]  [2800/3542]  eta: 0:09:32  lr: 0.000003  min_lr: 0.000003  loss: 1.8555 (1.8515)  loss_scale: 8192.0000 (10403.0503)  weight_decay: 0.0500 (0.0500)  time: 0.6635  data: 0.0783  max mem: 13008
Epoch: [8]  [2810/3542]  eta: 0:09:24  lr: 0.000003  min_lr: 0.000003  loss: 1.8613 (1.8514)  loss_scale: 8192.0000 (10395.1846)  weight_decay: 0.0500 (0.0500)  time: 0.6244  data: 0.0396  max mem: 13008
Epoch: [8]  [2820/3542]  eta: 0:09:16  lr: 0.000003  min_lr: 0.000003  loss: 1.7686 (1.8511)  loss_scale: 8192.0000 (10387.3747)  weight_decay: 0.0500 (0.0500)  time: 0.6246  data: 0.0395  max mem: 13008
Epoch: [8]  [2830/3542]  eta: 0:09:08  lr: 0.000003  min_lr: 0.000003  loss: 1.8115 (1.8512)  loss_scale: 8192.0000 (10379.6199)  weight_decay: 0.0500 (0.0500)  time: 0.6314  data: 0.0456  max mem: 13008
Epoch: [8]  [2840/3542]  eta: 0:09:00  lr: 0.000003  min_lr: 0.000003  loss: 1.8838 (1.8512)  loss_scale: 8192.0000 (10371.9197)  weight_decay: 0.0500 (0.0500)  time: 0.6551  data: 0.0689  max mem: 13008
Epoch: [8]  [2850/3542]  eta: 0:08:52  lr: 0.000003  min_lr: 0.000003  loss: 1.8232 (1.8509)  loss_scale: 8192.0000 (10364.2736)  weight_decay: 0.0500 (0.0500)  time: 0.6714  data: 0.0859  max mem: 13008
Epoch: [8]  [2860/3542]  eta: 0:08:44  lr: 0.000003  min_lr: 0.000003  loss: 1.7949 (1.8508)  loss_scale: 8192.0000 (10356.6809)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.0549  max mem: 13008
Epoch: [8]  [2870/3542]  eta: 0:08:36  lr: 0.000003  min_lr: 0.000003  loss: 1.8428 (1.8508)  loss_scale: 8192.0000 (10349.1411)  weight_decay: 0.0500 (0.0500)  time: 0.6506  data: 0.0654  max mem: 13008
Epoch: [8]  [2880/3542]  eta: 0:08:28  lr: 0.000003  min_lr: 0.000003  loss: 1.8672 (1.8507)  loss_scale: 8192.0000 (10341.6536)  weight_decay: 0.0500 (0.0500)  time: 0.6543  data: 0.0690  max mem: 13008
Epoch: [8]  [2890/3542]  eta: 0:08:20  lr: 0.000003  min_lr: 0.000003  loss: 1.8672 (1.8508)  loss_scale: 8192.0000 (10334.2179)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.0504  max mem: 13008
Epoch: [8]  [2900/3542]  eta: 0:08:12  lr: 0.000003  min_lr: 0.000003  loss: 1.8467 (1.8509)  loss_scale: 8192.0000 (10326.8335)  weight_decay: 0.0500 (0.0500)  time: 0.6619  data: 0.0771  max mem: 13008
Epoch: [8]  [2910/3542]  eta: 0:08:04  lr: 0.000003  min_lr: 0.000003  loss: 1.8047 (1.8506)  loss_scale: 8192.0000 (10319.4998)  weight_decay: 0.0500 (0.0500)  time: 0.6828  data: 0.0986  max mem: 13008
Epoch: [8]  [2920/3542]  eta: 0:07:56  lr: 0.000003  min_lr: 0.000003  loss: 1.7852 (1.8505)  loss_scale: 8192.0000 (10312.2164)  weight_decay: 0.0500 (0.0500)  time: 0.6674  data: 0.0817  max mem: 13008
Epoch: [8]  [2930/3542]  eta: 0:07:49  lr: 0.000003  min_lr: 0.000003  loss: 1.8252 (1.8505)  loss_scale: 8192.0000 (10304.9826)  weight_decay: 0.0500 (0.0500)  time: 0.6703  data: 0.0852  max mem: 13008
Epoch: [8]  [2940/3542]  eta: 0:07:41  lr: 0.000003  min_lr: 0.000003  loss: 1.8320 (1.8505)  loss_scale: 8192.0000 (10297.7980)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.0567  max mem: 13008
Epoch: [8]  [2950/3542]  eta: 0:07:33  lr: 0.000003  min_lr: 0.000003  loss: 1.8252 (1.8506)  loss_scale: 8192.0000 (10290.6621)  weight_decay: 0.0500 (0.0500)  time: 0.6622  data: 0.0768  max mem: 13008
Epoch: [8]  [2960/3542]  eta: 0:07:25  lr: 0.000003  min_lr: 0.000003  loss: 1.8613 (1.8508)  loss_scale: 8192.0000 (10283.5745)  weight_decay: 0.0500 (0.0500)  time: 0.6737  data: 0.0887  max mem: 13008
Epoch: [8]  [2970/3542]  eta: 0:07:17  lr: 0.000003  min_lr: 0.000003  loss: 1.8613 (1.8507)  loss_scale: 8192.0000 (10276.5345)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0147  max mem: 13008
Epoch: [8]  [2980/3542]  eta: 0:07:09  lr: 0.000003  min_lr: 0.000003  loss: 1.8486 (1.8508)  loss_scale: 8192.0000 (10269.5418)  weight_decay: 0.0500 (0.0500)  time: 0.6780  data: 0.0927  max mem: 13008
Epoch: [8]  [2990/3542]  eta: 0:07:01  lr: 0.000003  min_lr: 0.000003  loss: 1.8320 (1.8507)  loss_scale: 8192.0000 (10262.5958)  weight_decay: 0.0500 (0.0500)  time: 0.7278  data: 0.1431  max mem: 13008
Epoch: [8]  [3000/3542]  eta: 0:06:54  lr: 0.000003  min_lr: 0.000003  loss: 1.7979 (1.8507)  loss_scale: 8192.0000 (10255.6961)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0732  max mem: 13008
Epoch: [8]  [3010/3542]  eta: 0:06:46  lr: 0.000003  min_lr: 0.000003  loss: 1.7783 (1.8506)  loss_scale: 8192.0000 (10248.8422)  weight_decay: 0.0500 (0.0500)  time: 0.6534  data: 0.0674  max mem: 13008
Epoch: [8]  [3020/3542]  eta: 0:06:38  lr: 0.000003  min_lr: 0.000003  loss: 1.7783 (1.8504)  loss_scale: 8192.0000 (10242.0338)  weight_decay: 0.0500 (0.0500)  time: 0.6583  data: 0.0732  max mem: 13008
Epoch: [8]  [3030/3542]  eta: 0:06:30  lr: 0.000003  min_lr: 0.000003  loss: 1.8105 (1.8507)  loss_scale: 8192.0000 (10235.2702)  weight_decay: 0.0500 (0.0500)  time: 0.6605  data: 0.0757  max mem: 13008
Epoch: [8]  [3040/3542]  eta: 0:06:22  lr: 0.000003  min_lr: 0.000003  loss: 1.8506 (1.8506)  loss_scale: 8192.0000 (10228.5511)  weight_decay: 0.0500 (0.0500)  time: 0.6783  data: 0.0928  max mem: 13008
Epoch: [8]  [3050/3542]  eta: 0:06:15  lr: 0.000003  min_lr: 0.000003  loss: 1.8506 (1.8506)  loss_scale: 8192.0000 (10221.8761)  weight_decay: 0.0500 (0.0500)  time: 0.6973  data: 0.1124  max mem: 13008
[2023-05-16 13:19:58,177] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 13:19:58,177] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [8]  [3060/3542]  eta: 0:06:07  lr: 0.000003  min_lr: 0.000003  loss: 1.8682 (1.8506)  loss_scale: 8192.0000 (10228.6259)  weight_decay: 0.0500 (0.0500)  time: 0.6878  data: 0.1030  max mem: 13008
Epoch: [8]  [3070/3542]  eta: 0:05:59  lr: 0.000003  min_lr: 0.000003  loss: 1.7812 (1.8503)  loss_scale: 16384.0000 (10248.6695)  weight_decay: 0.0500 (0.0500)  time: 0.6696  data: 0.0850  max mem: 13008
Epoch: [8]  [3080/3542]  eta: 0:05:51  lr: 0.000002  min_lr: 0.000002  loss: 1.8252 (1.8505)  loss_scale: 16384.0000 (10268.5829)  weight_decay: 0.0500 (0.0500)  time: 0.6857  data: 0.1010  max mem: 13008
Epoch: [8]  [3090/3542]  eta: 0:05:44  lr: 0.000002  min_lr: 0.000002  loss: 1.8701 (1.8506)  loss_scale: 16384.0000 (10288.3675)  weight_decay: 0.0500 (0.0500)  time: 0.6584  data: 0.0737  max mem: 13008
Epoch: [8]  [3100/3542]  eta: 0:05:36  lr: 0.000002  min_lr: 0.000002  loss: 1.8350 (1.8507)  loss_scale: 16384.0000 (10308.0245)  weight_decay: 0.0500 (0.0500)  time: 0.6155  data: 0.0302  max mem: 13008
Epoch: [8]  [3110/3542]  eta: 0:05:28  lr: 0.000002  min_lr: 0.000002  loss: 1.8154 (1.8507)  loss_scale: 16384.0000 (10327.5551)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0225  max mem: 13008
Epoch: [8]  [3120/3542]  eta: 0:05:20  lr: 0.000002  min_lr: 0.000002  loss: 1.8193 (1.8508)  loss_scale: 16384.0000 (10346.9606)  weight_decay: 0.0500 (0.0500)  time: 0.6358  data: 0.0510  max mem: 13008
Epoch: [8]  [3130/3542]  eta: 0:05:12  lr: 0.000002  min_lr: 0.000002  loss: 1.8564 (1.8508)  loss_scale: 16384.0000 (10366.2421)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.0515  max mem: 13008
Epoch: [8]  [3140/3542]  eta: 0:05:05  lr: 0.000002  min_lr: 0.000002  loss: 1.8564 (1.8508)  loss_scale: 16384.0000 (10385.4008)  weight_decay: 0.0500 (0.0500)  time: 0.6468  data: 0.0622  max mem: 13008
[2023-05-16 13:20:56,036] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 31482
[2023-05-16 13:20:56,036] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 13:20:56,036] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [8]  [3150/3542]  eta: 0:04:57  lr: 0.000002  min_lr: 0.000002  loss: 1.9014 (1.8513)  loss_scale: 16384.0000 (10391.4389)  weight_decay: 0.0500 (0.0500)  time: 0.6456  data: 0.0620  max mem: 13008
Epoch: [8]  [3160/3542]  eta: 0:04:49  lr: 0.000002  min_lr: 0.000002  loss: 1.9150 (1.8512)  loss_scale: 8192.0000 (10384.4809)  weight_decay: 0.0500 (0.0500)  time: 0.6271  data: 0.0425  max mem: 13008
Epoch: [8]  [3170/3542]  eta: 0:04:41  lr: 0.000002  min_lr: 0.000002  loss: 1.8184 (1.8512)  loss_scale: 8192.0000 (10377.5667)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0427  max mem: 13008
Epoch: [8]  [3180/3542]  eta: 0:04:34  lr: 0.000002  min_lr: 0.000002  loss: 1.8184 (1.8511)  loss_scale: 8192.0000 (10370.6960)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.0405  max mem: 13008
Epoch: [8]  [3190/3542]  eta: 0:04:26  lr: 0.000002  min_lr: 0.000002  loss: 1.7803 (1.8510)  loss_scale: 8192.0000 (10363.8684)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0439  max mem: 13008
Epoch: [8]  [3200/3542]  eta: 0:04:18  lr: 0.000002  min_lr: 0.000002  loss: 1.7803 (1.8510)  loss_scale: 8192.0000 (10357.0834)  weight_decay: 0.0500 (0.0500)  time: 0.6099  data: 0.0246  max mem: 13008
Epoch: [8]  [3210/3542]  eta: 0:04:11  lr: 0.000002  min_lr: 0.000002  loss: 1.8564 (1.8508)  loss_scale: 8192.0000 (10350.3407)  weight_decay: 0.0500 (0.0500)  time: 0.6036  data: 0.0185  max mem: 13008
Epoch: [8]  [3220/3542]  eta: 0:04:03  lr: 0.000002  min_lr: 0.000002  loss: 1.8076 (1.8508)  loss_scale: 8192.0000 (10343.6399)  weight_decay: 0.0500 (0.0500)  time: 0.6233  data: 0.0387  max mem: 13008
Epoch: [8]  [3230/3542]  eta: 0:03:55  lr: 0.000002  min_lr: 0.000002  loss: 1.7520 (1.8505)  loss_scale: 8192.0000 (10336.9805)  weight_decay: 0.0500 (0.0500)  time: 0.6369  data: 0.0516  max mem: 13008
Epoch: [8]  [3240/3542]  eta: 0:03:48  lr: 0.000002  min_lr: 0.000002  loss: 1.7490 (1.8503)  loss_scale: 8192.0000 (10330.3622)  weight_decay: 0.0500 (0.0500)  time: 0.6604  data: 0.0746  max mem: 13008
Epoch: [8]  [3250/3542]  eta: 0:03:40  lr: 0.000002  min_lr: 0.000002  loss: 1.7480 (1.8501)  loss_scale: 8192.0000 (10323.7847)  weight_decay: 0.0500 (0.0500)  time: 0.6580  data: 0.0731  max mem: 13008
Epoch: [8]  [3260/3542]  eta: 0:03:32  lr: 0.000002  min_lr: 0.000002  loss: 1.7178 (1.8498)  loss_scale: 8192.0000 (10317.2475)  weight_decay: 0.0500 (0.0500)  time: 0.6408  data: 0.0559  max mem: 13008
Epoch: [8]  [3270/3542]  eta: 0:03:25  lr: 0.000002  min_lr: 0.000002  loss: 1.8145 (1.8499)  loss_scale: 8192.0000 (10310.7502)  weight_decay: 0.0500 (0.0500)  time: 0.6160  data: 0.0305  max mem: 13008
Epoch: [8]  [3280/3542]  eta: 0:03:17  lr: 0.000002  min_lr: 0.000002  loss: 1.8164 (1.8496)  loss_scale: 8192.0000 (10304.2926)  weight_decay: 0.0500 (0.0500)  time: 0.6027  data: 0.0174  max mem: 13008
Epoch: [8]  [3290/3542]  eta: 0:03:09  lr: 0.000002  min_lr: 0.000002  loss: 1.7549 (1.8494)  loss_scale: 8192.0000 (10297.8742)  weight_decay: 0.0500 (0.0500)  time: 0.6333  data: 0.0482  max mem: 13008
Epoch: [8]  [3300/3542]  eta: 0:03:02  lr: 0.000002  min_lr: 0.000002  loss: 1.8125 (1.8495)  loss_scale: 8192.0000 (10291.4947)  weight_decay: 0.0500 (0.0500)  time: 0.6167  data: 0.0311  max mem: 13008
Epoch: [8]  [3310/3542]  eta: 0:02:54  lr: 0.000002  min_lr: 0.000002  loss: 1.8125 (1.8494)  loss_scale: 8192.0000 (10285.1537)  weight_decay: 0.0500 (0.0500)  time: 0.6215  data: 0.0363  max mem: 13008
Epoch: [8]  [3320/3542]  eta: 0:02:46  lr: 0.000002  min_lr: 0.000002  loss: 1.7871 (1.8493)  loss_scale: 8192.0000 (10278.8509)  weight_decay: 0.0500 (0.0500)  time: 0.6229  data: 0.0383  max mem: 13008
Epoch: [8]  [3330/3542]  eta: 0:02:39  lr: 0.000002  min_lr: 0.000002  loss: 1.8223 (1.8495)  loss_scale: 8192.0000 (10272.5860)  weight_decay: 0.0500 (0.0500)  time: 0.6025  data: 0.0178  max mem: 13008
Epoch: [8]  [3340/3542]  eta: 0:02:31  lr: 0.000002  min_lr: 0.000002  loss: 1.8857 (1.8496)  loss_scale: 8192.0000 (10266.3586)  weight_decay: 0.0500 (0.0500)  time: 0.6154  data: 0.0306  max mem: 13008
Epoch: [8]  [3350/3542]  eta: 0:02:24  lr: 0.000002  min_lr: 0.000002  loss: 1.8457 (1.8494)  loss_scale: 8192.0000 (10260.1683)  weight_decay: 0.0500 (0.0500)  time: 0.6312  data: 0.0465  max mem: 13008
Epoch: [8]  [3360/3542]  eta: 0:02:16  lr: 0.000002  min_lr: 0.000002  loss: 1.7539 (1.8491)  loss_scale: 8192.0000 (10254.0149)  weight_decay: 0.0500 (0.0500)  time: 0.6425  data: 0.0587  max mem: 13008
Epoch: [8]  [3370/3542]  eta: 0:02:09  lr: 0.000002  min_lr: 0.000002  loss: 1.7832 (1.8491)  loss_scale: 8192.0000 (10247.8980)  weight_decay: 0.0500 (0.0500)  time: 0.6335  data: 0.0489  max mem: 13008
Epoch: [8]  [3380/3542]  eta: 0:02:01  lr: 0.000002  min_lr: 0.000002  loss: 1.7607 (1.8487)  loss_scale: 8192.0000 (10241.8172)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.0420  max mem: 13008
Epoch: [8]  [3390/3542]  eta: 0:01:53  lr: 0.000002  min_lr: 0.000002  loss: 1.8115 (1.8489)  loss_scale: 8192.0000 (10235.7723)  weight_decay: 0.0500 (0.0500)  time: 0.6074  data: 0.0211  max mem: 13008
Epoch: [8]  [3400/3542]  eta: 0:01:46  lr: 0.000002  min_lr: 0.000002  loss: 1.9014 (1.8490)  loss_scale: 8192.0000 (10229.7630)  weight_decay: 0.0500 (0.0500)  time: 0.6294  data: 0.0440  max mem: 13008
Epoch: [8]  [3410/3542]  eta: 0:01:38  lr: 0.000002  min_lr: 0.000002  loss: 1.8379 (1.8489)  loss_scale: 8192.0000 (10223.7889)  weight_decay: 0.0500 (0.0500)  time: 0.6753  data: 0.0902  max mem: 13008
Epoch: [8]  [3420/3542]  eta: 0:01:31  lr: 0.000002  min_lr: 0.000002  loss: 1.8311 (1.8489)  loss_scale: 8192.0000 (10217.8498)  weight_decay: 0.0500 (0.0500)  time: 0.6689  data: 0.0846  max mem: 13008
Epoch: [8]  [3430/3542]  eta: 0:01:23  lr: 0.000002  min_lr: 0.000002  loss: 1.8311 (1.8488)  loss_scale: 8192.0000 (10211.9452)  weight_decay: 0.0500 (0.0500)  time: 0.6668  data: 0.0821  max mem: 13008
Epoch: [8]  [3440/3542]  eta: 0:01:16  lr: 0.000002  min_lr: 0.000002  loss: 1.7734 (1.8488)  loss_scale: 8192.0000 (10206.0750)  weight_decay: 0.0500 (0.0500)  time: 0.6568  data: 0.0717  max mem: 13008
Epoch: [8]  [3450/3542]  eta: 0:01:08  lr: 0.000002  min_lr: 0.000002  loss: 1.8750 (1.8490)  loss_scale: 8192.0000 (10200.2388)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.0491  max mem: 13008
Epoch: [8]  [3460/3542]  eta: 0:01:01  lr: 0.000002  min_lr: 0.000002  loss: 1.7793 (1.8487)  loss_scale: 8192.0000 (10194.4363)  weight_decay: 0.0500 (0.0500)  time: 0.6762  data: 0.0917  max mem: 13008
Epoch: [8]  [3470/3542]  eta: 0:00:53  lr: 0.000002  min_lr: 0.000002  loss: 1.7617 (1.8487)  loss_scale: 8192.0000 (10188.6672)  weight_decay: 0.0500 (0.0500)  time: 0.6826  data: 0.0979  max mem: 13008
Epoch: [8]  [3480/3542]  eta: 0:00:46  lr: 0.000002  min_lr: 0.000002  loss: 1.7822 (1.8486)  loss_scale: 8192.0000 (10182.9313)  weight_decay: 0.0500 (0.0500)  time: 0.6421  data: 0.0579  max mem: 13008
Epoch: [8]  [3490/3542]  eta: 0:00:38  lr: 0.000002  min_lr: 0.000002  loss: 1.8154 (1.8486)  loss_scale: 8192.0000 (10177.2283)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0447  max mem: 13008
Epoch: [8]  [3500/3542]  eta: 0:00:31  lr: 0.000002  min_lr: 0.000002  loss: 1.8291 (1.8486)  loss_scale: 8192.0000 (10171.5578)  weight_decay: 0.0500 (0.0500)  time: 0.6006  data: 0.0151  max mem: 13008
Epoch: [8]  [3510/3542]  eta: 0:00:23  lr: 0.000002  min_lr: 0.000002  loss: 1.7979 (1.8485)  loss_scale: 8192.0000 (10165.9197)  weight_decay: 0.0500 (0.0500)  time: 0.6132  data: 0.0286  max mem: 13008
Epoch: [8]  [3520/3542]  eta: 0:00:16  lr: 0.000002  min_lr: 0.000002  loss: 1.7607 (1.8482)  loss_scale: 8192.0000 (10160.3135)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.0605  max mem: 13008
Epoch: [8]  [3530/3542]  eta: 0:00:08  lr: 0.000002  min_lr: 0.000002  loss: 1.8154 (1.8483)  loss_scale: 8192.0000 (10154.7392)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0381  max mem: 13008
Epoch: [8]  [3540/3542]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000002  loss: 1.8564 (1.8484)  loss_scale: 8192.0000 (10149.1963)  weight_decay: 0.0500 (0.0500)  time: 0.5895  data: 0.0061  max mem: 13008
Epoch: [8]  [3541/3542]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000002  loss: 1.8564 (1.8484)  loss_scale: 8192.0000 (10148.6437)  weight_decay: 0.0500 (0.0500)  time: 0.5891  data: 0.0061  max mem: 13008
Epoch: [8] Total time: 0:43:58 (0.7450 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000002  loss: 1.8564 (1.8484)  loss_scale: 8192.0000 (10148.6437)  weight_decay: 0.0500 (0.0500)
[2023-05-16 13:25:05,878] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-8/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:32:28    time: 12.4927  data: 9.1248  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:10:24    time: 4.2792  data: 0.8297  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 20/156]  eta: 0:08:55    time: 3.5096  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 30/156]  eta: 0:07:59    time: 3.5391  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 40/156]  eta: 0:07:14    time: 3.5414  data: 0.0002  max mem: 13008
Test:  [ 50/156]  eta: 0:06:35    time: 3.6157  data: 0.0002  max mem: 13008
Test:  [ 60/156]  eta: 0:05:54    time: 3.5989  data: 0.0002  max mem: 13008
Test:  [ 70/156]  eta: 0:05:16    time: 3.5660  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:38    time: 3.5493  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 90/156]  eta: 0:04:00    time: 3.4922  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:23    time: 3.5354  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:47    time: 3.6324  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:10    time: 3.6490  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:34    time: 3.6369  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:00:58    time: 3.5577  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:21    time: 3.5027  data: 0.0001  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.4660  data: 0.0001  max mem: 13008
Test: Total time: 0:09:24 (3.6196 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e8.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e8.json and result file is ./output_freeze/coco_captioning_result_val_e8.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e8.json
loading annotations into memory...
Done (t=0.03s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1339248.88 tokens per second.
PTBTokenizer tokenized 61829 tokens at 470799.77 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 50867, 'reflen': 49154, 'guess': [50867, 45875, 40883, 35891], 'correct': [33581, 17338, 7816, 3217]}
ratio: 1.0348496561825888
Bleu_1: 0.660
Bleu_2: 0.500
Bleu_3: 0.363
Bleu_4: 0.256
computing METEOR score...
METEOR: 0.240
computing Rouge score...
ROUGE_L: 0.509
computing CIDEr score...
CIDEr: 0.827
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].
Threads( StanfordCoreNLP ) [15.777 seconds]
SPICE evaluation took: 26.82 s
SPICE: 0.176
Performance of the network on the 5000 val images: 0.8%
Max performance: 0.83%
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [9]  [   0/3542]  eta: 19:58:33  lr: 0.000002  min_lr: 0.000002  loss: 1.8330 (1.8330)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 20.3032  data: 19.6680  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [9]  [  10/3542]  eta: 3:24:34  lr: 0.000002  min_lr: 0.000002  loss: 1.9121 (1.9159)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 3.4751  data: 2.8847  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [9]  [  20/3542]  eta: 2:31:16  lr: 0.000002  min_lr: 0.000002  loss: 1.8828 (1.8805)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6907  data: 1.1077  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Epoch: [9]  [  30/3542]  eta: 2:10:48  lr: 0.000002  min_lr: 0.000002  loss: 1.8457 (1.8714)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5524  data: 0.9724  max mem: 13008
Epoch: [9]  [  40/3542]  eta: 2:03:53  lr: 0.000002  min_lr: 0.000002  loss: 1.8486 (1.8769)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.6457  data: 1.0637  max mem: 13008
Epoch: [9]  [  50/3542]  eta: 1:54:48  lr: 0.000002  min_lr: 0.000002  loss: 1.8945 (1.8853)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.5668  data: 0.9845  max mem: 13008
Epoch: [9]  [  60/3542]  eta: 1:46:32  lr: 0.000002  min_lr: 0.000002  loss: 1.8945 (1.8850)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.2480  data: 0.6672  max mem: 13008
Epoch: [9]  [  70/3542]  eta: 1:38:55  lr: 0.000002  min_lr: 0.000002  loss: 1.8867 (1.8871)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0387  data: 0.4578  max mem: 13008
Epoch: [9]  [  80/3542]  eta: 1:33:08  lr: 0.000002  min_lr: 0.000002  loss: 1.8115 (1.8744)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9376  data: 0.3549  max mem: 13008
Epoch: [9]  [  90/3542]  eta: 1:29:25  lr: 0.000002  min_lr: 0.000002  loss: 1.8066 (1.8709)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0027  data: 0.4199  max mem: 13008
Epoch: [9]  [ 100/3542]  eta: 1:26:19  lr: 0.000002  min_lr: 0.000002  loss: 1.8115 (1.8657)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0617  data: 0.4814  max mem: 13008
Epoch: [9]  [ 110/3542]  eta: 1:24:14  lr: 0.000002  min_lr: 0.000002  loss: 1.8320 (1.8717)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1014  data: 0.5203  max mem: 13008
Epoch: [9]  [ 120/3542]  eta: 1:22:25  lr: 0.000002  min_lr: 0.000002  loss: 1.9648 (1.8763)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.1453  data: 0.5633  max mem: 13008
[2023-05-16 13:38:15,957] [INFO] [logging.py:60:log_dist] [Rank 0] step=32000, skipped=29, lr=[2.097768660484079e-06, 2.097768660484079e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 13:38:15,960] [INFO] [timer.py:157:stop] 0/32000, SamplesPerSec=55.56064903383069
Epoch: [9]  [ 130/3542]  eta: 1:19:52  lr: 0.000002  min_lr: 0.000002  loss: 1.9209 (1.8753)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 1.0272  data: 0.4443  max mem: 13008
Epoch: [9]  [ 140/3542]  eta: 1:17:24  lr: 0.000002  min_lr: 0.000002  loss: 1.7871 (1.8714)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8799  data: 0.2966  max mem: 13008
Epoch: [9]  [ 150/3542]  eta: 1:15:14  lr: 0.000002  min_lr: 0.000002  loss: 1.7656 (1.8641)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8472  data: 0.2648  max mem: 13008
Epoch: [9]  [ 160/3542]  eta: 1:13:51  lr: 0.000002  min_lr: 0.000002  loss: 1.7656 (1.8607)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9241  data: 0.3421  max mem: 13008
Epoch: [9]  [ 170/3542]  eta: 1:12:01  lr: 0.000002  min_lr: 0.000002  loss: 1.7842 (1.8615)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.9094  data: 0.3268  max mem: 13008
Epoch: [9]  [ 180/3542]  eta: 1:10:43  lr: 0.000002  min_lr: 0.000002  loss: 1.7695 (1.8590)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8734  data: 0.2897  max mem: 13008
Epoch: [9]  [ 190/3542]  eta: 1:09:04  lr: 0.000002  min_lr: 0.000002  loss: 1.7695 (1.8571)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8498  data: 0.2664  max mem: 13008
Epoch: [9]  [ 200/3542]  eta: 1:07:28  lr: 0.000002  min_lr: 0.000002  loss: 1.7998 (1.8559)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7525  data: 0.1697  max mem: 13008
Epoch: [9]  [ 210/3542]  eta: 1:06:30  lr: 0.000002  min_lr: 0.000002  loss: 1.7705 (1.8511)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8270  data: 0.2420  max mem: 13008
Epoch: [9]  [ 220/3542]  eta: 1:05:00  lr: 0.000002  min_lr: 0.000002  loss: 1.7236 (1.8473)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7997  data: 0.2155  max mem: 13008
Epoch: [9]  [ 230/3542]  eta: 1:03:33  lr: 0.000002  min_lr: 0.000002  loss: 1.7793 (1.8470)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6662  data: 0.0843  max mem: 13008
Epoch: [9]  [ 240/3542]  eta: 1:02:14  lr: 0.000002  min_lr: 0.000002  loss: 1.8477 (1.8483)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6528  data: 0.0700  max mem: 13008
Epoch: [9]  [ 250/3542]  eta: 1:01:21  lr: 0.000002  min_lr: 0.000002  loss: 1.8604 (1.8496)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7365  data: 0.1535  max mem: 13008
Epoch: [9]  [ 260/3542]  eta: 1:00:16  lr: 0.000002  min_lr: 0.000002  loss: 1.8945 (1.8532)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7515  data: 0.1684  max mem: 13008
Epoch: [9]  [ 270/3542]  eta: 0:59:16  lr: 0.000002  min_lr: 0.000002  loss: 1.9316 (1.8551)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6901  data: 0.1066  max mem: 13008
Epoch: [9]  [ 280/3542]  eta: 0:58:18  lr: 0.000002  min_lr: 0.000002  loss: 1.8809 (1.8547)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6894  data: 0.1055  max mem: 13008
Epoch: [9]  [ 290/3542]  eta: 0:57:12  lr: 0.000002  min_lr: 0.000002  loss: 1.8496 (1.8556)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6328  data: 0.0491  max mem: 13008
Epoch: [9]  [ 300/3542]  eta: 0:56:17  lr: 0.000002  min_lr: 0.000002  loss: 1.8262 (1.8557)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6123  data: 0.0290  max mem: 13008
Epoch: [9]  [ 310/3542]  eta: 0:55:30  lr: 0.000002  min_lr: 0.000002  loss: 1.8262 (1.8572)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0823  max mem: 13008
Epoch: [9]  [ 320/3542]  eta: 0:55:02  lr: 0.000002  min_lr: 0.000002  loss: 1.8877 (1.8575)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7701  data: 0.1862  max mem: 13008
Epoch: [9]  [ 330/3542]  eta: 0:54:27  lr: 0.000002  min_lr: 0.000002  loss: 1.8145 (1.8580)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.8104  data: 0.2269  max mem: 13008
Epoch: [9]  [ 340/3542]  eta: 0:53:54  lr: 0.000002  min_lr: 0.000002  loss: 1.8496 (1.8582)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7717  data: 0.1886  max mem: 13008
Epoch: [9]  [ 350/3542]  eta: 0:53:20  lr: 0.000002  min_lr: 0.000002  loss: 1.8574 (1.8581)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7631  data: 0.1798  max mem: 13008
Epoch: [9]  [ 360/3542]  eta: 0:52:43  lr: 0.000002  min_lr: 0.000002  loss: 1.8760 (1.8589)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7263  data: 0.1425  max mem: 13008
Epoch: [9]  [ 370/3542]  eta: 0:51:59  lr: 0.000002  min_lr: 0.000002  loss: 1.8730 (1.8582)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6452  data: 0.0609  max mem: 13008
Epoch: [9]  [ 380/3542]  eta: 0:51:26  lr: 0.000002  min_lr: 0.000002  loss: 1.8311 (1.8573)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6486  data: 0.0646  max mem: 13008
Epoch: [9]  [ 390/3542]  eta: 0:50:47  lr: 0.000002  min_lr: 0.000002  loss: 1.8184 (1.8559)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6607  data: 0.0768  max mem: 13008
Epoch: [9]  [ 400/3542]  eta: 0:50:13  lr: 0.000002  min_lr: 0.000002  loss: 1.7764 (1.8566)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.0474  max mem: 13008
Epoch: [9]  [ 410/3542]  eta: 0:49:40  lr: 0.000002  min_lr: 0.000002  loss: 1.7764 (1.8550)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6508  data: 0.0668  max mem: 13008
Epoch: [9]  [ 420/3542]  eta: 0:49:07  lr: 0.000002  min_lr: 0.000002  loss: 1.7939 (1.8557)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6420  data: 0.0578  max mem: 13008
Epoch: [9]  [ 430/3542]  eta: 0:48:38  lr: 0.000002  min_lr: 0.000002  loss: 1.8613 (1.8567)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6538  data: 0.0701  max mem: 13008
Epoch: [9]  [ 440/3542]  eta: 0:48:09  lr: 0.000002  min_lr: 0.000002  loss: 1.8213 (1.8548)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6670  data: 0.0835  max mem: 13008
Epoch: [9]  [ 450/3542]  eta: 0:47:42  lr: 0.000002  min_lr: 0.000002  loss: 1.8184 (1.8560)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.0809  max mem: 13008
Epoch: [9]  [ 460/3542]  eta: 0:47:15  lr: 0.000002  min_lr: 0.000002  loss: 1.8184 (1.8554)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0886  max mem: 13008
Epoch: [9]  [ 470/3542]  eta: 0:46:49  lr: 0.000002  min_lr: 0.000002  loss: 1.7588 (1.8552)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6618  data: 0.0773  max mem: 13008
Epoch: [9]  [ 480/3542]  eta: 0:46:20  lr: 0.000002  min_lr: 0.000002  loss: 1.8760 (1.8552)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6257  data: 0.0414  max mem: 13008
Epoch: [9]  [ 490/3542]  eta: 0:45:54  lr: 0.000002  min_lr: 0.000002  loss: 1.8066 (1.8524)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6243  data: 0.0394  max mem: 13008
Epoch: [9]  [ 500/3542]  eta: 0:45:35  lr: 0.000002  min_lr: 0.000002  loss: 1.7764 (1.8527)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6880  data: 0.1031  max mem: 13008
Epoch: [9]  [ 510/3542]  eta: 0:45:08  lr: 0.000002  min_lr: 0.000002  loss: 1.7764 (1.8513)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6657  data: 0.0816  max mem: 13008
Epoch: [9]  [ 520/3542]  eta: 0:44:47  lr: 0.000002  min_lr: 0.000002  loss: 1.8721 (1.8537)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.0603  max mem: 13008
Epoch: [9]  [ 530/3542]  eta: 0:44:21  lr: 0.000002  min_lr: 0.000002  loss: 1.8896 (1.8536)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6343  data: 0.0502  max mem: 13008
Epoch: [9]  [ 540/3542]  eta: 0:44:02  lr: 0.000002  min_lr: 0.000002  loss: 1.8096 (1.8539)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6385  data: 0.0541  max mem: 13008
Epoch: [9]  [ 550/3542]  eta: 0:43:39  lr: 0.000002  min_lr: 0.000002  loss: 1.7559 (1.8529)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.0740  max mem: 13008
Epoch: [9]  [ 560/3542]  eta: 0:43:27  lr: 0.000002  min_lr: 0.000002  loss: 1.8389 (1.8538)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7177  data: 0.1324  max mem: 13008
Epoch: [9]  [ 570/3542]  eta: 0:43:03  lr: 0.000002  min_lr: 0.000002  loss: 1.8496 (1.8535)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.7006  data: 0.1154  max mem: 13008
Epoch: [9]  [ 580/3542]  eta: 0:42:42  lr: 0.000002  min_lr: 0.000002  loss: 1.8584 (1.8548)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6107  data: 0.0252  max mem: 13008
Epoch: [9]  [ 590/3542]  eta: 0:42:26  lr: 0.000002  min_lr: 0.000002  loss: 1.8838 (1.8552)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6699  data: 0.0843  max mem: 13008
Epoch: [9]  [ 600/3542]  eta: 0:42:07  lr: 0.000002  min_lr: 0.000002  loss: 1.8857 (1.8558)  loss_scale: 8192.0000 (8192.0000)  weight_decay: 0.0500 (0.0500)  time: 0.6819  data: 0.0977  max mem: 13008
[2023-05-16 13:44:00,553] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 13:44:00,553] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [9]  [ 610/3542]  eta: 0:41:49  lr: 0.000002  min_lr: 0.000002  loss: 1.8545 (1.8551)  loss_scale: 8192.0000 (8272.4452)  weight_decay: 0.0500 (0.0500)  time: 0.6570  data: 0.0720  max mem: 13008
Epoch: [9]  [ 620/3542]  eta: 0:41:32  lr: 0.000002  min_lr: 0.000002  loss: 1.8213 (1.8553)  loss_scale: 16384.0000 (8403.0660)  weight_decay: 0.0500 (0.0500)  time: 0.6721  data: 0.0858  max mem: 13008
Epoch: [9]  [ 630/3542]  eta: 0:41:19  lr: 0.000002  min_lr: 0.000002  loss: 1.8447 (1.8550)  loss_scale: 16384.0000 (8529.5468)  weight_decay: 0.0500 (0.0500)  time: 0.7161  data: 0.1306  max mem: 13008
Epoch: [9]  [ 640/3542]  eta: 0:41:01  lr: 0.000002  min_lr: 0.000002  loss: 1.8232 (1.8541)  loss_scale: 16384.0000 (8652.0811)  weight_decay: 0.0500 (0.0500)  time: 0.6991  data: 0.1136  max mem: 13008
Epoch: [9]  [ 650/3542]  eta: 0:40:49  lr: 0.000002  min_lr: 0.000002  loss: 1.8232 (1.8547)  loss_scale: 16384.0000 (8770.8510)  weight_decay: 0.0500 (0.0500)  time: 0.7101  data: 0.1246  max mem: 13008
Epoch: [9]  [ 660/3542]  eta: 0:40:32  lr: 0.000002  min_lr: 0.000002  loss: 1.8809 (1.8544)  loss_scale: 16384.0000 (8886.0272)  weight_decay: 0.0500 (0.0500)  time: 0.7057  data: 0.1206  max mem: 13008
Epoch: [9]  [ 670/3542]  eta: 0:40:16  lr: 0.000002  min_lr: 0.000002  loss: 1.8555 (1.8551)  loss_scale: 16384.0000 (8997.7705)  weight_decay: 0.0500 (0.0500)  time: 0.6530  data: 0.0683  max mem: 13008
Epoch: [9]  [ 680/3542]  eta: 0:40:00  lr: 0.000002  min_lr: 0.000002  loss: 1.8330 (1.8552)  loss_scale: 16384.0000 (9106.2320)  weight_decay: 0.0500 (0.0500)  time: 0.6636  data: 0.0786  max mem: 13008
Epoch: [9]  [ 690/3542]  eta: 0:39:45  lr: 0.000002  min_lr: 0.000002  loss: 1.8252 (1.8552)  loss_scale: 16384.0000 (9211.5543)  weight_decay: 0.0500 (0.0500)  time: 0.6681  data: 0.0839  max mem: 13008
Epoch: [9]  [ 700/3542]  eta: 0:39:30  lr: 0.000002  min_lr: 0.000002  loss: 1.8252 (1.8552)  loss_scale: 16384.0000 (9313.8716)  weight_decay: 0.0500 (0.0500)  time: 0.6780  data: 0.0942  max mem: 13008
Epoch: [9]  [ 710/3542]  eta: 0:39:20  lr: 0.000002  min_lr: 0.000002  loss: 1.7969 (1.8544)  loss_scale: 16384.0000 (9413.3108)  weight_decay: 0.0500 (0.0500)  time: 0.7314  data: 0.1465  max mem: 13008
Epoch: [9]  [ 720/3542]  eta: 0:39:06  lr: 0.000002  min_lr: 0.000002  loss: 1.8311 (1.8543)  loss_scale: 16384.0000 (9509.9917)  weight_decay: 0.0500 (0.0500)  time: 0.7423  data: 0.1582  max mem: 13008
Epoch: [9]  [ 730/3542]  eta: 0:38:51  lr: 0.000002  min_lr: 0.000002  loss: 1.8389 (1.8546)  loss_scale: 16384.0000 (9604.0274)  weight_decay: 0.0500 (0.0500)  time: 0.6776  data: 0.0941  max mem: 13008
Epoch: [9]  [ 740/3542]  eta: 0:38:36  lr: 0.000002  min_lr: 0.000002  loss: 1.8516 (1.8547)  loss_scale: 16384.0000 (9695.5250)  weight_decay: 0.0500 (0.0500)  time: 0.6529  data: 0.0686  max mem: 13008
Epoch: [9]  [ 750/3542]  eta: 0:38:21  lr: 0.000002  min_lr: 0.000002  loss: 1.8281 (1.8539)  loss_scale: 16384.0000 (9784.5859)  weight_decay: 0.0500 (0.0500)  time: 0.6470  data: 0.0627  max mem: 13008
Epoch: [9]  [ 760/3542]  eta: 0:38:04  lr: 0.000002  min_lr: 0.000002  loss: 1.8281 (1.8543)  loss_scale: 16384.0000 (9871.3062)  weight_decay: 0.0500 (0.0500)  time: 0.6178  data: 0.0342  max mem: 13008
Epoch: [9]  [ 770/3542]  eta: 0:37:54  lr: 0.000002  min_lr: 0.000002  loss: 1.9287 (1.8554)  loss_scale: 16384.0000 (9955.7769)  weight_decay: 0.0500 (0.0500)  time: 0.6867  data: 0.1024  max mem: 13008
Epoch: [9]  [ 780/3542]  eta: 0:37:40  lr: 0.000002  min_lr: 0.000002  loss: 1.8096 (1.8543)  loss_scale: 16384.0000 (10038.0845)  weight_decay: 0.0500 (0.0500)  time: 0.7176  data: 0.1329  max mem: 13008
Epoch: [9]  [ 790/3542]  eta: 0:37:28  lr: 0.000002  min_lr: 0.000002  loss: 1.7432 (1.8538)  loss_scale: 16384.0000 (10118.3110)  weight_decay: 0.0500 (0.0500)  time: 0.6743  data: 0.0896  max mem: 13008
Epoch: [9]  [ 800/3542]  eta: 0:37:16  lr: 0.000002  min_lr: 0.000002  loss: 1.7959 (1.8537)  loss_scale: 16384.0000 (10196.5343)  weight_decay: 0.0500 (0.0500)  time: 0.6979  data: 0.1126  max mem: 13008
Epoch: [9]  [ 810/3542]  eta: 0:37:04  lr: 0.000002  min_lr: 0.000002  loss: 1.8105 (1.8538)  loss_scale: 16384.0000 (10272.8286)  weight_decay: 0.0500 (0.0500)  time: 0.7071  data: 0.1216  max mem: 13008
Epoch: [9]  [ 820/3542]  eta: 0:36:52  lr: 0.000002  min_lr: 0.000002  loss: 1.8223 (1.8531)  loss_scale: 16384.0000 (10347.2643)  weight_decay: 0.0500 (0.0500)  time: 0.7062  data: 0.1218  max mem: 13008
Epoch: [9]  [ 830/3542]  eta: 0:36:37  lr: 0.000002  min_lr: 0.000002  loss: 1.8164 (1.8519)  loss_scale: 16384.0000 (10419.9085)  weight_decay: 0.0500 (0.0500)  time: 0.6502  data: 0.0664  max mem: 13008
Epoch: [9]  [ 840/3542]  eta: 0:36:25  lr: 0.000002  min_lr: 0.000002  loss: 1.8184 (1.8515)  loss_scale: 16384.0000 (10490.8252)  weight_decay: 0.0500 (0.0500)  time: 0.6372  data: 0.0530  max mem: 13008
Epoch: [9]  [ 850/3542]  eta: 0:36:14  lr: 0.000002  min_lr: 0.000002  loss: 1.8359 (1.8516)  loss_scale: 16384.0000 (10560.0752)  weight_decay: 0.0500 (0.0500)  time: 0.6968  data: 0.1122  max mem: 13008
Epoch: [9]  [ 860/3542]  eta: 0:36:03  lr: 0.000002  min_lr: 0.000002  loss: 1.8359 (1.8511)  loss_scale: 16384.0000 (10627.7166)  weight_decay: 0.0500 (0.0500)  time: 0.7194  data: 0.1346  max mem: 13008
Epoch: [9]  [ 870/3542]  eta: 0:35:48  lr: 0.000002  min_lr: 0.000002  loss: 1.7412 (1.8499)  loss_scale: 16384.0000 (10693.8048)  weight_decay: 0.0500 (0.0500)  time: 0.6544  data: 0.0695  max mem: 13008
Epoch: [9]  [ 880/3542]  eta: 0:35:34  lr: 0.000002  min_lr: 0.000002  loss: 1.8027 (1.8505)  loss_scale: 16384.0000 (10758.3927)  weight_decay: 0.0500 (0.0500)  time: 0.5934  data: 0.0083  max mem: 13008
Epoch: [9]  [ 890/3542]  eta: 0:35:23  lr: 0.000002  min_lr: 0.000002  loss: 1.9473 (1.8519)  loss_scale: 16384.0000 (10821.5309)  weight_decay: 0.0500 (0.0500)  time: 0.6501  data: 0.0648  max mem: 13008
Epoch: [9]  [ 900/3542]  eta: 0:35:11  lr: 0.000002  min_lr: 0.000002  loss: 1.8887 (1.8507)  loss_scale: 16384.0000 (10883.2675)  weight_decay: 0.0500 (0.0500)  time: 0.6784  data: 0.0930  max mem: 13008
Epoch: [9]  [ 910/3542]  eta: 0:34:59  lr: 0.000002  min_lr: 0.000002  loss: 1.7305 (1.8494)  loss_scale: 16384.0000 (10943.6487)  weight_decay: 0.0500 (0.0500)  time: 0.6594  data: 0.0744  max mem: 13008
Epoch: [9]  [ 920/3542]  eta: 0:34:46  lr: 0.000002  min_lr: 0.000002  loss: 1.7441 (1.8492)  loss_scale: 16384.0000 (11002.7188)  weight_decay: 0.0500 (0.0500)  time: 0.6405  data: 0.0555  max mem: 13008
Epoch: [9]  [ 930/3542]  eta: 0:34:36  lr: 0.000002  min_lr: 0.000002  loss: 1.8311 (1.8490)  loss_scale: 16384.0000 (11060.5199)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.0881  max mem: 13008
Epoch: [9]  [ 940/3542]  eta: 0:34:27  lr: 0.000002  min_lr: 0.000002  loss: 1.8311 (1.8487)  loss_scale: 16384.0000 (11117.0925)  weight_decay: 0.0500 (0.0500)  time: 0.7519  data: 0.1671  max mem: 13008
Epoch: [9]  [ 950/3542]  eta: 0:34:16  lr: 0.000002  min_lr: 0.000002  loss: 1.8672 (1.8487)  loss_scale: 16384.0000 (11172.4753)  weight_decay: 0.0500 (0.0500)  time: 0.7226  data: 0.1381  max mem: 13008
Epoch: [9]  [ 960/3542]  eta: 0:34:04  lr: 0.000002  min_lr: 0.000002  loss: 1.8857 (1.8494)  loss_scale: 16384.0000 (11226.7055)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.0657  max mem: 13008
Epoch: [9]  [ 970/3542]  eta: 0:33:55  lr: 0.000002  min_lr: 0.000002  loss: 1.8779 (1.8495)  loss_scale: 16384.0000 (11279.8187)  weight_decay: 0.0500 (0.0500)  time: 0.7075  data: 0.1229  max mem: 13008
[2023-05-16 13:48:11,581] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 32852
[2023-05-16 13:48:11,581] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 13:48:11,581] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [9]  [ 980/3542]  eta: 0:33:46  lr: 0.000002  min_lr: 0.000002  loss: 1.8779 (1.8495)  loss_scale: 16384.0000 (11273.3945)  weight_decay: 0.0500 (0.0500)  time: 0.7478  data: 0.1640  max mem: 13008
Epoch: [9]  [ 990/3542]  eta: 0:33:35  lr: 0.000002  min_lr: 0.000002  loss: 1.8486 (1.8492)  loss_scale: 8192.0000 (11242.3007)  weight_decay: 0.0500 (0.0500)  time: 0.6946  data: 0.1103  max mem: 13008
Epoch: [9]  [1000/3542]  eta: 0:33:24  lr: 0.000002  min_lr: 0.000002  loss: 1.8203 (1.8491)  loss_scale: 8192.0000 (11211.8282)  weight_decay: 0.0500 (0.0500)  time: 0.6755  data: 0.0914  max mem: 13008
Epoch: [9]  [1010/3542]  eta: 0:33:16  lr: 0.000002  min_lr: 0.000002  loss: 1.8096 (1.8487)  loss_scale: 8192.0000 (11181.9585)  weight_decay: 0.0500 (0.0500)  time: 0.7274  data: 0.1437  max mem: 13008
Epoch: [9]  [1020/3542]  eta: 0:33:05  lr: 0.000002  min_lr: 0.000002  loss: 1.7900 (1.8489)  loss_scale: 8192.0000 (11152.6738)  weight_decay: 0.0500 (0.0500)  time: 0.7199  data: 0.1358  max mem: 13008
Epoch: [9]  [1030/3542]  eta: 0:32:54  lr: 0.000002  min_lr: 0.000002  loss: 1.7930 (1.8484)  loss_scale: 8192.0000 (11123.9573)  weight_decay: 0.0500 (0.0500)  time: 0.6676  data: 0.0836  max mem: 13008
Epoch: [9]  [1040/3542]  eta: 0:32:44  lr: 0.000002  min_lr: 0.000002  loss: 1.8281 (1.8481)  loss_scale: 8192.0000 (11095.7925)  weight_decay: 0.0500 (0.0500)  time: 0.6903  data: 0.1058  max mem: 13008
Epoch: [9]  [1050/3542]  eta: 0:32:34  lr: 0.000002  min_lr: 0.000002  loss: 1.8389 (1.8486)  loss_scale: 8192.0000 (11068.1637)  weight_decay: 0.0500 (0.0500)  time: 0.6907  data: 0.1061  max mem: 13008
Epoch: [9]  [1060/3542]  eta: 0:32:23  lr: 0.000002  min_lr: 0.000002  loss: 1.8877 (1.8487)  loss_scale: 8192.0000 (11041.0556)  weight_decay: 0.0500 (0.0500)  time: 0.6687  data: 0.0845  max mem: 13008
Epoch: [9]  [1070/3542]  eta: 0:32:13  lr: 0.000002  min_lr: 0.000002  loss: 1.8027 (1.8484)  loss_scale: 8192.0000 (11014.4538)  weight_decay: 0.0500 (0.0500)  time: 0.6800  data: 0.0962  max mem: 13008
Epoch: [9]  [1080/3542]  eta: 0:32:01  lr: 0.000002  min_lr: 0.000002  loss: 1.8486 (1.8491)  loss_scale: 8192.0000 (10988.3441)  weight_decay: 0.0500 (0.0500)  time: 0.6417  data: 0.0576  max mem: 13008
Epoch: [9]  [1090/3542]  eta: 0:31:52  lr: 0.000002  min_lr: 0.000002  loss: 1.8779 (1.8498)  loss_scale: 8192.0000 (10962.7131)  weight_decay: 0.0500 (0.0500)  time: 0.6603  data: 0.0757  max mem: 13008
Epoch: [9]  [1100/3542]  eta: 0:31:41  lr: 0.000002  min_lr: 0.000002  loss: 1.9248 (1.8508)  loss_scale: 8192.0000 (10937.5477)  weight_decay: 0.0500 (0.0500)  time: 0.6730  data: 0.0888  max mem: 13008
Epoch: [9]  [1110/3542]  eta: 0:31:32  lr: 0.000002  min_lr: 0.000002  loss: 1.8975 (1.8503)  loss_scale: 8192.0000 (10912.8353)  weight_decay: 0.0500 (0.0500)  time: 0.6649  data: 0.0804  max mem: 13008
Epoch: [9]  [1120/3542]  eta: 0:31:21  lr: 0.000002  min_lr: 0.000002  loss: 1.8027 (1.8505)  loss_scale: 8192.0000 (10888.5638)  weight_decay: 0.0500 (0.0500)  time: 0.6787  data: 0.0933  max mem: 13008
[2023-05-16 13:49:51,940] [INFO] [logging.py:60:log_dist] [Rank 0] step=33000, skipped=30, lr=[1.5523961270942508e-06, 1.5523961270942508e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 13:49:51,943] [INFO] [timer.py:157:stop] 0/33000, SamplesPerSec=55.56411924715151
Epoch: [9]  [1130/3542]  eta: 0:31:11  lr: 0.000002  min_lr: 0.000002  loss: 1.7598 (1.8502)  loss_scale: 8192.0000 (10864.7215)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.0648  max mem: 13008
Epoch: [9]  [1140/3542]  eta: 0:31:02  lr: 0.000002  min_lr: 0.000002  loss: 1.8203 (1.8507)  loss_scale: 8192.0000 (10841.2971)  weight_decay: 0.0500 (0.0500)  time: 0.6880  data: 0.1028  max mem: 13008
Epoch: [9]  [1150/3542]  eta: 0:30:50  lr: 0.000002  min_lr: 0.000002  loss: 1.7959 (1.8496)  loss_scale: 8192.0000 (10818.2798)  weight_decay: 0.0500 (0.0500)  time: 0.6652  data: 0.0799  max mem: 13008
Epoch: [9]  [1160/3542]  eta: 0:30:40  lr: 0.000002  min_lr: 0.000002  loss: 1.7949 (1.8499)  loss_scale: 8192.0000 (10795.6589)  weight_decay: 0.0500 (0.0500)  time: 0.6306  data: 0.0446  max mem: 13008
Epoch: [9]  [1170/3542]  eta: 0:30:29  lr: 0.000002  min_lr: 0.000002  loss: 1.8613 (1.8497)  loss_scale: 8192.0000 (10773.4244)  weight_decay: 0.0500 (0.0500)  time: 0.6292  data: 0.0428  max mem: 13008
Epoch: [9]  [1180/3542]  eta: 0:30:19  lr: 0.000002  min_lr: 0.000002  loss: 1.7969 (1.8493)  loss_scale: 8192.0000 (10751.5665)  weight_decay: 0.0500 (0.0500)  time: 0.6439  data: 0.0575  max mem: 13008
Epoch: [9]  [1190/3542]  eta: 0:30:10  lr: 0.000002  min_lr: 0.000002  loss: 1.8213 (1.8494)  loss_scale: 8192.0000 (10730.0756)  weight_decay: 0.0500 (0.0500)  time: 0.6892  data: 0.1035  max mem: 13008
Epoch: [9]  [1200/3542]  eta: 0:30:00  lr: 0.000002  min_lr: 0.000002  loss: 1.9053 (1.8498)  loss_scale: 8192.0000 (10708.9425)  weight_decay: 0.0500 (0.0500)  time: 0.6720  data: 0.0876  max mem: 13008
Epoch: [9]  [1210/3542]  eta: 0:29:50  lr: 0.000002  min_lr: 0.000002  loss: 1.8174 (1.8492)  loss_scale: 8192.0000 (10688.1585)  weight_decay: 0.0500 (0.0500)  time: 0.6389  data: 0.0551  max mem: 13008
Epoch: [9]  [1220/3542]  eta: 0:29:39  lr: 0.000002  min_lr: 0.000002  loss: 1.8174 (1.8497)  loss_scale: 8192.0000 (10667.7150)  weight_decay: 0.0500 (0.0500)  time: 0.6135  data: 0.0277  max mem: 13008
Epoch: [9]  [1230/3542]  eta: 0:29:29  lr: 0.000002  min_lr: 0.000002  loss: 1.8477 (1.8493)  loss_scale: 8192.0000 (10647.6036)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0319  max mem: 13008
Epoch: [9]  [1240/3542]  eta: 0:29:19  lr: 0.000001  min_lr: 0.000001  loss: 1.8418 (1.8495)  loss_scale: 8192.0000 (10627.8163)  weight_decay: 0.0500 (0.0500)  time: 0.6429  data: 0.0576  max mem: 13008
Epoch: [9]  [1250/3542]  eta: 0:29:09  lr: 0.000001  min_lr: 0.000001  loss: 1.8145 (1.8490)  loss_scale: 8192.0000 (10608.3453)  weight_decay: 0.0500 (0.0500)  time: 0.6231  data: 0.0383  max mem: 13008
Epoch: [9]  [1260/3542]  eta: 0:28:59  lr: 0.000001  min_lr: 0.000001  loss: 1.8447 (1.8498)  loss_scale: 8192.0000 (10589.1832)  weight_decay: 0.0500 (0.0500)  time: 0.6311  data: 0.0459  max mem: 13008
Epoch: [9]  [1270/3542]  eta: 0:28:51  lr: 0.000001  min_lr: 0.000001  loss: 1.8457 (1.8496)  loss_scale: 8192.0000 (10570.3226)  weight_decay: 0.0500 (0.0500)  time: 0.6880  data: 0.1014  max mem: 13008
Epoch: [9]  [1280/3542]  eta: 0:28:42  lr: 0.000001  min_lr: 0.000001  loss: 1.8291 (1.8495)  loss_scale: 8192.0000 (10551.7564)  weight_decay: 0.0500 (0.0500)  time: 0.7261  data: 0.1401  max mem: 13008
Epoch: [9]  [1290/3542]  eta: 0:28:33  lr: 0.000001  min_lr: 0.000001  loss: 1.8242 (1.8490)  loss_scale: 8192.0000 (10533.4779)  weight_decay: 0.0500 (0.0500)  time: 0.6929  data: 0.1081  max mem: 13008
Epoch: [9]  [1300/3542]  eta: 0:28:25  lr: 0.000001  min_lr: 0.000001  loss: 1.8164 (1.8490)  loss_scale: 8192.0000 (10515.4804)  weight_decay: 0.0500 (0.0500)  time: 0.6885  data: 0.1032  max mem: 13008
Epoch: [9]  [1310/3542]  eta: 0:28:16  lr: 0.000001  min_lr: 0.000001  loss: 1.8428 (1.8493)  loss_scale: 8192.0000 (10497.7574)  weight_decay: 0.0500 (0.0500)  time: 0.7064  data: 0.1208  max mem: 13008
Epoch: [9]  [1320/3542]  eta: 0:28:08  lr: 0.000001  min_lr: 0.000001  loss: 1.8604 (1.8494)  loss_scale: 8192.0000 (10480.3028)  weight_decay: 0.0500 (0.0500)  time: 0.7047  data: 0.1205  max mem: 13008
Epoch: [9]  [1330/3542]  eta: 0:28:02  lr: 0.000001  min_lr: 0.000001  loss: 1.7979 (1.8494)  loss_scale: 8192.0000 (10463.1104)  weight_decay: 0.0500 (0.0500)  time: 0.7957  data: 0.2125  max mem: 13008
Epoch: [9]  [1340/3542]  eta: 0:27:56  lr: 0.000001  min_lr: 0.000001  loss: 1.7725 (1.8489)  loss_scale: 8192.0000 (10446.1745)  weight_decay: 0.0500 (0.0500)  time: 0.8725  data: 0.2894  max mem: 13008
Epoch: [9]  [1350/3542]  eta: 0:27:50  lr: 0.000001  min_lr: 0.000001  loss: 1.7910 (1.8490)  loss_scale: 8192.0000 (10429.4893)  weight_decay: 0.0500 (0.0500)  time: 0.8654  data: 0.2825  max mem: 13008
Epoch: [9]  [1360/3542]  eta: 0:27:44  lr: 0.000001  min_lr: 0.000001  loss: 1.8398 (1.8488)  loss_scale: 8192.0000 (10413.0492)  weight_decay: 0.0500 (0.0500)  time: 0.8561  data: 0.2729  max mem: 13008
Epoch: [9]  [1370/3542]  eta: 0:27:37  lr: 0.000001  min_lr: 0.000001  loss: 1.7998 (1.8486)  loss_scale: 8192.0000 (10396.8490)  weight_decay: 0.0500 (0.0500)  time: 0.8335  data: 0.2497  max mem: 13008
Epoch: [9]  [1380/3542]  eta: 0:27:33  lr: 0.000001  min_lr: 0.000001  loss: 1.7861 (1.8480)  loss_scale: 8192.0000 (10380.8834)  weight_decay: 0.0500 (0.0500)  time: 0.8914  data: 0.3077  max mem: 13008
Epoch: [9]  [1390/3542]  eta: 0:27:31  lr: 0.000001  min_lr: 0.000001  loss: 1.8281 (1.8485)  loss_scale: 8192.0000 (10365.1474)  weight_decay: 0.0500 (0.0500)  time: 1.0495  data: 0.4648  max mem: 13008
Epoch: [9]  [1400/3542]  eta: 0:27:27  lr: 0.000001  min_lr: 0.000001  loss: 1.8711 (1.8486)  loss_scale: 8192.0000 (10349.6360)  weight_decay: 0.0500 (0.0500)  time: 1.0715  data: 0.4879  max mem: 13008
Epoch: [9]  [1410/3542]  eta: 0:27:23  lr: 0.000001  min_lr: 0.000001  loss: 1.8213 (1.8488)  loss_scale: 8192.0000 (10334.3444)  weight_decay: 0.0500 (0.0500)  time: 1.0252  data: 0.4433  max mem: 13008
Epoch: [9]  [1420/3542]  eta: 0:27:21  lr: 0.000001  min_lr: 0.000001  loss: 1.8213 (1.8485)  loss_scale: 8192.0000 (10319.2681)  weight_decay: 0.0500 (0.0500)  time: 1.0863  data: 0.5040  max mem: 13008
Epoch: [9]  [1430/3542]  eta: 0:27:18  lr: 0.000001  min_lr: 0.000001  loss: 1.8604 (1.8485)  loss_scale: 8192.0000 (10304.4025)  weight_decay: 0.0500 (0.0500)  time: 1.1115  data: 0.5280  max mem: 13008
Epoch: [9]  [1440/3542]  eta: 0:27:16  lr: 0.000001  min_lr: 0.000001  loss: 1.7852 (1.8479)  loss_scale: 8192.0000 (10289.7432)  weight_decay: 0.0500 (0.0500)  time: 1.1503  data: 0.5673  max mem: 13008
Epoch: [9]  [1450/3542]  eta: 0:27:13  lr: 0.000001  min_lr: 0.000001  loss: 1.8291 (1.8485)  loss_scale: 8192.0000 (10275.2860)  weight_decay: 0.0500 (0.0500)  time: 1.1428  data: 0.5612  max mem: 13008
Epoch: [9]  [1460/3542]  eta: 0:27:10  lr: 0.000001  min_lr: 0.000001  loss: 1.8525 (1.8485)  loss_scale: 8192.0000 (10261.0267)  weight_decay: 0.0500 (0.0500)  time: 1.0883  data: 0.5074  max mem: 13008
Epoch: [9]  [1470/3542]  eta: 0:27:07  lr: 0.000001  min_lr: 0.000001  loss: 1.8545 (1.8486)  loss_scale: 8192.0000 (10246.9613)  weight_decay: 0.0500 (0.0500)  time: 1.1300  data: 0.5494  max mem: 13008
Epoch: [9]  [1480/3542]  eta: 0:26:59  lr: 0.000001  min_lr: 0.000001  loss: 1.7490 (1.8479)  loss_scale: 8192.0000 (10233.0858)  weight_decay: 0.0500 (0.0500)  time: 0.9575  data: 0.3762  max mem: 13008
Epoch: [9]  [1490/3542]  eta: 0:26:57  lr: 0.000001  min_lr: 0.000001  loss: 1.7803 (1.8479)  loss_scale: 8192.0000 (10219.3964)  weight_decay: 0.0500 (0.0500)  time: 0.9912  data: 0.4074  max mem: 13008
Epoch: [9]  [1500/3542]  eta: 0:26:55  lr: 0.000001  min_lr: 0.000001  loss: 1.8828 (1.8484)  loss_scale: 8192.0000 (10205.8894)  weight_decay: 0.0500 (0.0500)  time: 1.2243  data: 0.6401  max mem: 13008
Epoch: [9]  [1510/3542]  eta: 0:26:52  lr: 0.000001  min_lr: 0.000001  loss: 1.8535 (1.8482)  loss_scale: 8192.0000 (10192.5612)  weight_decay: 0.0500 (0.0500)  time: 1.1845  data: 0.6032  max mem: 13008
Epoch: [9]  [1520/3542]  eta: 0:26:50  lr: 0.000001  min_lr: 0.000001  loss: 1.8535 (1.8486)  loss_scale: 8192.0000 (10179.4083)  weight_decay: 0.0500 (0.0500)  time: 1.1991  data: 0.6184  max mem: 13008
Epoch: [9]  [1530/3542]  eta: 0:26:48  lr: 0.000001  min_lr: 0.000001  loss: 1.8525 (1.8483)  loss_scale: 8192.0000 (10166.4272)  weight_decay: 0.0500 (0.0500)  time: 1.2515  data: 0.6688  max mem: 13008
Epoch: [9]  [1540/3542]  eta: 0:26:42  lr: 0.000001  min_lr: 0.000001  loss: 1.7744 (1.8480)  loss_scale: 8192.0000 (10153.6145)  weight_decay: 0.0500 (0.0500)  time: 1.1073  data: 0.5241  max mem: 13008
Epoch: [9]  [1550/3542]  eta: 0:26:39  lr: 0.000001  min_lr: 0.000001  loss: 1.7744 (1.8482)  loss_scale: 8192.0000 (10140.9671)  weight_decay: 0.0500 (0.0500)  time: 1.0844  data: 0.5026  max mem: 13008
Epoch: [9]  [1560/3542]  eta: 0:26:36  lr: 0.000001  min_lr: 0.000001  loss: 1.8242 (1.8481)  loss_scale: 8192.0000 (10128.4817)  weight_decay: 0.0500 (0.0500)  time: 1.2038  data: 0.6229  max mem: 13008
Epoch: [9]  [1570/3542]  eta: 0:26:33  lr: 0.000001  min_lr: 0.000001  loss: 1.8633 (1.8484)  loss_scale: 8192.0000 (10116.1553)  weight_decay: 0.0500 (0.0500)  time: 1.1692  data: 0.5865  max mem: 13008
Epoch: [9]  [1580/3542]  eta: 0:26:26  lr: 0.000001  min_lr: 0.000001  loss: 1.8555 (1.8480)  loss_scale: 8192.0000 (10103.9848)  weight_decay: 0.0500 (0.0500)  time: 1.0191  data: 0.4358  max mem: 13008
Epoch: [9]  [1590/3542]  eta: 0:26:18  lr: 0.000001  min_lr: 0.000001  loss: 1.8135 (1.8479)  loss_scale: 8192.0000 (10091.9673)  weight_decay: 0.0500 (0.0500)  time: 0.8897  data: 0.3083  max mem: 13008
Epoch: [9]  [1600/3542]  eta: 0:26:10  lr: 0.000001  min_lr: 0.000001  loss: 1.8330 (1.8478)  loss_scale: 8192.0000 (10080.0999)  weight_decay: 0.0500 (0.0500)  time: 0.8353  data: 0.2537  max mem: 13008
Epoch: [9]  [1610/3542]  eta: 0:26:04  lr: 0.000001  min_lr: 0.000001  loss: 1.8105 (1.8479)  loss_scale: 8192.0000 (10068.3799)  weight_decay: 0.0500 (0.0500)  time: 0.8641  data: 0.2814  max mem: 13008
Epoch: [9]  [1620/3542]  eta: 0:25:55  lr: 0.000001  min_lr: 0.000001  loss: 1.8105 (1.8481)  loss_scale: 8192.0000 (10056.8044)  weight_decay: 0.0500 (0.0500)  time: 0.8509  data: 0.2685  max mem: 13008
Epoch: [9]  [1630/3542]  eta: 0:25:47  lr: 0.000001  min_lr: 0.000001  loss: 1.8799 (1.8483)  loss_scale: 8192.0000 (10045.3709)  weight_decay: 0.0500 (0.0500)  time: 0.7749  data: 0.1929  max mem: 13008
Epoch: [9]  [1640/3542]  eta: 0:25:37  lr: 0.000001  min_lr: 0.000001  loss: 1.8711 (1.8485)  loss_scale: 8192.0000 (10034.0768)  weight_decay: 0.0500 (0.0500)  time: 0.7530  data: 0.1702  max mem: 13008
Epoch: [9]  [1650/3542]  eta: 0:25:30  lr: 0.000001  min_lr: 0.000001  loss: 1.8340 (1.8484)  loss_scale: 8192.0000 (10022.9194)  weight_decay: 0.0500 (0.0500)  time: 0.7975  data: 0.2153  max mem: 13008
Epoch: [9]  [1660/3542]  eta: 0:25:22  lr: 0.000001  min_lr: 0.000001  loss: 1.8564 (1.8484)  loss_scale: 8192.0000 (10011.8964)  weight_decay: 0.0500 (0.0500)  time: 0.8289  data: 0.2477  max mem: 13008
Epoch: [9]  [1670/3542]  eta: 0:25:14  lr: 0.000001  min_lr: 0.000001  loss: 1.9043 (1.8486)  loss_scale: 8192.0000 (10001.0054)  weight_decay: 0.0500 (0.0500)  time: 0.8318  data: 0.2503  max mem: 13008
Epoch: [9]  [1680/3542]  eta: 0:25:06  lr: 0.000001  min_lr: 0.000001  loss: 1.8115 (1.8483)  loss_scale: 8192.0000 (9990.2439)  weight_decay: 0.0500 (0.0500)  time: 0.8510  data: 0.2693  max mem: 13008
Epoch: [9]  [1690/3542]  eta: 0:25:00  lr: 0.000001  min_lr: 0.000001  loss: 1.7715 (1.8480)  loss_scale: 8192.0000 (9979.6097)  weight_decay: 0.0500 (0.0500)  time: 0.8705  data: 0.2890  max mem: 13008
Epoch: [9]  [1700/3542]  eta: 0:24:53  lr: 0.000001  min_lr: 0.000001  loss: 1.8271 (1.8479)  loss_scale: 8192.0000 (9969.1005)  weight_decay: 0.0500 (0.0500)  time: 0.9207  data: 0.3374  max mem: 13008
Epoch: [9]  [1710/3542]  eta: 0:24:47  lr: 0.000001  min_lr: 0.000001  loss: 1.8545 (1.8483)  loss_scale: 8192.0000 (9958.7142)  weight_decay: 0.0500 (0.0500)  time: 0.9767  data: 0.3932  max mem: 13008
Epoch: [9]  [1720/3542]  eta: 0:24:41  lr: 0.000001  min_lr: 0.000001  loss: 1.8545 (1.8479)  loss_scale: 8192.0000 (9948.4486)  weight_decay: 0.0500 (0.0500)  time: 1.0054  data: 0.4236  max mem: 13008
Epoch: [9]  [1730/3542]  eta: 0:24:35  lr: 0.000001  min_lr: 0.000001  loss: 1.7852 (1.8478)  loss_scale: 8192.0000 (9938.3016)  weight_decay: 0.0500 (0.0500)  time: 1.0260  data: 0.4444  max mem: 13008
Epoch: [9]  [1740/3542]  eta: 0:24:28  lr: 0.000001  min_lr: 0.000001  loss: 1.7764 (1.8473)  loss_scale: 8192.0000 (9928.2711)  weight_decay: 0.0500 (0.0500)  time: 0.9976  data: 0.4151  max mem: 13008
Epoch: [9]  [1750/3542]  eta: 0:24:21  lr: 0.000001  min_lr: 0.000001  loss: 1.7725 (1.8470)  loss_scale: 8192.0000 (9918.3552)  weight_decay: 0.0500 (0.0500)  time: 0.9048  data: 0.3223  max mem: 13008
Epoch: [9]  [1760/3542]  eta: 0:24:16  lr: 0.000001  min_lr: 0.000001  loss: 1.7754 (1.8468)  loss_scale: 8192.0000 (9908.5520)  weight_decay: 0.0500 (0.0500)  time: 1.0063  data: 0.4222  max mem: 13008
Epoch: [9]  [1770/3542]  eta: 0:24:08  lr: 0.000001  min_lr: 0.000001  loss: 1.8516 (1.8471)  loss_scale: 8192.0000 (9898.8594)  weight_decay: 0.0500 (0.0500)  time: 1.0061  data: 0.4218  max mem: 13008
Epoch: [9]  [1780/3542]  eta: 0:24:02  lr: 0.000001  min_lr: 0.000001  loss: 1.8691 (1.8474)  loss_scale: 8192.0000 (9889.2757)  weight_decay: 0.0500 (0.0500)  time: 0.9506  data: 0.3685  max mem: 13008
Epoch: [9]  [1790/3542]  eta: 0:23:54  lr: 0.000001  min_lr: 0.000001  loss: 1.8047 (1.8468)  loss_scale: 8192.0000 (9879.7990)  weight_decay: 0.0500 (0.0500)  time: 0.9439  data: 0.3615  max mem: 13008
Epoch: [9]  [1800/3542]  eta: 0:23:47  lr: 0.000001  min_lr: 0.000001  loss: 1.7471 (1.8466)  loss_scale: 8192.0000 (9870.4275)  weight_decay: 0.0500 (0.0500)  time: 0.8756  data: 0.2937  max mem: 13008
Epoch: [9]  [1810/3542]  eta: 0:23:38  lr: 0.000001  min_lr: 0.000001  loss: 1.7871 (1.8464)  loss_scale: 8192.0000 (9861.1596)  weight_decay: 0.0500 (0.0500)  time: 0.8005  data: 0.2182  max mem: 13008
Epoch: [9]  [1820/3542]  eta: 0:23:29  lr: 0.000001  min_lr: 0.000001  loss: 1.8164 (1.8466)  loss_scale: 8192.0000 (9851.9934)  weight_decay: 0.0500 (0.0500)  time: 0.7690  data: 0.1862  max mem: 13008
Epoch: [9]  [1830/3542]  eta: 0:23:21  lr: 0.000001  min_lr: 0.000001  loss: 1.8252 (1.8470)  loss_scale: 8192.0000 (9842.9274)  weight_decay: 0.0500 (0.0500)  time: 0.8043  data: 0.2220  max mem: 13008
Epoch: [9]  [1840/3542]  eta: 0:23:13  lr: 0.000001  min_lr: 0.000001  loss: 1.8105 (1.8467)  loss_scale: 8192.0000 (9833.9598)  weight_decay: 0.0500 (0.0500)  time: 0.8127  data: 0.2283  max mem: 13008
Epoch: [9]  [1850/3542]  eta: 0:23:05  lr: 0.000001  min_lr: 0.000001  loss: 1.8359 (1.8468)  loss_scale: 8192.0000 (9825.0891)  weight_decay: 0.0500 (0.0500)  time: 0.8542  data: 0.2687  max mem: 13008
Epoch: [9]  [1860/3542]  eta: 0:22:57  lr: 0.000001  min_lr: 0.000001  loss: 1.8359 (1.8467)  loss_scale: 8192.0000 (9816.3138)  weight_decay: 0.0500 (0.0500)  time: 0.8629  data: 0.2775  max mem: 13008
Epoch: [9]  [1870/3542]  eta: 0:22:48  lr: 0.000001  min_lr: 0.000001  loss: 1.8271 (1.8468)  loss_scale: 8192.0000 (9807.6323)  weight_decay: 0.0500 (0.0500)  time: 0.7489  data: 0.1637  max mem: 13008
Epoch: [9]  [1880/3542]  eta: 0:22:39  lr: 0.000001  min_lr: 0.000001  loss: 1.8379 (1.8471)  loss_scale: 8192.0000 (9799.0431)  weight_decay: 0.0500 (0.0500)  time: 0.7191  data: 0.1342  max mem: 13008
Epoch: [9]  [1890/3542]  eta: 0:22:30  lr: 0.000001  min_lr: 0.000001  loss: 1.8887 (1.8473)  loss_scale: 8192.0000 (9790.5447)  weight_decay: 0.0500 (0.0500)  time: 0.7287  data: 0.1443  max mem: 13008
Epoch: [9]  [1900/3542]  eta: 0:22:21  lr: 0.000001  min_lr: 0.000001  loss: 1.8887 (1.8475)  loss_scale: 8192.0000 (9782.1357)  weight_decay: 0.0500 (0.0500)  time: 0.6966  data: 0.1127  max mem: 13008
Epoch: [9]  [1910/3542]  eta: 0:22:12  lr: 0.000001  min_lr: 0.000001  loss: 1.8271 (1.8474)  loss_scale: 8192.0000 (9773.8148)  weight_decay: 0.0500 (0.0500)  time: 0.7317  data: 0.1479  max mem: 13008
Epoch: [9]  [1920/3542]  eta: 0:22:04  lr: 0.000001  min_lr: 0.000001  loss: 1.8213 (1.8475)  loss_scale: 8192.0000 (9765.5804)  weight_decay: 0.0500 (0.0500)  time: 0.7727  data: 0.1893  max mem: 13008
Epoch: [9]  [1930/3542]  eta: 0:21:55  lr: 0.000001  min_lr: 0.000001  loss: 1.8643 (1.8475)  loss_scale: 8192.0000 (9757.4314)  weight_decay: 0.0500 (0.0500)  time: 0.7641  data: 0.1804  max mem: 13008
Epoch: [9]  [1940/3542]  eta: 0:21:45  lr: 0.000001  min_lr: 0.000001  loss: 1.8643 (1.8474)  loss_scale: 8192.0000 (9749.3663)  weight_decay: 0.0500 (0.0500)  time: 0.6855  data: 0.1014  max mem: 13008
Epoch: [9]  [1950/3542]  eta: 0:21:36  lr: 0.000001  min_lr: 0.000001  loss: 1.8906 (1.8475)  loss_scale: 8192.0000 (9741.3839)  weight_decay: 0.0500 (0.0500)  time: 0.6968  data: 0.1113  max mem: 13008
Epoch: [9]  [1960/3542]  eta: 0:21:27  lr: 0.000001  min_lr: 0.000001  loss: 1.9131 (1.8477)  loss_scale: 8192.0000 (9733.4829)  weight_decay: 0.0500 (0.0500)  time: 0.6878  data: 0.1016  max mem: 13008
Epoch: [9]  [1970/3542]  eta: 0:21:17  lr: 0.000001  min_lr: 0.000001  loss: 1.8857 (1.8478)  loss_scale: 8192.0000 (9725.6621)  weight_decay: 0.0500 (0.0500)  time: 0.6386  data: 0.0538  max mem: 13008
[2023-05-16 14:02:06,529] [INFO] [fused_optimizer.py:330:_update_scale] No Grad overflow for 1000 iterations
[2023-05-16 14:02:06,530] [INFO] [fused_optimizer.py:332:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
Epoch: [9]  [1980/3542]  eta: 0:21:08  lr: 0.000001  min_lr: 0.000001  loss: 1.8398 (1.8479)  loss_scale: 8192.0000 (9742.7320)  weight_decay: 0.0500 (0.0500)  time: 0.6483  data: 0.0637  max mem: 13008
Epoch: [9]  [1990/3542]  eta: 0:20:59  lr: 0.000001  min_lr: 0.000001  loss: 1.8604 (1.8482)  loss_scale: 16384.0000 (9776.0884)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.0734  max mem: 13008
Epoch: [9]  [2000/3542]  eta: 0:20:49  lr: 0.000001  min_lr: 0.000001  loss: 1.8018 (1.8480)  loss_scale: 16384.0000 (9809.1114)  weight_decay: 0.0500 (0.0500)  time: 0.6658  data: 0.0810  max mem: 13008
Epoch: [9]  [2010/3542]  eta: 0:20:40  lr: 0.000001  min_lr: 0.000001  loss: 1.7910 (1.8481)  loss_scale: 16384.0000 (9841.8061)  weight_decay: 0.0500 (0.0500)  time: 0.6817  data: 0.0972  max mem: 13008
Epoch: [9]  [2020/3542]  eta: 0:20:31  lr: 0.000001  min_lr: 0.000001  loss: 1.7646 (1.8479)  loss_scale: 16384.0000 (9874.1771)  weight_decay: 0.0500 (0.0500)  time: 0.6586  data: 0.0738  max mem: 13008
Epoch: [9]  [2030/3542]  eta: 0:20:22  lr: 0.000001  min_lr: 0.000001  loss: 1.7646 (1.8475)  loss_scale: 16384.0000 (9906.2294)  weight_decay: 0.0500 (0.0500)  time: 0.6363  data: 0.0516  max mem: 13008
Epoch: [9]  [2040/3542]  eta: 0:20:12  lr: 0.000001  min_lr: 0.000001  loss: 1.8164 (1.8475)  loss_scale: 16384.0000 (9937.9677)  weight_decay: 0.0500 (0.0500)  time: 0.6487  data: 0.0652  max mem: 13008
Epoch: [9]  [2050/3542]  eta: 0:20:03  lr: 0.000001  min_lr: 0.000001  loss: 1.8164 (1.8473)  loss_scale: 16384.0000 (9969.3964)  weight_decay: 0.0500 (0.0500)  time: 0.6644  data: 0.0800  max mem: 13008
Epoch: [9]  [2060/3542]  eta: 0:19:54  lr: 0.000001  min_lr: 0.000001  loss: 1.7832 (1.8474)  loss_scale: 16384.0000 (10000.5201)  weight_decay: 0.0500 (0.0500)  time: 0.6325  data: 0.0474  max mem: 13008
Epoch: [9]  [2070/3542]  eta: 0:19:45  lr: 0.000001  min_lr: 0.000001  loss: 1.8770 (1.8477)  loss_scale: 16384.0000 (10031.3433)  weight_decay: 0.0500 (0.0500)  time: 0.6334  data: 0.0490  max mem: 13008
Epoch: [9]  [2080/3542]  eta: 0:19:35  lr: 0.000001  min_lr: 0.000001  loss: 1.8955 (1.8478)  loss_scale: 16384.0000 (10061.8703)  weight_decay: 0.0500 (0.0500)  time: 0.6459  data: 0.0612  max mem: 13008
Epoch: [9]  [2090/3542]  eta: 0:19:27  lr: 0.000001  min_lr: 0.000001  loss: 1.8906 (1.8480)  loss_scale: 16384.0000 (10092.1052)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.0652  max mem: 13008
Epoch: [9]  [2100/3542]  eta: 0:19:17  lr: 0.000001  min_lr: 0.000001  loss: 1.8730 (1.8481)  loss_scale: 16384.0000 (10122.0524)  weight_decay: 0.0500 (0.0500)  time: 0.6373  data: 0.0530  max mem: 13008
Epoch: [9]  [2110/3542]  eta: 0:19:08  lr: 0.000001  min_lr: 0.000001  loss: 1.9004 (1.8483)  loss_scale: 16384.0000 (10151.7158)  weight_decay: 0.0500 (0.0500)  time: 0.6370  data: 0.0508  max mem: 13008
Epoch: [9]  [2120/3542]  eta: 0:18:59  lr: 0.000001  min_lr: 0.000001  loss: 1.8350 (1.8482)  loss_scale: 16384.0000 (10181.0995)  weight_decay: 0.0500 (0.0500)  time: 0.6674  data: 0.0804  max mem: 13008
[2023-05-16 14:03:41,218] [INFO] [logging.py:60:log_dist] [Rank 0] step=34000, skipped=30, lr=[1.1908976194804132e-06, 1.1908976194804132e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 14:03:41,221] [INFO] [timer.py:157:stop] 0/34000, SamplesPerSec=55.56880120716386
Epoch: [9]  [2130/3542]  eta: 0:18:50  lr: 0.000001  min_lr: 0.000001  loss: 1.7920 (1.8482)  loss_scale: 16384.0000 (10210.2074)  weight_decay: 0.0500 (0.0500)  time: 0.6289  data: 0.0432  max mem: 13008
Epoch: [9]  [2140/3542]  eta: 0:18:41  lr: 0.000001  min_lr: 0.000001  loss: 1.7734 (1.8482)  loss_scale: 16384.0000 (10239.0434)  weight_decay: 0.0500 (0.0500)  time: 0.6512  data: 0.0663  max mem: 13008
Epoch: [9]  [2150/3542]  eta: 0:18:32  lr: 0.000001  min_lr: 0.000001  loss: 1.7852 (1.8479)  loss_scale: 16384.0000 (10267.6113)  weight_decay: 0.0500 (0.0500)  time: 0.6658  data: 0.0812  max mem: 13008
Epoch: [9]  [2160/3542]  eta: 0:18:23  lr: 0.000001  min_lr: 0.000001  loss: 1.8164 (1.8478)  loss_scale: 16384.0000 (10295.9149)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.0660  max mem: 13008
Epoch: [9]  [2170/3542]  eta: 0:18:14  lr: 0.000001  min_lr: 0.000001  loss: 1.7314 (1.8473)  loss_scale: 16384.0000 (10323.9576)  weight_decay: 0.0500 (0.0500)  time: 0.6447  data: 0.0601  max mem: 13008
Epoch: [9]  [2180/3542]  eta: 0:18:05  lr: 0.000001  min_lr: 0.000001  loss: 1.7783 (1.8472)  loss_scale: 16384.0000 (10351.7432)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0474  max mem: 13008
Epoch: [9]  [2190/3542]  eta: 0:17:56  lr: 0.000001  min_lr: 0.000001  loss: 1.8252 (1.8472)  loss_scale: 16384.0000 (10379.2752)  weight_decay: 0.0500 (0.0500)  time: 0.6450  data: 0.0611  max mem: 13008
Epoch: [9]  [2200/3542]  eta: 0:17:47  lr: 0.000001  min_lr: 0.000001  loss: 1.7871 (1.8469)  loss_scale: 16384.0000 (10406.5570)  weight_decay: 0.0500 (0.0500)  time: 0.6473  data: 0.0617  max mem: 13008
Epoch: [9]  [2210/3542]  eta: 0:17:40  lr: 0.000001  min_lr: 0.000001  loss: 1.8311 (1.8470)  loss_scale: 16384.0000 (10433.5920)  weight_decay: 0.0500 (0.0500)  time: 0.7384  data: 0.1526  max mem: 13008
Epoch: [9]  [2220/3542]  eta: 0:17:31  lr: 0.000001  min_lr: 0.000001  loss: 1.8789 (1.8471)  loss_scale: 16384.0000 (10460.3836)  weight_decay: 0.0500 (0.0500)  time: 0.7604  data: 0.1760  max mem: 13008
Epoch: [9]  [2230/3542]  eta: 0:17:22  lr: 0.000001  min_lr: 0.000001  loss: 1.8623 (1.8470)  loss_scale: 16384.0000 (10486.9350)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.0609  max mem: 13008
Epoch: [9]  [2240/3542]  eta: 0:17:13  lr: 0.000001  min_lr: 0.000001  loss: 1.8291 (1.8471)  loss_scale: 16384.0000 (10513.2494)  weight_decay: 0.0500 (0.0500)  time: 0.6172  data: 0.0313  max mem: 13008
Epoch: [9]  [2250/3542]  eta: 0:17:04  lr: 0.000001  min_lr: 0.000001  loss: 1.7998 (1.8467)  loss_scale: 16384.0000 (10539.3301)  weight_decay: 0.0500 (0.0500)  time: 0.6287  data: 0.0431  max mem: 13008
Epoch: [9]  [2260/3542]  eta: 0:16:55  lr: 0.000001  min_lr: 0.000001  loss: 1.7998 (1.8465)  loss_scale: 16384.0000 (10565.1800)  weight_decay: 0.0500 (0.0500)  time: 0.6424  data: 0.0571  max mem: 13008
Epoch: [9]  [2270/3542]  eta: 0:16:47  lr: 0.000001  min_lr: 0.000001  loss: 1.8076 (1.8463)  loss_scale: 16384.0000 (10590.8023)  weight_decay: 0.0500 (0.0500)  time: 0.6393  data: 0.0542  max mem: 13008
Epoch: [9]  [2280/3542]  eta: 0:16:38  lr: 0.000001  min_lr: 0.000001  loss: 1.8232 (1.8463)  loss_scale: 16384.0000 (10616.1999)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0449  max mem: 13008
Epoch: [9]  [2290/3542]  eta: 0:16:29  lr: 0.000001  min_lr: 0.000001  loss: 1.8574 (1.8464)  loss_scale: 16384.0000 (10641.3758)  weight_decay: 0.0500 (0.0500)  time: 0.6349  data: 0.0497  max mem: 13008
Epoch: [9]  [2300/3542]  eta: 0:16:20  lr: 0.000001  min_lr: 0.000001  loss: 1.8604 (1.8464)  loss_scale: 16384.0000 (10666.3329)  weight_decay: 0.0500 (0.0500)  time: 0.6426  data: 0.0566  max mem: 13008
Epoch: [9]  [2310/3542]  eta: 0:16:12  lr: 0.000001  min_lr: 0.000001  loss: 1.8252 (1.8462)  loss_scale: 16384.0000 (10691.0740)  weight_decay: 0.0500 (0.0500)  time: 0.6496  data: 0.0648  max mem: 13008
Epoch: [9]  [2320/3542]  eta: 0:16:03  lr: 0.000001  min_lr: 0.000001  loss: 1.7344 (1.8460)  loss_scale: 16384.0000 (10715.6019)  weight_decay: 0.0500 (0.0500)  time: 0.6191  data: 0.0342  max mem: 13008
Epoch: [9]  [2330/3542]  eta: 0:15:54  lr: 0.000001  min_lr: 0.000001  loss: 1.7910 (1.8460)  loss_scale: 16384.0000 (10739.9193)  weight_decay: 0.0500 (0.0500)  time: 0.6008  data: 0.0153  max mem: 13008
Epoch: [9]  [2340/3542]  eta: 0:15:45  lr: 0.000001  min_lr: 0.000001  loss: 1.7910 (1.8459)  loss_scale: 16384.0000 (10764.0290)  weight_decay: 0.0500 (0.0500)  time: 0.6259  data: 0.0410  max mem: 13008
Epoch: [9]  [2350/3542]  eta: 0:15:37  lr: 0.000001  min_lr: 0.000001  loss: 1.7803 (1.8458)  loss_scale: 16384.0000 (10787.9336)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0434  max mem: 13008
Epoch: [9]  [2360/3542]  eta: 0:15:28  lr: 0.000001  min_lr: 0.000001  loss: 1.7832 (1.8455)  loss_scale: 16384.0000 (10811.6357)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0287  max mem: 13008
Epoch: [9]  [2370/3542]  eta: 0:15:19  lr: 0.000001  min_lr: 0.000001  loss: 1.7930 (1.8455)  loss_scale: 16384.0000 (10835.1379)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0115  max mem: 13008
Epoch: [9]  [2380/3542]  eta: 0:15:10  lr: 0.000001  min_lr: 0.000001  loss: 1.8389 (1.8454)  loss_scale: 16384.0000 (10858.4427)  weight_decay: 0.0500 (0.0500)  time: 0.5861  data: 0.0004  max mem: 13008
Epoch: [9]  [2390/3542]  eta: 0:15:02  lr: 0.000001  min_lr: 0.000001  loss: 1.8457 (1.8457)  loss_scale: 16384.0000 (10881.5525)  weight_decay: 0.0500 (0.0500)  time: 0.5975  data: 0.0119  max mem: 13008
Epoch: [9]  [2400/3542]  eta: 0:14:53  lr: 0.000001  min_lr: 0.000001  loss: 1.8643 (1.8456)  loss_scale: 16384.0000 (10904.4698)  weight_decay: 0.0500 (0.0500)  time: 0.6192  data: 0.0339  max mem: 13008
Epoch: [9]  [2410/3542]  eta: 0:14:44  lr: 0.000001  min_lr: 0.000001  loss: 1.8242 (1.8454)  loss_scale: 16384.0000 (10927.1970)  weight_decay: 0.0500 (0.0500)  time: 0.6290  data: 0.0440  max mem: 13008
Epoch: [9]  [2420/3542]  eta: 0:14:36  lr: 0.000001  min_lr: 0.000001  loss: 1.8057 (1.8453)  loss_scale: 16384.0000 (10949.7365)  weight_decay: 0.0500 (0.0500)  time: 0.6195  data: 0.0338  max mem: 13008
Epoch: [9]  [2430/3542]  eta: 0:14:27  lr: 0.000001  min_lr: 0.000001  loss: 1.8193 (1.8454)  loss_scale: 16384.0000 (10972.0905)  weight_decay: 0.0500 (0.0500)  time: 0.6104  data: 0.0248  max mem: 13008
Epoch: [9]  [2440/3542]  eta: 0:14:19  lr: 0.000001  min_lr: 0.000001  loss: 1.8193 (1.8455)  loss_scale: 16384.0000 (10994.2614)  weight_decay: 0.0500 (0.0500)  time: 0.6043  data: 0.0196  max mem: 13008
Epoch: [9]  [2450/3542]  eta: 0:14:10  lr: 0.000001  min_lr: 0.000001  loss: 1.8535 (1.8458)  loss_scale: 16384.0000 (11016.2513)  weight_decay: 0.0500 (0.0500)  time: 0.6159  data: 0.0311  max mem: 13008
Epoch: [9]  [2460/3542]  eta: 0:14:02  lr: 0.000001  min_lr: 0.000001  loss: 1.8076 (1.8455)  loss_scale: 16384.0000 (11038.0626)  weight_decay: 0.0500 (0.0500)  time: 0.6098  data: 0.0247  max mem: 13008
Epoch: [9]  [2470/3542]  eta: 0:13:53  lr: 0.000001  min_lr: 0.000001  loss: 1.8076 (1.8455)  loss_scale: 16384.0000 (11059.6973)  weight_decay: 0.0500 (0.0500)  time: 0.5905  data: 0.0048  max mem: 13008
Epoch: [9]  [2480/3542]  eta: 0:13:45  lr: 0.000001  min_lr: 0.000001  loss: 1.8984 (1.8459)  loss_scale: 16384.0000 (11081.1576)  weight_decay: 0.0500 (0.0500)  time: 0.6077  data: 0.0224  max mem: 13008
Epoch: [9]  [2490/3542]  eta: 0:13:37  lr: 0.000001  min_lr: 0.000001  loss: 1.8584 (1.8456)  loss_scale: 16384.0000 (11102.4456)  weight_decay: 0.0500 (0.0500)  time: 0.7016  data: 0.1171  max mem: 13008
Epoch: [9]  [2500/3542]  eta: 0:13:28  lr: 0.000001  min_lr: 0.000001  loss: 1.7773 (1.8458)  loss_scale: 16384.0000 (11123.5634)  weight_decay: 0.0500 (0.0500)  time: 0.7009  data: 0.1166  max mem: 13008
Epoch: [9]  [2510/3542]  eta: 0:13:20  lr: 0.000001  min_lr: 0.000001  loss: 1.8447 (1.8459)  loss_scale: 16384.0000 (11144.5129)  weight_decay: 0.0500 (0.0500)  time: 0.6454  data: 0.0603  max mem: 13008
Epoch: [9]  [2520/3542]  eta: 0:13:12  lr: 0.000001  min_lr: 0.000001  loss: 1.8428 (1.8459)  loss_scale: 16384.0000 (11165.2963)  weight_decay: 0.0500 (0.0500)  time: 0.6602  data: 0.0749  max mem: 13008
Epoch: [9]  [2530/3542]  eta: 0:13:04  lr: 0.000001  min_lr: 0.000001  loss: 1.8428 (1.8458)  loss_scale: 16384.0000 (11185.9154)  weight_decay: 0.0500 (0.0500)  time: 0.6315  data: 0.0465  max mem: 13008
Epoch: [9]  [2540/3542]  eta: 0:12:55  lr: 0.000001  min_lr: 0.000001  loss: 1.8584 (1.8458)  loss_scale: 16384.0000 (11206.3723)  weight_decay: 0.0500 (0.0500)  time: 0.6299  data: 0.0437  max mem: 13008
Epoch: [9]  [2550/3542]  eta: 0:12:47  lr: 0.000001  min_lr: 0.000001  loss: 1.8584 (1.8457)  loss_scale: 16384.0000 (11226.6688)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0347  max mem: 13008
Epoch: [9]  [2560/3542]  eta: 0:12:38  lr: 0.000001  min_lr: 0.000001  loss: 1.8164 (1.8457)  loss_scale: 16384.0000 (11246.8067)  weight_decay: 0.0500 (0.0500)  time: 0.5911  data: 0.0059  max mem: 13008
Epoch: [9]  [2570/3542]  eta: 0:12:30  lr: 0.000001  min_lr: 0.000001  loss: 1.8164 (1.8456)  loss_scale: 16384.0000 (11266.7880)  weight_decay: 0.0500 (0.0500)  time: 0.6131  data: 0.0273  max mem: 13008
Epoch: [9]  [2580/3542]  eta: 0:12:22  lr: 0.000001  min_lr: 0.000001  loss: 1.7959 (1.8454)  loss_scale: 16384.0000 (11286.6145)  weight_decay: 0.0500 (0.0500)  time: 0.6208  data: 0.0346  max mem: 13008
Epoch: [9]  [2590/3542]  eta: 0:12:14  lr: 0.000001  min_lr: 0.000001  loss: 1.7578 (1.8452)  loss_scale: 16384.0000 (11306.2879)  weight_decay: 0.0500 (0.0500)  time: 0.6242  data: 0.0380  max mem: 13008
Epoch: [9]  [2600/3542]  eta: 0:12:05  lr: 0.000001  min_lr: 0.000001  loss: 1.8086 (1.8451)  loss_scale: 16384.0000 (11325.8101)  weight_decay: 0.0500 (0.0500)  time: 0.6428  data: 0.0567  max mem: 13008
Epoch: [9]  [2610/3542]  eta: 0:11:57  lr: 0.000001  min_lr: 0.000001  loss: 1.8086 (1.8451)  loss_scale: 16384.0000 (11345.1827)  weight_decay: 0.0500 (0.0500)  time: 0.6124  data: 0.0263  max mem: 13008
Epoch: [9]  [2620/3542]  eta: 0:11:49  lr: 0.000001  min_lr: 0.000001  loss: 1.8076 (1.8447)  loss_scale: 16384.0000 (11364.4075)  weight_decay: 0.0500 (0.0500)  time: 0.6284  data: 0.0424  max mem: 13008
Epoch: [9]  [2630/3542]  eta: 0:11:41  lr: 0.000001  min_lr: 0.000001  loss: 1.8262 (1.8449)  loss_scale: 16384.0000 (11383.4861)  weight_decay: 0.0500 (0.0500)  time: 0.6634  data: 0.0782  max mem: 13008
Epoch: [9]  [2640/3542]  eta: 0:11:33  lr: 0.000001  min_lr: 0.000001  loss: 1.8604 (1.8446)  loss_scale: 16384.0000 (11402.4203)  weight_decay: 0.0500 (0.0500)  time: 0.6415  data: 0.0566  max mem: 13008
Epoch: [9]  [2650/3542]  eta: 0:11:25  lr: 0.000001  min_lr: 0.000001  loss: 1.7471 (1.8445)  loss_scale: 16384.0000 (11421.2116)  weight_decay: 0.0500 (0.0500)  time: 0.6298  data: 0.0442  max mem: 13008
Epoch: [9]  [2660/3542]  eta: 0:11:16  lr: 0.000001  min_lr: 0.000001  loss: 1.8066 (1.8442)  loss_scale: 16384.0000 (11439.8617)  weight_decay: 0.0500 (0.0500)  time: 0.6092  data: 0.0238  max mem: 13008
Epoch: [9]  [2670/3542]  eta: 0:11:08  lr: 0.000001  min_lr: 0.000001  loss: 1.8145 (1.8441)  loss_scale: 16384.0000 (11458.3721)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0161  max mem: 13008
Epoch: [9]  [2680/3542]  eta: 0:11:01  lr: 0.000001  min_lr: 0.000001  loss: 1.8525 (1.8442)  loss_scale: 16384.0000 (11476.7445)  weight_decay: 0.0500 (0.0500)  time: 0.7185  data: 0.1336  max mem: 13008
Epoch: [9]  [2690/3542]  eta: 0:10:53  lr: 0.000001  min_lr: 0.000001  loss: 1.8369 (1.8439)  loss_scale: 16384.0000 (11494.9803)  weight_decay: 0.0500 (0.0500)  time: 0.7380  data: 0.1533  max mem: 13008
Epoch: [9]  [2700/3542]  eta: 0:10:44  lr: 0.000001  min_lr: 0.000001  loss: 1.8262 (1.8440)  loss_scale: 16384.0000 (11513.0811)  weight_decay: 0.0500 (0.0500)  time: 0.6367  data: 0.0523  max mem: 13008
Epoch: [9]  [2710/3542]  eta: 0:10:36  lr: 0.000001  min_lr: 0.000001  loss: 1.8438 (1.8440)  loss_scale: 16384.0000 (11531.0483)  weight_decay: 0.0500 (0.0500)  time: 0.6126  data: 0.0279  max mem: 13008
Epoch: [9]  [2720/3542]  eta: 0:10:28  lr: 0.000001  min_lr: 0.000001  loss: 1.8438 (1.8439)  loss_scale: 16384.0000 (11548.8835)  weight_decay: 0.0500 (0.0500)  time: 0.6206  data: 0.0356  max mem: 13008
Epoch: [9]  [2730/3542]  eta: 0:10:20  lr: 0.000001  min_lr: 0.000001  loss: 1.8574 (1.8440)  loss_scale: 16384.0000 (11566.5881)  weight_decay: 0.0500 (0.0500)  time: 0.6174  data: 0.0330  max mem: 13008
Epoch: [9]  [2740/3542]  eta: 0:10:12  lr: 0.000001  min_lr: 0.000001  loss: 1.8574 (1.8440)  loss_scale: 16384.0000 (11584.1634)  weight_decay: 0.0500 (0.0500)  time: 0.6002  data: 0.0150  max mem: 13008
Epoch: [9]  [2750/3542]  eta: 0:10:04  lr: 0.000001  min_lr: 0.000001  loss: 1.8037 (1.8438)  loss_scale: 16384.0000 (11601.6111)  weight_decay: 0.0500 (0.0500)  time: 0.5997  data: 0.0141  max mem: 13008
Epoch: [9]  [2760/3542]  eta: 0:09:56  lr: 0.000001  min_lr: 0.000001  loss: 1.8398 (1.8439)  loss_scale: 16384.0000 (11618.9323)  weight_decay: 0.0500 (0.0500)  time: 0.6316  data: 0.0464  max mem: 13008
Epoch: [9]  [2770/3542]  eta: 0:09:48  lr: 0.000001  min_lr: 0.000001  loss: 1.8555 (1.8439)  loss_scale: 16384.0000 (11636.1285)  weight_decay: 0.0500 (0.0500)  time: 0.6588  data: 0.0725  max mem: 13008
Epoch: [9]  [2780/3542]  eta: 0:09:40  lr: 0.000001  min_lr: 0.000001  loss: 1.8555 (1.8439)  loss_scale: 16384.0000 (11653.2010)  weight_decay: 0.0500 (0.0500)  time: 0.6650  data: 0.0791  max mem: 13008
Epoch: [9]  [2790/3542]  eta: 0:09:32  lr: 0.000001  min_lr: 0.000001  loss: 1.8604 (1.8439)  loss_scale: 16384.0000 (11670.1512)  weight_decay: 0.0500 (0.0500)  time: 0.6436  data: 0.0589  max mem: 13008
Epoch: [9]  [2800/3542]  eta: 0:09:25  lr: 0.000001  min_lr: 0.000001  loss: 1.8604 (1.8439)  loss_scale: 16384.0000 (11686.9804)  weight_decay: 0.0500 (0.0500)  time: 0.7039  data: 0.1194  max mem: 13008
Epoch: [9]  [2810/3542]  eta: 0:09:17  lr: 0.000001  min_lr: 0.000001  loss: 1.8594 (1.8441)  loss_scale: 16384.0000 (11703.6898)  weight_decay: 0.0500 (0.0500)  time: 0.7261  data: 0.1414  max mem: 13008
Epoch: [9]  [2820/3542]  eta: 0:09:09  lr: 0.000001  min_lr: 0.000001  loss: 1.8594 (1.8440)  loss_scale: 16384.0000 (11720.2808)  weight_decay: 0.0500 (0.0500)  time: 0.7295  data: 0.1452  max mem: 13008
Epoch: [9]  [2830/3542]  eta: 0:09:01  lr: 0.000001  min_lr: 0.000001  loss: 1.9004 (1.8440)  loss_scale: 16384.0000 (11736.7545)  weight_decay: 0.0500 (0.0500)  time: 0.7017  data: 0.1182  max mem: 13008
Epoch: [9]  [2840/3542]  eta: 0:08:54  lr: 0.000001  min_lr: 0.000001  loss: 1.8311 (1.8440)  loss_scale: 16384.0000 (11753.1123)  weight_decay: 0.0500 (0.0500)  time: 0.6549  data: 0.0705  max mem: 13008
Epoch: [9]  [2850/3542]  eta: 0:08:46  lr: 0.000001  min_lr: 0.000001  loss: 1.8564 (1.8444)  loss_scale: 16384.0000 (11769.3553)  weight_decay: 0.0500 (0.0500)  time: 0.7167  data: 0.1320  max mem: 13008
[2023-05-16 14:11:31,186] [INFO] [fused_optimizer.py:320:_update_scale] 
Grad overflow on iteration 34730
[2023-05-16 14:11:31,186] [INFO] [fused_optimizer.py:321:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-05-16 14:11:31,186] [INFO] [logging.py:60:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
Epoch: [9]  [2860/3542]  eta: 0:08:38  lr: 0.000001  min_lr: 0.000001  loss: 1.9053 (1.8445)  loss_scale: 16384.0000 (11759.7148)  weight_decay: 0.0500 (0.0500)  time: 0.6906  data: 0.1064  max mem: 13008
Epoch: [9]  [2870/3542]  eta: 0:08:30  lr: 0.000001  min_lr: 0.000001  loss: 1.8076 (1.8443)  loss_scale: 8192.0000 (11747.2881)  weight_decay: 0.0500 (0.0500)  time: 0.6213  data: 0.0365  max mem: 13008
Epoch: [9]  [2880/3542]  eta: 0:08:22  lr: 0.000001  min_lr: 0.000001  loss: 1.7920 (1.8443)  loss_scale: 8192.0000 (11734.9476)  weight_decay: 0.0500 (0.0500)  time: 0.6181  data: 0.0331  max mem: 13008
Epoch: [9]  [2890/3542]  eta: 0:08:14  lr: 0.000001  min_lr: 0.000001  loss: 1.8203 (1.8443)  loss_scale: 8192.0000 (11722.6925)  weight_decay: 0.0500 (0.0500)  time: 0.6503  data: 0.0656  max mem: 13008
Epoch: [9]  [2900/3542]  eta: 0:08:06  lr: 0.000001  min_lr: 0.000001  loss: 1.8145 (1.8441)  loss_scale: 8192.0000 (11710.5219)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.0609  max mem: 13008
Epoch: [9]  [2910/3542]  eta: 0:07:59  lr: 0.000001  min_lr: 0.000001  loss: 1.8047 (1.8440)  loss_scale: 8192.0000 (11698.4349)  weight_decay: 0.0500 (0.0500)  time: 0.6448  data: 0.0599  max mem: 13008
Epoch: [9]  [2920/3542]  eta: 0:07:51  lr: 0.000001  min_lr: 0.000001  loss: 1.8174 (1.8440)  loss_scale: 8192.0000 (11686.4307)  weight_decay: 0.0500 (0.0500)  time: 0.6505  data: 0.0656  max mem: 13008
Epoch: [9]  [2930/3542]  eta: 0:07:43  lr: 0.000001  min_lr: 0.000001  loss: 1.8438 (1.8444)  loss_scale: 8192.0000 (11674.5084)  weight_decay: 0.0500 (0.0500)  time: 0.6366  data: 0.0518  max mem: 13008
Epoch: [9]  [2940/3542]  eta: 0:07:35  lr: 0.000001  min_lr: 0.000001  loss: 1.9512 (1.8447)  loss_scale: 8192.0000 (11662.6671)  weight_decay: 0.0500 (0.0500)  time: 0.6268  data: 0.0427  max mem: 13008
Epoch: [9]  [2950/3542]  eta: 0:07:27  lr: 0.000001  min_lr: 0.000001  loss: 1.8691 (1.8447)  loss_scale: 8192.0000 (11650.9061)  weight_decay: 0.0500 (0.0500)  time: 0.6541  data: 0.0706  max mem: 13008
Epoch: [9]  [2960/3542]  eta: 0:07:20  lr: 0.000001  min_lr: 0.000001  loss: 1.7822 (1.8447)  loss_scale: 8192.0000 (11639.2246)  weight_decay: 0.0500 (0.0500)  time: 0.6646  data: 0.0806  max mem: 13008
Epoch: [9]  [2970/3542]  eta: 0:07:12  lr: 0.000001  min_lr: 0.000001  loss: 1.8496 (1.8448)  loss_scale: 8192.0000 (11627.6217)  weight_decay: 0.0500 (0.0500)  time: 0.6863  data: 0.1015  max mem: 13008
Epoch: [9]  [2980/3542]  eta: 0:07:04  lr: 0.000001  min_lr: 0.000001  loss: 1.8877 (1.8448)  loss_scale: 8192.0000 (11616.0966)  weight_decay: 0.0500 (0.0500)  time: 0.6702  data: 0.0852  max mem: 13008
Epoch: [9]  [2990/3542]  eta: 0:06:56  lr: 0.000001  min_lr: 0.000001  loss: 1.8027 (1.8447)  loss_scale: 8192.0000 (11604.6486)  weight_decay: 0.0500 (0.0500)  time: 0.6364  data: 0.0516  max mem: 13008
Epoch: [9]  [3000/3542]  eta: 0:06:49  lr: 0.000001  min_lr: 0.000001  loss: 1.8281 (1.8446)  loss_scale: 8192.0000 (11593.2769)  weight_decay: 0.0500 (0.0500)  time: 0.6504  data: 0.0646  max mem: 13008
Epoch: [9]  [3010/3542]  eta: 0:06:41  lr: 0.000001  min_lr: 0.000001  loss: 1.8428 (1.8448)  loss_scale: 8192.0000 (11581.9807)  weight_decay: 0.0500 (0.0500)  time: 0.6297  data: 0.0448  max mem: 13008
Epoch: [9]  [3020/3542]  eta: 0:06:33  lr: 0.000001  min_lr: 0.000001  loss: 1.8486 (1.8450)  loss_scale: 8192.0000 (11570.7594)  weight_decay: 0.0500 (0.0500)  time: 0.6495  data: 0.0661  max mem: 13008
Epoch: [9]  [3030/3542]  eta: 0:06:25  lr: 0.000001  min_lr: 0.000001  loss: 1.8291 (1.8452)  loss_scale: 8192.0000 (11559.6120)  weight_decay: 0.0500 (0.0500)  time: 0.6536  data: 0.0697  max mem: 13008
Epoch: [9]  [3040/3542]  eta: 0:06:18  lr: 0.000001  min_lr: 0.000001  loss: 1.8994 (1.8454)  loss_scale: 8192.0000 (11548.5380)  weight_decay: 0.0500 (0.0500)  time: 0.6360  data: 0.0511  max mem: 13008
Epoch: [9]  [3050/3542]  eta: 0:06:10  lr: 0.000001  min_lr: 0.000001  loss: 1.8779 (1.8455)  loss_scale: 8192.0000 (11537.5365)  weight_decay: 0.0500 (0.0500)  time: 0.6391  data: 0.0532  max mem: 13008
Epoch: [9]  [3060/3542]  eta: 0:06:02  lr: 0.000001  min_lr: 0.000001  loss: 1.8535 (1.8456)  loss_scale: 8192.0000 (11526.6070)  weight_decay: 0.0500 (0.0500)  time: 0.6221  data: 0.0364  max mem: 13008
Epoch: [9]  [3070/3542]  eta: 0:05:55  lr: 0.000001  min_lr: 0.000001  loss: 1.8535 (1.8456)  loss_scale: 8192.0000 (11515.7486)  weight_decay: 0.0500 (0.0500)  time: 0.6402  data: 0.0549  max mem: 13008
Epoch: [9]  [3080/3542]  eta: 0:05:47  lr: 0.000001  min_lr: 0.000001  loss: 1.8584 (1.8456)  loss_scale: 8192.0000 (11504.9607)  weight_decay: 0.0500 (0.0500)  time: 0.6895  data: 0.1036  max mem: 13008
Epoch: [9]  [3090/3542]  eta: 0:05:39  lr: 0.000001  min_lr: 0.000001  loss: 1.7832 (1.8453)  loss_scale: 8192.0000 (11494.2426)  weight_decay: 0.0500 (0.0500)  time: 0.7061  data: 0.1205  max mem: 13008
Epoch: [9]  [3100/3542]  eta: 0:05:32  lr: 0.000001  min_lr: 0.000001  loss: 1.7588 (1.8453)  loss_scale: 8192.0000 (11483.5937)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0743  max mem: 13008
Epoch: [9]  [3110/3542]  eta: 0:05:24  lr: 0.000001  min_lr: 0.000001  loss: 1.8525 (1.8452)  loss_scale: 8192.0000 (11473.0132)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.0499  max mem: 13008
Epoch: [9]  [3120/3542]  eta: 0:05:16  lr: 0.000001  min_lr: 0.000001  loss: 1.8398 (1.8453)  loss_scale: 8192.0000 (11462.5005)  weight_decay: 0.0500 (0.0500)  time: 0.6518  data: 0.0666  max mem: 13008
[2023-05-16 14:14:25,196] [INFO] [logging.py:60:log_dist] [Rank 0] step=35000, skipped=31, lr=[1.0167812427493024e-06, 1.0167812427493024e-06], mom=[[0.9, 0.999], [0.9, 0.999]]
[2023-05-16 14:14:25,198] [INFO] [timer.py:157:stop] 0/35000, SamplesPerSec=55.56948495429926
Epoch: [9]  [3130/3542]  eta: 0:05:09  lr: 0.000001  min_lr: 0.000001  loss: 1.8711 (1.8453)  loss_scale: 8192.0000 (11452.0549)  weight_decay: 0.0500 (0.0500)  time: 0.6673  data: 0.0812  max mem: 13008
Epoch: [9]  [3140/3542]  eta: 0:05:01  lr: 0.000001  min_lr: 0.000001  loss: 1.9053 (1.8455)  loss_scale: 8192.0000 (11441.6759)  weight_decay: 0.0500 (0.0500)  time: 0.6887  data: 0.1031  max mem: 13008
Epoch: [9]  [3150/3542]  eta: 0:04:54  lr: 0.000001  min_lr: 0.000001  loss: 1.8525 (1.8454)  loss_scale: 8192.0000 (11431.3627)  weight_decay: 0.0500 (0.0500)  time: 0.6573  data: 0.0723  max mem: 13008
Epoch: [9]  [3160/3542]  eta: 0:04:46  lr: 0.000001  min_lr: 0.000001  loss: 1.8066 (1.8453)  loss_scale: 8192.0000 (11421.1148)  weight_decay: 0.0500 (0.0500)  time: 0.6223  data: 0.0362  max mem: 13008
Epoch: [9]  [3170/3542]  eta: 0:04:38  lr: 0.000001  min_lr: 0.000001  loss: 1.7822 (1.8451)  loss_scale: 8192.0000 (11410.9316)  weight_decay: 0.0500 (0.0500)  time: 0.6285  data: 0.0425  max mem: 13008
Epoch: [9]  [3180/3542]  eta: 0:04:31  lr: 0.000001  min_lr: 0.000001  loss: 1.7930 (1.8452)  loss_scale: 8192.0000 (11400.8123)  weight_decay: 0.0500 (0.0500)  time: 0.6140  data: 0.0287  max mem: 13008
Epoch: [9]  [3190/3542]  eta: 0:04:23  lr: 0.000001  min_lr: 0.000001  loss: 1.7930 (1.8452)  loss_scale: 8192.0000 (11390.7565)  weight_decay: 0.0500 (0.0500)  time: 0.6460  data: 0.0606  max mem: 13008
Epoch: [9]  [3200/3542]  eta: 0:04:15  lr: 0.000001  min_lr: 0.000001  loss: 1.8018 (1.8451)  loss_scale: 8192.0000 (11380.7635)  weight_decay: 0.0500 (0.0500)  time: 0.6612  data: 0.0760  max mem: 13008
Epoch: [9]  [3210/3542]  eta: 0:04:08  lr: 0.000001  min_lr: 0.000001  loss: 1.8467 (1.8451)  loss_scale: 8192.0000 (11370.8328)  weight_decay: 0.0500 (0.0500)  time: 0.6444  data: 0.0592  max mem: 13008
Epoch: [9]  [3220/3542]  eta: 0:04:00  lr: 0.000001  min_lr: 0.000001  loss: 1.9287 (1.8453)  loss_scale: 8192.0000 (11360.9637)  weight_decay: 0.0500 (0.0500)  time: 0.6357  data: 0.0506  max mem: 13008
Epoch: [9]  [3230/3542]  eta: 0:03:53  lr: 0.000001  min_lr: 0.000001  loss: 1.8652 (1.8452)  loss_scale: 8192.0000 (11351.1557)  weight_decay: 0.0500 (0.0500)  time: 0.6149  data: 0.0299  max mem: 13008
Epoch: [9]  [3240/3542]  eta: 0:03:45  lr: 0.000001  min_lr: 0.000001  loss: 1.7852 (1.8452)  loss_scale: 8192.0000 (11341.4082)  weight_decay: 0.0500 (0.0500)  time: 0.6302  data: 0.0455  max mem: 13008
Epoch: [9]  [3250/3542]  eta: 0:03:37  lr: 0.000001  min_lr: 0.000001  loss: 1.7959 (1.8451)  loss_scale: 8192.0000 (11331.7207)  weight_decay: 0.0500 (0.0500)  time: 0.6156  data: 0.0312  max mem: 13008
Epoch: [9]  [3260/3542]  eta: 0:03:30  lr: 0.000001  min_lr: 0.000001  loss: 1.8154 (1.8450)  loss_scale: 8192.0000 (11322.0926)  weight_decay: 0.0500 (0.0500)  time: 0.5890  data: 0.0032  max mem: 13008
Epoch: [9]  [3270/3542]  eta: 0:03:22  lr: 0.000001  min_lr: 0.000001  loss: 1.7783 (1.8448)  loss_scale: 8192.0000 (11312.5234)  weight_decay: 0.0500 (0.0500)  time: 0.6227  data: 0.0372  max mem: 13008
Epoch: [9]  [3280/3542]  eta: 0:03:15  lr: 0.000001  min_lr: 0.000001  loss: 1.7832 (1.8449)  loss_scale: 8192.0000 (11303.0125)  weight_decay: 0.0500 (0.0500)  time: 0.6445  data: 0.0600  max mem: 13008
Epoch: [9]  [3290/3542]  eta: 0:03:07  lr: 0.000001  min_lr: 0.000001  loss: 1.8271 (1.8448)  loss_scale: 8192.0000 (11293.5594)  weight_decay: 0.0500 (0.0500)  time: 0.6296  data: 0.0445  max mem: 13008
Epoch: [9]  [3300/3542]  eta: 0:03:00  lr: 0.000001  min_lr: 0.000001  loss: 1.7871 (1.8448)  loss_scale: 8192.0000 (11284.1636)  weight_decay: 0.0500 (0.0500)  time: 0.6279  data: 0.0431  max mem: 13008
Epoch: [9]  [3310/3542]  eta: 0:02:52  lr: 0.000001  min_lr: 0.000001  loss: 1.7871 (1.8446)  loss_scale: 8192.0000 (11274.8245)  weight_decay: 0.0500 (0.0500)  time: 0.6186  data: 0.0345  max mem: 13008
Epoch: [9]  [3320/3542]  eta: 0:02:45  lr: 0.000001  min_lr: 0.000001  loss: 1.7979 (1.8445)  loss_scale: 8192.0000 (11265.5417)  weight_decay: 0.0500 (0.0500)  time: 0.6016  data: 0.0169  max mem: 13008
Epoch: [9]  [3330/3542]  eta: 0:02:37  lr: 0.000001  min_lr: 0.000001  loss: 1.8506 (1.8448)  loss_scale: 8192.0000 (11256.3146)  weight_decay: 0.0500 (0.0500)  time: 0.6332  data: 0.0471  max mem: 13008
Epoch: [9]  [3340/3542]  eta: 0:02:30  lr: 0.000001  min_lr: 0.000001  loss: 1.8779 (1.8449)  loss_scale: 8192.0000 (11247.1428)  weight_decay: 0.0500 (0.0500)  time: 0.6356  data: 0.0497  max mem: 13008
Epoch: [9]  [3350/3542]  eta: 0:02:22  lr: 0.000001  min_lr: 0.000001  loss: 1.8262 (1.8448)  loss_scale: 8192.0000 (11238.0257)  weight_decay: 0.0500 (0.0500)  time: 0.6463  data: 0.0611  max mem: 13008
Epoch: [9]  [3360/3542]  eta: 0:02:15  lr: 0.000001  min_lr: 0.000001  loss: 1.8262 (1.8450)  loss_scale: 8192.0000 (11228.9628)  weight_decay: 0.0500 (0.0500)  time: 0.6733  data: 0.0882  max mem: 13008
Epoch: [9]  [3370/3542]  eta: 0:02:07  lr: 0.000001  min_lr: 0.000001  loss: 1.8564 (1.8450)  loss_scale: 8192.0000 (11219.9537)  weight_decay: 0.0500 (0.0500)  time: 0.6217  data: 0.0365  max mem: 13008
Epoch: [9]  [3380/3542]  eta: 0:02:00  lr: 0.000001  min_lr: 0.000001  loss: 1.8564 (1.8451)  loss_scale: 8192.0000 (11210.9979)  weight_decay: 0.0500 (0.0500)  time: 0.6080  data: 0.0227  max mem: 13008
Epoch: [9]  [3390/3542]  eta: 0:01:52  lr: 0.000001  min_lr: 0.000001  loss: 1.8398 (1.8451)  loss_scale: 8192.0000 (11202.0950)  weight_decay: 0.0500 (0.0500)  time: 0.6079  data: 0.0227  max mem: 13008
Epoch: [9]  [3400/3542]  eta: 0:01:45  lr: 0.000001  min_lr: 0.000001  loss: 1.8066 (1.8452)  loss_scale: 8192.0000 (11193.2443)  weight_decay: 0.0500 (0.0500)  time: 0.6014  data: 0.0166  max mem: 13008
Epoch: [9]  [3410/3542]  eta: 0:01:37  lr: 0.000001  min_lr: 0.000001  loss: 1.8516 (1.8452)  loss_scale: 8192.0000 (11184.4456)  weight_decay: 0.0500 (0.0500)  time: 0.6404  data: 0.0551  max mem: 13008
Epoch: [9]  [3420/3542]  eta: 0:01:30  lr: 0.000001  min_lr: 0.000001  loss: 1.8125 (1.8453)  loss_scale: 8192.0000 (11175.6983)  weight_decay: 0.0500 (0.0500)  time: 0.6511  data: 0.0656  max mem: 13008
Epoch: [9]  [3430/3542]  eta: 0:01:22  lr: 0.000001  min_lr: 0.000001  loss: 1.9004 (1.8456)  loss_scale: 8192.0000 (11167.0020)  weight_decay: 0.0500 (0.0500)  time: 0.6291  data: 0.0439  max mem: 13008
Epoch: [9]  [3440/3542]  eta: 0:01:15  lr: 0.000001  min_lr: 0.000001  loss: 1.9004 (1.8458)  loss_scale: 8192.0000 (11158.3563)  weight_decay: 0.0500 (0.0500)  time: 0.6443  data: 0.0590  max mem: 13008
Epoch: [9]  [3450/3542]  eta: 0:01:08  lr: 0.000001  min_lr: 0.000001  loss: 1.8438 (1.8457)  loss_scale: 8192.0000 (11149.7606)  weight_decay: 0.0500 (0.0500)  time: 0.6276  data: 0.0421  max mem: 13008
Epoch: [9]  [3460/3542]  eta: 0:01:00  lr: 0.000001  min_lr: 0.000001  loss: 1.8438 (1.8456)  loss_scale: 8192.0000 (11141.2147)  weight_decay: 0.0500 (0.0500)  time: 0.5961  data: 0.0107  max mem: 13008
Epoch: [9]  [3470/3542]  eta: 0:00:53  lr: 0.000001  min_lr: 0.000001  loss: 1.8496 (1.8457)  loss_scale: 8192.0000 (11132.7179)  weight_decay: 0.0500 (0.0500)  time: 0.6351  data: 0.0495  max mem: 13008
Epoch: [9]  [3480/3542]  eta: 0:00:45  lr: 0.000001  min_lr: 0.000001  loss: 1.8613 (1.8457)  loss_scale: 8192.0000 (11124.2700)  weight_decay: 0.0500 (0.0500)  time: 0.6597  data: 0.0749  max mem: 13008
Epoch: [9]  [3490/3542]  eta: 0:00:38  lr: 0.000001  min_lr: 0.000001  loss: 1.8193 (1.8455)  loss_scale: 8192.0000 (11115.8705)  weight_decay: 0.0500 (0.0500)  time: 0.6355  data: 0.0508  max mem: 13008
Epoch: [9]  [3500/3542]  eta: 0:00:31  lr: 0.000001  min_lr: 0.000001  loss: 1.8252 (1.8455)  loss_scale: 8192.0000 (11107.5190)  weight_decay: 0.0500 (0.0500)  time: 0.6661  data: 0.0804  max mem: 13008
Epoch: [9]  [3510/3542]  eta: 0:00:23  lr: 0.000001  min_lr: 0.000001  loss: 1.8096 (1.8453)  loss_scale: 8192.0000 (11099.2150)  weight_decay: 0.0500 (0.0500)  time: 0.6757  data: 0.0898  max mem: 13008
Epoch: [9]  [3520/3542]  eta: 0:00:16  lr: 0.000001  min_lr: 0.000001  loss: 1.7812 (1.8450)  loss_scale: 8192.0000 (11090.9583)  weight_decay: 0.0500 (0.0500)  time: 0.6338  data: 0.0484  max mem: 13008
Epoch: [9]  [3530/3542]  eta: 0:00:08  lr: 0.000001  min_lr: 0.000001  loss: 1.8193 (1.8450)  loss_scale: 8192.0000 (11082.7482)  weight_decay: 0.0500 (0.0500)  time: 0.6096  data: 0.0243  max mem: 13008
Epoch: [9]  [3540/3542]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 1.8682 (1.8449)  loss_scale: 8192.0000 (11074.5846)  weight_decay: 0.0500 (0.0500)  time: 0.5849  data: 0.0003  max mem: 13008
Epoch: [9]  [3541/3542]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 1.8682 (1.8449)  loss_scale: 8192.0000 (11073.7708)  weight_decay: 0.0500 (0.0500)  time: 0.5845  data: 0.0003  max mem: 13008
Epoch: [9] Total time: 0:43:30 (0.7371 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 1.8682 (1.8449)  loss_scale: 8192.0000 (11073.7708)  weight_decay: 0.0500 (0.0500)
/scratch/mm12318/mambaforge/envs/beit/lib/python3.8/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2023-05-16 14:18:51,168] [INFO] [logging.py:60:log_dist] [Rank 0] Saving model checkpoint: ./output_freeze/checkpoint-9/mp_rank_00_model_states.pt
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [  0/156]  eta: 0:35:36    time: 13.6943  data: 9.9373  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 10/156]  eta: 0:11:04    time: 4.5480  data: 0.9036  max mem: 13008
Test:  [ 20/156]  eta: 0:09:14    time: 3.5941  data: 0.0002  max mem: 13008
Test:  [ 30/156]  eta: 0:08:10    time: 3.5307  data: 0.0002  max mem: 13008
/home/mm12318/DL_Class/BEiT/unilm/beit3/randaug.py:31: RuntimeWarning: overflow encountered in scalar negative
  offset = -low * scale
Test:  [ 40/156]  eta: 0:07:21    time: 3.5229  data: 0.0002  max mem: 13008
Test:  [ 50/156]  eta: 0:06:39    time: 3.5694  data: 0.0002  max mem: 13008
Test:  [ 60/156]  eta: 0:05:56    time: 3.5231  data: 0.0002  max mem: 13008
Test:  [ 70/156]  eta: 0:05:18    time: 3.5450  data: 0.0002  max mem: 13008
Test:  [ 80/156]  eta: 0:04:40    time: 3.6053  data: 0.0002  max mem: 13008
Test:  [ 90/156]  eta: 0:04:02    time: 3.5974  data: 0.0002  max mem: 13008
Test:  [100/156]  eta: 0:03:25    time: 3.6099  data: 0.0002  max mem: 13008
Test:  [110/156]  eta: 0:02:48    time: 3.5890  data: 0.0002  max mem: 13008
Test:  [120/156]  eta: 0:02:11    time: 3.6396  data: 0.0002  max mem: 13008
Test:  [130/156]  eta: 0:01:35    time: 3.6938  data: 0.0002  max mem: 13008
Test:  [140/156]  eta: 0:00:58    time: 3.6455  data: 0.0002  max mem: 13008
Test:  [150/156]  eta: 0:00:21    time: 3.5404  data: 0.0001  max mem: 13008
Test:  [155/156]  eta: 0:00:03    time: 3.5326  data: 0.0001  max mem: 13008
Test: Total time: 0:09:29 (3.6534 s / it)
coco_captioning
Global rank for dumping predictions: 0
Infer 4992 examples into ./output_freeze/submit_coco_captioning_val_e9.json
Prediction file is ./output_freeze/submit_coco_captioning_val_e9.json and result file is ./output_freeze/coco_captioning_result_val_e9.json
Using downloaded and verified file: ./output_freeze/coco_karpathy_val_gt.json
Annotation file is ./output_freeze/./output_freeze/coco_karpathy_val_gt.json
Results file is ./output_freeze/submit_coco_captioning_val_e9.json
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.01s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 307342 tokens at 1273767.86 tokens per second.
PTBTokenizer tokenized 62492 tokens at 483552.85 tokens per second.
setting up scorers...
computing Bleu score...
{'testlen': 51413, 'reflen': 49338, 'guess': [51413, 46421, 41429, 36437], 'correct': [33674, 17327, 7763, 3239]}
ratio: 1.0420568324617732
Bleu_1: 0.655
Bleu_2: 0.494
Bleu_3: 0.358
Bleu_4: 0.253
computing METEOR score...
METEOR: 0.240
computing Rouge score...
ROUGE_L: 0.507
computing CIDEr score...
CIDEr: 0.822
computing SPICE score...
Parsing reference captions
Parsing test captions
Initiating Stanford parsing pipeline
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].
Threads( StanfordCoreNLP ) [14.673 seconds]
SPICE evaluation took: 25.74 s
SPICE: 0.174
Performance of the network on the 5000 val images: 0.8%
Max performance: 0.83%
Training time 9:03:33
