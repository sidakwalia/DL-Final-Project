# VQA
## 1
- [Papers with Code](https://paperswithcode.com/task/visual-question-answering/latest)
- LLaMA weights are available
    - [magnet](magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&dn=LLaMA)
    - [pyllama](https://github.com/juncongmoo/pyllama)
    - 7B parameters are almost 6 GiG
- Another opensource alternative: Pythia
- [OpenAssistant](https://open-assistant.io/) Apr 15
- LangChain && SudoLang

## 2
- Make an interview summary based on things
    - [Resume Dataset Kaggle](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset?select=data)
- Verilog from image / SQL from image
- Enchance Chatgpt 

## 3
- Virtual Interior Designer and placement of object
- Translation of a video into multiple languages

## 4
- We can use self-instruct code in conjuction with various models on various coding sites

- Maybe use Beit for visual transformers and reduce the size
- Use GLHF
- Look for Pythia
- Some Papers:
    - [Paper1](https://arxiv.org/pdf/2012.12877.pdf)
    - [Paper2](https://arxiv.org/pdf/2209.02432.pdf)
- [HF Beit](https://huggingface.co/docs/transformers/model_doc/beit)
- [Google Conceptual Captions](https://github.com/google-research-datasets/conceptual-captions)


