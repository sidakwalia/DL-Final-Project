{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "from transformers import VisionEncoderDecoderModel, AutoFeatureExtractor,AutoTokenizer\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor\n",
    "\n",
    "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "text_decode_model = \"gpt2\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    image_encoder_model, text_decode_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/mambaforge/envs/DL/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# image feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(image_encoder_model)\n",
    "# text tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_decode_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/vit-gpt-model/tokenizer_config.json',\n",
       " './model/vit-gpt-model/special_tokens_map.json',\n",
       " './model/vit-gpt-model/vocab.json',\n",
       " './model/vit-gpt-model/merges.txt',\n",
       " './model/vit-gpt-model/added_tokens.json',\n",
       " './model/vit-gpt-model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./model/vit-gpt-model\"\n",
    "model.save_pretrained(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset coco_dataset_script (/home/mohit/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-cad0e5c392169c54/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0714ccd58f4d14bdab0d2bbfbeadd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "COCO_DIR = \"/media/mohit/New Volume1/Data/COCO\"\n",
    "ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\", data_dir=COCO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 203564,\n",
       " 'caption_id': 37,\n",
       " 'caption': 'A bicycle replica with a clock as the front wheel.',\n",
       " 'height': 400,\n",
       " 'width': 400,\n",
       " 'file_name': '000000203564.jpg',\n",
       " 'coco_url': 'http://images.cocodataset.org/train2017/000000203564.jpg',\n",
       " 'image_path': '/home/mohit/.cache/huggingface/datasets/downloads/extracted/7f35498e22a3f1fcf04745a731e57613c814149dc6e949ad5ef953d722f47632/train2017/000000203564.jpg'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(len(ds['train'])):\n",
    "#     print(ds['train'][i]['image_id'])\n",
    "#     # print(ds['train'][i]['image'])\n",
    "#     # print(ds['train'][i]['caption'])\n",
    "#     print(\"*********************\")\n",
    "\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# text preprocessing step\n",
    "def tokenization_fn(captions, max_target_length):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(captions, \n",
    "                      padding=\"max_length\", \n",
    "                      max_length=max_target_length).input_ids\n",
    "\n",
    "    return labels\n",
    "\n",
    "# image preprocessing step\n",
    "def feature_extraction_fn(image_paths, check_image=True):\n",
    "    \"\"\"\n",
    "    Run feature extraction on images\n",
    "    If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n",
    "    Otherwise, an exception will be thrown.\n",
    "    \"\"\"\n",
    "\n",
    "    model_inputs = {}\n",
    "\n",
    "    if check_image:\n",
    "        images = []\n",
    "        to_keep = []\n",
    "        for image_file in image_paths:\n",
    "            try:\n",
    "                img = Image.open(image_file)\n",
    "                images.append(img)\n",
    "                to_keep.append(True)\n",
    "            except Exception:\n",
    "                to_keep.append(False)\n",
    "    else:\n",
    "        images = [Image.open(image_file) for image_file in image_paths]\n",
    "\n",
    "    encoder_inputs = feature_extractor(images=images, return_tensors=\"np\")\n",
    "\n",
    "    return encoder_inputs.pixel_values\n",
    "\n",
    "def preprocess_fn(examples, max_target_length, check_image = True):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    image_paths = examples['image_path']\n",
    "    captions = examples['caption']    \n",
    "    \n",
    "    model_inputs = {}\n",
    "    # This contains image path column\n",
    "    model_inputs['labels'] = tokenization_fn(captions, max_target_length)\n",
    "    model_inputs['pixel_values'] = feature_extraction_fn(image_paths, check_image=check_image)\n",
    "\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21dbdb8ab39475d97be49ee20c525e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/591753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported number of image dimensions: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      2\u001b[0m     function\u001b[39m=\u001b[39;49mpreprocess_fn,\n\u001b[1;32m      3\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      4\u001b[0m     fn_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mmax_target_length\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m128\u001b[39;49m},\n\u001b[1;32m      5\u001b[0m     remove_columns\u001b[39m=\u001b[39;49mds[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mcolumn_names\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/datasets/dataset_dict.py:851\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 851\u001b[0m     {\n\u001b[1;32m    852\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/datasets/dataset_dict.py:852\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    851\u001b[0m     {\n\u001b[0;32m--> 852\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    579\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    580\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    581\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3066\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3067\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3068\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3071\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3074\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3075\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/datasets/arrow_dataset.py:3449\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3445\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3446\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3447\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3448\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3449\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3450\u001b[0m         batch,\n\u001b[1;32m   3451\u001b[0m         indices,\n\u001b[1;32m   3452\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3453\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3454\u001b[0m     )\n\u001b[1;32m   3455\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3456\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3458\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/datasets/arrow_dataset.py:3330\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3328\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3329\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3330\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3332\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3333\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3334\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m, in \u001b[0;36mpreprocess_fn\u001b[0;34m(examples, max_target_length, check_image)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m# This contains image path column\u001b[39;00m\n\u001b[1;32m     46\u001b[0m model_inputs[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m tokenization_fn(captions, max_target_length)\n\u001b[0;32m---> 47\u001b[0m model_inputs[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m feature_extraction_fn(image_paths, check_image\u001b[39m=\u001b[39;49mcheck_image)\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m model_inputs\n",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m, in \u001b[0;36mfeature_extraction_fn\u001b[0;34m(image_paths, check_image)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     images \u001b[39m=\u001b[39m [Image\u001b[39m.\u001b[39mopen(image_file) \u001b[39mfor\u001b[39;00m image_file \u001b[39min\u001b[39;00m image_paths]\n\u001b[0;32m---> 35\u001b[0m encoder_inputs \u001b[39m=\u001b[39m feature_extractor(images\u001b[39m=\u001b[39;49mimages, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m encoder_inputs\u001b[39m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/transformers/image_processing_utils.py:458\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    457\u001b[0m     \u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py:262\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m images \u001b[39m=\u001b[39m [to_numpy_array(image) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 262\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39msize_dict, resample\u001b[39m=\u001b[39mresample) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m do_rescale:\n\u001b[1;32m    265\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m images \u001b[39m=\u001b[39m [to_numpy_array(image) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 262\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize(image\u001b[39m=\u001b[39;49mimage, size\u001b[39m=\u001b[39;49msize_dict, resample\u001b[39m=\u001b[39;49mresample) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m do_rescale:\n\u001b[1;32m    265\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py:126\u001b[0m, in \u001b[0;36mViTImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m size \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m size:\n\u001b[1;32m    125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe `size` dictionary must contain the keys `height` and `width`. Got \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m resize(\n\u001b[1;32m    127\u001b[0m     image, size\u001b[39m=\u001b[39;49m(size[\u001b[39m\"\u001b[39;49m\u001b[39mheight\u001b[39;49m\u001b[39m\"\u001b[39;49m], size[\u001b[39m\"\u001b[39;49m\u001b[39mwidth\u001b[39;49m\u001b[39m\"\u001b[39;49m]), resample\u001b[39m=\u001b[39;49mresample, data_format\u001b[39m=\u001b[39;49mdata_format, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/transformers/image_transforms.py:300\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msize must have 2 elements\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m \u001b[39m# For all transformations, we want to keep the same data format as the input image unless otherwise specified.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m# The resized image from PIL will always have channels last, so find the input format first.\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m data_format \u001b[39m=\u001b[39m infer_channel_dimension_format(image) \u001b[39mif\u001b[39;00m data_format \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m data_format\n\u001b[1;32m    302\u001b[0m \u001b[39m# To maintain backwards compatibility with the resizing done in previous image feature extractors, we use\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m# the pillow library to resize the image and then convert back to numpy\u001b[39;00m\n\u001b[1;32m    304\u001b[0m do_rescale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/DL/lib/python3.10/site-packages/transformers/image_utils.py:159\u001b[0m, in \u001b[0;36minfer_channel_dimension_format\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    157\u001b[0m     first_dim, last_dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported number of image dimensions: \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39mshape[first_dim] \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m ChannelDimension\u001b[39m.\u001b[39mFIRST\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported number of image dimensions: 2"
     ]
    }
   ],
   "source": [
    "processed_dataset = ds.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"max_target_length\": 128},\n",
    "    remove_columns=ds['train'].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./image-captioning-output\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['validation'],\n",
    "    data_collator=default_data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./image-captioning-output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./image-captioning-output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "image_captioner = pipeline(\"image-to-text\", model=\"./image-captioning-output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_text(\"https://ankur3107.github.io/assets/images/image-captioning-example.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
